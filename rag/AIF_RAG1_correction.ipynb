{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2faf85fc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc520151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/HDDLRAG/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "\n",
    "# General imports\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch and transformers (for LLM)\n",
    "import transformers, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModel\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# For loading documents from a path\n",
    "from pathlib import Path\n",
    "\n",
    "# For the embedding module\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# %%\n",
    "\n",
    "# Load device\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # MPS is the GPU model in Mac technology\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device =torch.device(\"cpu\")\n",
    "\n",
    "print (torch.ones(1, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59757853",
   "metadata": {},
   "source": [
    "# The different Modules of the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7792c",
   "metadata": {},
   "source": [
    "## The foundation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0033c5d",
   "metadata": {},
   "source": [
    "We are going to work with Foundation Models, that is models that have been pre-trained on large data sets; but we want to work with OpenSource and hosted models. To do so we are going to fetch models from the [HuggingFace plateform](https://huggingface.co/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d18335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoundationModel():\n",
    "\n",
    "    def __init__(self,FOUND_MODEL_PATH,TEMPERATURE=None,MAX_NEW_TOKENS=10000):\n",
    "        \n",
    "        self.model=AutoModelForCausalLM.from_pretrained(FOUND_MODEL_PATH, \n",
    "                                             #device_map=mps_device,\n",
    "                                             #device_map=cuda,\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             ).to(device)\n",
    "        \n",
    "        \n",
    "        self.tokenizer= AutoTokenizer.from_pretrained(FOUND_MODEL_PATH)\n",
    "\n",
    "        self.model.generation_config.temperature=TEMPERATURE # Config of the temperature\n",
    "        self.model.generation_config.top_p=None              # Config parameter related to the type of generation (like greedy decoding for instance)\n",
    "\n",
    "        self.llm = pipeline(\"text-generation\",\n",
    "                     model=self.model,\n",
    "                     tokenizer=self.tokenizer,\n",
    "                     return_full_text=False,\n",
    "                     max_new_tokens=MAX_NEW_TOKENS,\n",
    "                     do_sample=True\n",
    "                     )\n",
    "        \n",
    "        self.num_parameters = self.model.num_parameters()\n",
    "        \n",
    "        print('Number of parameters in my model','{:.2e}'.format(self.num_parameters))\n",
    "\n",
    "\n",
    "    def generate_response(self,prompt):\n",
    "        \n",
    "        messages = [\n",
    "            {'role':'user', 'content':prompt}\n",
    "            ]\n",
    "        \n",
    "        output=self.llm(messages)\n",
    "        # Note that the output is a list of len 1 which is a dict with key 'generated_text'\n",
    "        return output\n",
    "\n",
    "\n",
    "    # We anticipate the use of RAG and create a generate response taking into account the context\n",
    "\n",
    "    def generate_response_with_context(self, prompt, context):\n",
    "    \n",
    "        # The context is a list of str \n",
    "    \n",
    "        messages = []\n",
    "\n",
    "        if context:\n",
    "            for i, ctx in enumerate(context):\n",
    "                messages.append({'role': 'system','content': f\"context {i+1}: {ctx}\"})\n",
    "\n",
    "        messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "    \n",
    "        output=self.llm(messages)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ffd22",
   "metadata": {},
   "source": [
    "As you will see output of the models will be a list on len 1 in dict format with key ```generated_text```. We somehow reformat the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34684084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(output):\n",
    "    # output is a list of len 1 as a dict with key 'generated_text'\n",
    "\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c662e",
   "metadata": {},
   "source": [
    "Another component of the answer is the inclusion of a reasoning component (which is identified with tags ```<think> ...</think>```). We write which only extract the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75136f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_response(output):\n",
    "\n",
    "    response=extract_response(output=output)\n",
    "    #text = \"Before <think>to delete</think> After\"\n",
    "    short = re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL)\n",
    "    return short.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5dcf49",
   "metadata": {},
   "source": [
    "### Unit test for the foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d4b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors\n",
      "Will use dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside Qwen/Qwen3-0.6B.\n",
      "loading file vocab.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/vocab.json\n",
      "loading file merges.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in my model 5.96e+08\n"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "# Start with a model\n",
    "# Here we list some models and choose a small model Qwen 0.6\n",
    "\n",
    "Path_SDS=\"HuggingFaceTB/SmolLM3-3B\"\n",
    "Path_Qwen_4B = \"Qwen/Qwen3-4B\"\n",
    "Path_DSR1=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "Path_Q_06=\"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "f_model = FoundationModel(FOUND_MODEL_PATH=Path_Q_06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a2802",
   "metadata": {},
   "source": [
    "Have a look at the different elements while loading the model; do you recognize some constitutive elements of Transformers ? Look also the number of parameters of the model.\n",
    "\n",
    "Try several prompts, look at the type of the output directly from the llm or simply the output produced with `generate_response`methods (standard or short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8efc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a query\n",
    "\n",
    "# print the output and look to the format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621622ed",
   "metadata": {},
   "source": [
    "We are going to study a specific example. Generate and print the answer to the query below. Which natural remark once have to keep in mind while using Open Source models ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55963179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A specific prompt\n",
    "\n",
    "prompt_RF='Is Robert Redford alive ?; Answer must be [Yes] or [No]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "061cfc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Is Robert Redford alive ?; Answer must be [Yes] or [No]\n",
      "[No]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt:\",prompt_RF)\n",
    "\n",
    "output = f_model.generate_response(prompt=prompt_RF)\n",
    "\n",
    "response = short_response(output=output)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644919e1",
   "metadata": {},
   "source": [
    "## The Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622ad5b",
   "metadata": {},
   "source": [
    "Our foundation model will be mostly used as a decoder, even though it includes an encoder one usually chooses a separate embedding model. Indeed the embedding model of the decoder is designed for the generation process but fails to produce most adequate embeddings for a semantic study (like computing similarity). We make use here of an Open Source embedding model from Library `SentenceTransformer` (in particular we will create embeddings for sentences or more precisely for *chunks*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51bb91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel():\n",
    "\n",
    "    def __init__(self,EMBEDD_MODEL_PATH):\n",
    "        \n",
    "        # EMBEDD_MODEL_PATH is the name of the embedding model used within the SentenceTransformer lib\n",
    "\n",
    "        self.Embedmodel=SentenceTransformer(EMBEDD_MODEL_PATH).to(device)\n",
    "        self.dim=SentenceTransformer(EMBEDD_MODEL_PATH).get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "    def get_embeddings(self,texts):\n",
    "        \n",
    "        # texts is a list of strings (which is supposed to be the list of chinks; without the source)\n",
    "        # we return embeddings of torch type with shape (len(texts),self.dim)\n",
    "\n",
    "        embeddings=self.Embedmodel.encode(texts,convert_to_tensor=True,normalize_embeddings=True).to(device)\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "    def compute_cos_sim_embed(self,embed1,embed2):\n",
    "\n",
    "        # embed1,embeds2 are two embeddings of shape (1,dim)\n",
    "        # We compute the cos-similarity of two texts (it is returned as a float)\n",
    "\n",
    "        embed1=embed1.view(-1)\n",
    "        embed2=embed2.view(-1)\n",
    "\n",
    "        norm1=torch.norm(embed1,p=2,dim=0)\n",
    "        norm2=torch.norm(embed2,p=2,dim=0)\n",
    "\n",
    "        scal = torch.dot(embed1,embed2)\n",
    "        \n",
    "        return scal.item()/(norm1.item()*norm2.item())\n",
    "\n",
    "    \n",
    "    def compute_cos_sim_texts(self,text_1,text_2):\n",
    "\n",
    "        # text1,text2 are two str\n",
    "        # We compute the cos-similarity of two texts (it is returned as a float)\n",
    "\n",
    "        embeds = self.get_embeddings(texts=[text_1,text_2])\n",
    "        \n",
    "        return self.compute_cos_sim_embed(embeds[0],embeds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eda1e3",
   "metadata": {},
   "source": [
    "### Unit test for the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "912b3ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n",
      "loading file vocab.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n",
      "loading file vocab.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([2, 384]) <class 'torch.dtype'>\n",
      "0.3035542964935303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "\n",
    "Embed_mini=\"all-MiniLM-L6-v2\"\n",
    "EmbedModel=EmbeddingModel(EMBEDD_MODEL_PATH=Embed_mini)\n",
    "\n",
    "# Once again have a look at the parameters of the model.\n",
    "\n",
    "sentences=['Hello World','How is the weather ?']\n",
    "\n",
    "embeddings = EmbedModel.get_embeddings(texts=sentences)\n",
    "\n",
    "print(type(embeddings),embeddings.shape,type(embeddings[0].dtype))\n",
    "\n",
    "em = EmbedModel.compute_cos_sim_embed(embeddings[0],embeddings[1])\n",
    "\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793e0e0",
   "metadata": {},
   "source": [
    "## Notion of Chunk and Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b8b45",
   "metadata": {},
   "source": [
    "We decide to create a class for managing chunks. A *chunk* will be defined by:\n",
    "\n",
    "-  ```source``` the name of the .txt file from which the chunk comes from\n",
    "-  ```content``` which is the str that composes the chunk\n",
    "-  ```embedding``` which is the embedding associated to ```content```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7063c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk():\n",
    "\n",
    "    def __init__(self,source,content,embed_model: EmbeddingModel):\n",
    "\n",
    "        self.embedding_model=embed_model\n",
    "\n",
    "        #dim is the common dimension of the embeddings\n",
    "        dim = self.embedding_model.dim\n",
    "        \n",
    "        # A chunk is defined by its source (str); its content (str); its embedding (a torch which shape (1,dim))\n",
    "\n",
    "        self.source=str(source)\n",
    "        self.content=str(content)\n",
    "        self.embedding=self.embedding_model.get_embeddings(texts=[content]).reshape(1,dim)\n",
    "\n",
    "\n",
    "    def print_chunk(self):\n",
    "\n",
    "        print('source:',self.source,'content:',self.content,'embedding shape:',self.embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3631568",
   "metadata": {},
   "source": [
    "## Todwards the index: the splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0341e",
   "metadata": {},
   "source": [
    "We are going to chunk resources and then compare the similarity of each of these chunks with the one of the query. We follow the plan of the lecture and start with the splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24600bca",
   "metadata": {},
   "source": [
    "This class aims in getting the documents that will be in a folder with address ```path_doc```, and return as an output of the method ```get_chunks``` the chunks associated to these resources. Note that we keep track of which document is issued each chunk (via the ```source``` feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69095223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter():\n",
    "\n",
    "    def __init__(self,embed_model: EmbeddingModel):\n",
    "        \n",
    "        self.embedding_model=embed_model\n",
    "        \n",
    "        self.docs = [] \n",
    "        # We store the original documents as a list of .txt files (format is {\"source\":'File_name',\"content_page\":(str)})\n",
    "        self.chunks=[] \n",
    "        # This will be the list of chunks \n",
    "\n",
    "    def get_documents(self,path_doc):\n",
    "        # PATH_DOC is the Path form where the documents will be found (each document is a.txt file).\n",
    "        docs=[]\n",
    "\n",
    "        for file in Path(path_doc).rglob(\"*.txt\"):\n",
    "            name=file.name\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as file: \n",
    "                resource=file.read().strip()\n",
    "                if resource:\n",
    "                    #print(resource,len(resource))\n",
    "                    docs.append({\"source\":name,\"content_page\":resource})\n",
    "        \n",
    "        self.docs=docs\n",
    "\n",
    "\n",
    "    def get_chunks_contents_from_1_doc(self,file_name,content_page,chunk_size,overlap,sentence_split=False):\n",
    "\n",
    "        if chunk_size < overlap:\n",
    "            raise Exception('Careful overlap must be smaller than chunk_size')\n",
    "        \n",
    "        # Now we chunk according to chunk size and overlap\n",
    "\n",
    "        if sentence_split:\n",
    "\n",
    "            content=content_page.split(\".\")\n",
    "\n",
    "            for text in content:\n",
    "\n",
    "                text = text.lstrip()\n",
    "\n",
    "                if not text==\"\":\n",
    "                    self.chunks.append(Chunk(source=file_name,\n",
    "                      content=text,embed_model=self.embedding_model))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            current = 0\n",
    "\n",
    "            while current < len(content_page):\n",
    "                end = min(len(content_page),current+chunk_size)\n",
    "                content = content_page[current:end]\n",
    "            \n",
    "                self.chunks.append(Chunk(source=file_name,\n",
    "                      content=content,embed_model=self.embedding_model))\n",
    "                \n",
    "                current += chunk_size - overlap\n",
    "        \n",
    "\n",
    "    def get_chunks(self,path_doc,chunk_size,overlap,sentence_split=False):\n",
    "\n",
    "        self.get_documents(path_doc=path_doc)\n",
    "\n",
    "        docs=self.docs\n",
    "\n",
    "        for doc in docs:\n",
    "\n",
    "            self.get_chunks_contents_from_1_doc(file_name=doc[\"source\"],\n",
    "                                                content_page=doc[\"content_page\"],\n",
    "                                                chunk_size=chunk_size,\n",
    "                                                overlap=overlap,\n",
    "                                                sentence_split=sentence_split)\n",
    "    \n",
    "    def reset_splitter(self):\n",
    "\n",
    "        self.docs=[]\n",
    "        self.chunks=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba89ee",
   "metadata": {},
   "source": [
    "### Unit test for the splitting module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb008e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n",
      "loading file vocab.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n",
      "loading file vocab.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "\n",
    "this_path = Path.cwd()/\"Docs\"\n",
    "embed_model=EmbeddingModel(EMBEDD_MODEL_PATH=Embed_mini)\n",
    "\n",
    "Split=Splitter(embed_model)\n",
    "\n",
    "Split.reset_splitter()\n",
    "Split.get_documents(path_doc=this_path)\n",
    "Split.get_chunks(path_doc=this_path,chunk_size=30,overlap=15,sentence_split=True)\n",
    "\n",
    "print(len(Split.chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f7b4398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert_Redford.txt \n",
      " Robert Redford passed away last month \n",
      " torch.Size([1, 384])\n",
      "Personalities.txt \n",
      " Albert Einstein proposed the theory of relativity, which transformed our understanding of time,space, and gravity \n",
      " torch.Size([1, 384])\n",
      "Personalities.txt \n",
      " Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes \n",
      " torch.Size([1, 384])\n",
      "Personalities.txt \n",
      " Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics \n",
      " torch.Size([1, 384])\n",
      "Personalities.txt \n",
      " Charles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species' \n",
      " torch.Size([1, 384])\n",
      "Personalities.txt \n",
      " Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine \n",
      " torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "# Do your own tests, with overlaps, without, what is the impact ? like for example : \n",
    "\n",
    "for i in range(len(Split.chunks)):\n",
    "\n",
    "    print(Split.chunks[i].source,\"\\n\",Split.chunks[i].content,\"\\n\",Split.chunks[i].embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78545d",
   "metadata": {},
   "source": [
    "## Index, Database and Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ef5e8",
   "metadata": {},
   "source": [
    "We are now in position to define the retriever. In our setting with very few document we will build our index (when the number of resources is pretty high (over $10^4$) we make use of an index (which is trained for that purpose); one can for example make use of the FAISS library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever():\n",
    "\n",
    "    def __init__(self,embed_model: EmbeddingModel):\n",
    "        \n",
    "        self.embedding_model=embed_model\n",
    "        \n",
    "        # The index is a list of (Id(int),chunk); chunk needs the size DIM for the Embeddings\n",
    "        self.index=[]\n",
    "        \n",
    "        \n",
    "    def add_elements_to_index(self,chunks):\n",
    "\n",
    "        # chunks is a list of chunk\n",
    "\n",
    "        num = len(self.index)\n",
    "\n",
    "        for chunk in chunks:\n",
    "\n",
    "            self.index.append([num,chunk])\n",
    "            num+=1\n",
    "\n",
    "    def search_best(self,query,number_of_hits=3,adapt=False):\n",
    "        \n",
    "        # query is a str\n",
    "\n",
    "        query_embed = self.embedding_model.get_embeddings(texts=[query]).to(device).reshape(1,self.embedding_model.dim)\n",
    "\n",
    "        results=[]\n",
    "\n",
    "        index=self.index\n",
    "\n",
    "        scores=[]\n",
    "\n",
    "        for item in index:\n",
    "\n",
    "            id,chunk = item\n",
    "\n",
    "            sim = self.embedding_model.compute_cos_sim_embed(embed1=query_embed,embed2=chunk.embedding)\n",
    "\n",
    "            scores.append((id,chunk,sim))\n",
    "  \n",
    "        results=sorted(scores,key=lambda x:x[2],reverse=True)[:min(number_of_hits,len(index))]\n",
    "\n",
    "        # We can also add a criterion to exclude the worst hits; here we choose an arbitrary criterion (we exclude a hit if the similarity is smaller than half of the previous one among the number_of_hits chunks)\n",
    "\n",
    "        if adapt:\n",
    "\n",
    "            i=1\n",
    "            go=True\n",
    "            while go and i<len(results):\n",
    "                if results[i][2] < results[i-1][2]*0.5:\n",
    "                    go=False\n",
    "                else:\n",
    "                    i+=1\n",
    "            \n",
    "            results=results[:i]\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def reset_Retriever_index(self):\n",
    "\n",
    "        self.index=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7b72d",
   "metadata": {},
   "source": [
    "### Unit test of the Retriever module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56e46ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'Robert_Redford.txt', 'content_page': 'Robert Redford passed away last month.'}, {'source': 'Personalities.txt', 'content_page': \"Albert Einstein proposed the theory of relativity, which transformed our understanding of time,space, and gravity. \\nMarie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.\\nIsaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.\\nCharles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'.\\nAda Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine.\"}]\n",
      "6\n",
      "1\n",
      "results [(0, <__main__.Chunk object at 0x380cf5370>, 0.8707782626152039)]\n"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "\n",
    "# Recall that we have already in our previous unit tests defined an embedding model and a splitter\n",
    "\n",
    "        # embed_model=EmbeddingModel(EMBEDD_MODEL_PATH=Embed_mini)\n",
    "\n",
    "        # Split=Splitter(embed_model)\n",
    "        # Split.chunks=[]\n",
    "        # Split.get_chunks(path_doc=this_path,chunk_size=30,overlap=15,sentence_split=True)\n",
    "\n",
    "print(Split.docs)\n",
    "\n",
    "chunks=Split.chunks\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "retriever = Retriever(embed_model)\n",
    "\n",
    "\n",
    "# Add the chunks to the index\n",
    "\n",
    "# Get the best results using your retriever to the query \n",
    "\n",
    "        #prompt_RF='Is Robert Redford alive ?; Answer must be [Yes] or [No]'\n",
    "\n",
    "\n",
    "retriever.add_elements_to_index(chunks=chunks)\n",
    "\n",
    "query='Is Robert Redford alive'\n",
    "\n",
    "results=retriever.search_best(query=query,number_of_hits=3,adapt=True)\n",
    "\n",
    "print(\"results\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "794365c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'Robert_Redford.txt', 'content_page': 'Robert Redford passed away last month.'}, {'source': 'Personalities.txt', 'content_page': \"Albert Einstein proposed the theory of relativity, which transformed our understanding of time,space, and gravity. \\nMarie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.\\nIsaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.\\nCharles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'.\\nAda Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine.\"}]\n",
      "6\n",
      "results [(0, <__main__.Chunk object at 0x37fc430e0>, 0.8707782626152039), (1, <__main__.Chunk object at 0x380cf5130>, 0.08921658992767334)]\n"
     ]
    }
   ],
   "source": [
    "# Unit test follow up\n",
    "\n",
    "Split.chunks=[]\n",
    "\n",
    "Split.get_chunks(path_doc=this_path,chunk_size=30,overlap=15,sentence_split=True)\n",
    "\n",
    "print(Split.docs)\n",
    "\n",
    "chunks=Split.chunks\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "retriever = Retriever(embed_model)\n",
    "\n",
    "retriever.add_elements_to_index(chunks=chunks)\n",
    "\n",
    "query='Is Robert Redford alive'\n",
    "\n",
    "results=retriever.search_best(query=query,number_of_hits=2)\n",
    "\n",
    "print(\"results\",results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602eca1",
   "metadata": {},
   "source": [
    "# The RAG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13df139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG():\n",
    "\n",
    "    def __init__(self,CONFIG):\n",
    "        \n",
    "        self.foundation_model=FoundationModel(FOUND_MODEL_PATH=CONFIG['FOUND_MODEL_PATH'])\n",
    "        self.Embedding_model=EmbeddingModel(EMBEDD_MODEL_PATH=CONFIG['EMBEDD_MODEL_PATH'])\n",
    "        self.splitter=Splitter(self.Embedding_model)\n",
    "        self.retriever=Retriever(self.Embedding_model)\n",
    "\n",
    "        self.dim_embed = CONFIG['DIM_EMBED']\n",
    "        self.chunk_size = CONFIG['CHUNK_SIZE']\n",
    "        self.overlap = CONFIG['OVERLAP']\n",
    "\n",
    "    \n",
    "    def reset_index(self):\n",
    "\n",
    "        self.retriever.reset_Retriever_index()\n",
    "        self.splitter.reset_splitter()\n",
    "    \n",
    "    \n",
    "    def load_documents_and_get_chunks(self,path,sentence_split=False):\n",
    "\n",
    "        self.splitter.get_chunks(path_doc=path,\n",
    "                                 chunk_size=self.chunk_size,\n",
    "                                 overlap=self.overlap,\n",
    "                                 sentence_split=sentence_split)\n",
    "        \n",
    "        chunks = self.splitter.chunks\n",
    "\n",
    "        self.retriever.add_elements_to_index(chunks=chunks)\n",
    "        \n",
    "    \n",
    "    def get_retrieval(self,query,number_of_hits):\n",
    "\n",
    "        retrieved_info = self.retriever.search_best(query=query,number_of_hits=number_of_hits)\n",
    "\n",
    "        # It is the full information of the form (Id, chunk, sim)\n",
    "\n",
    "        retrieved=[]\n",
    "        \n",
    "        for elem in retrieved_info:\n",
    "\n",
    "            i,chunk, distance=elem\n",
    "\n",
    "            retrieved.append(chunk.content)\n",
    "        \n",
    "        # We get rid of repeated items\n",
    "        return list(dict.fromkeys(retrieved))\n",
    "    \n",
    "    def generate_response_rag(self,query):\n",
    "\n",
    "        retrieved=self.get_retrieval(query=query,\n",
    "                                          number_of_hits=3)\n",
    "        \n",
    "        return self.foundation_model.generate_response_with_context(prompt=query,\n",
    "                                                                   context=retrieved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89e8ff",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b959c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/model.safetensors\n",
      "Will use dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside Qwen/Qwen3-0.6B.\n",
      "loading file vocab.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/vocab.json\n",
      "loading file merges.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in my model 5.96e+08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n",
      "loading file vocab.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n",
      "loading file vocab.txt from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'FOUND_MODEL_PATH':Path_Q_06,\n",
    "    #'FOUND_MODEL_PATH':\"Qwen/Qwen3-4B\",\n",
    "    'EMBEDD_MODEL_PATH':\"all-MiniLM-L6-v2\",\n",
    "    'DIM_EMBED':384,\n",
    "    'CHUNK_SIZE':300,\n",
    "    'OVERLAP':30\n",
    "        }\n",
    "\n",
    "rag = RAG(CONFIG=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dab1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize our RAG with our documents\n",
    "\n",
    "rag.reset_index()\n",
    "\n",
    "rag.load_documents_and_get_chunks(path=Path.cwd()/\"Docs\",sentence_split=True)\n",
    "\n",
    "chunks=rag.splitter.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e014627",
   "metadata": {},
   "source": [
    "## The Robert Redford experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4c7845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"<think>\\nOkay, the user is asking if Robert Redford is alive. I need to check if there's any information I might be missing. First, I remember that Redford was born in 1930 and died in 2002. So, he was indeed alive at the time of his death. But wait, maybe there's a trick here. Let me think. Some people might confuse his death with his early life, but no, he was born in 1930 and died in 2002. So the answer should be [Yes].\\n</think>\\n\\n[Yes]\"}] \n",
      " [{'generated_text': \"<think>\\nOkay, the user is asking if Robert Redford is alive. Let me check the contexts provided.\\n\\nContext 1 says Robert Redford passed away last month. So the answer should be [Yes] because he did pass away. But wait, maybe there's a trick here. The user might be testing if I can recognize that passing away means he's not alive. But according to general knowledge, Redford did pass away. So the correct answer is Yes.\\n</think>\\n\\n[Yes]\"}]\n"
     ]
    }
   ],
   "source": [
    "# Once again for the prompt below compare the answer with and without RAG\n",
    "        # prompt_RF='Is Robert Redford alive ?; Answer must be [Yes] or [No]'\n",
    "\n",
    "# Print what is the retrieved context\n",
    "\n",
    "query = prompt_RF\n",
    "\n",
    "response_no_RAG = rag.foundation_model.generate_response(prompt=query)\n",
    "\n",
    "context = rag.get_retrieval(query=query,number_of_hits=3)\n",
    "\n",
    "response_RAG = rag.generate_response_rag(query=query)\n",
    "\n",
    "print(response_no_RAG,\"\\n\",response_RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cb02e",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad3da3",
   "metadata": {},
   "source": [
    "You noticed that since you asked for a fixed ```number_of_hits``` some of the context might be pointless. We aim in preventing this and selecting the retrieved context. We provide two ways for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15563f22",
   "metadata": {},
   "source": [
    "**1st we plot the similarity and try to find an accurate split out of it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "569b224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both methods we start with our retrieved information\n",
    "\n",
    "rag.reset_index()\n",
    "\n",
    "rag.load_documents_and_get_chunks(path=Path.cwd()/\"Docs\",sentence_split=True)\n",
    "\n",
    "retrieved_info = rag.retriever.search_best(query=query,number_of_hits=3)\n",
    "# It is (Id,chunk,sim) and it is ordered by sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e79cab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                              chunk       sim\n",
      "0   0              Robert Redford passed away last month  0.719682\n",
      "1   1  Albert Einstein proposed the theory of relativ...  0.094111\n",
      "2   3  Isaac Newton formulated the laws of motion and...  0.045098\n",
      "   Id                                  chunk       sim\n",
      "0   0  Robert Redford passed away last month  0.719682\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "select_data=[]\n",
    "\n",
    "temp_sim = retrieved_info[0][2] # The highest sim\n",
    "\n",
    "for item in retrieved_info:\n",
    "   \n",
    "    Id,chunk,sim=item\n",
    "    data.append((Id,chunk.content,sim))\n",
    "\n",
    "    temp = temp_sim*0.9 # the current sim - 10%\n",
    "\n",
    "    if sim > temp:\n",
    "\n",
    "        temp_sim = sim\n",
    "        select_data.append((Id,chunk.content,sim))\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data,columns=[\"Id\",\"chunk\",\"sim\"])\n",
    "selected_data = pd.DataFrame(select_data,columns=[\"Id\",\"chunk\",\"sim\"])\n",
    "\n",
    "print(data)\n",
    "print(selected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93254e19",
   "metadata": {},
   "source": [
    "**2nd method : *LLM as a judge***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e3812",
   "metadata": {},
   "source": [
    "This time we give the query and the retrieved information to a LLM called *judge (as a) LLM* and ask it to select the more suitable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de5d44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/config.json\n",
      "Model config SmolLM3Config {\n",
      "  \"architectures\": [\n",
      "    \"SmolLM3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128012,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 65536,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"smollm3\",\n",
      "  \"no_rope_layer_interval\": 4,\n",
      "  \"no_rope_layers\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    0\n",
      "  ],\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 5000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/reveilla/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/model.safetensors.index.json\n",
      "Will use dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating SmolLM3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128012,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:00<00:00, 27.28it/s]\n",
      "loading configuration file generation_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128012,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside HuggingFaceTB/SmolLM3-3B.\n",
      "loading file tokenizer.json from cache at /Users/reveilla/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/tokenizer.json\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/reveilla/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/reveilla/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at /Users/reveilla/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1/chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in my model 3.08e+09\n"
     ]
    }
   ],
   "source": [
    "# We define the judge (it is always better to take another LLM and to do the opposite of what we are doing now: usually one chooses the judge to be a more powerful model; here for VRAM reasons we select a weaker one).DS_Store\n",
    "\n",
    "judge_llm = FoundationModel(FOUND_MODEL_PATH=Path_SDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8403c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '<think>\\nOkay, let\\'s tackle this query. The user is asking if Robert Redford is alive, and they want the answer to be Yes or No. The provided resources are three sentences, and I need to determine which ones are useful for answering the question.\\n\\nFirst, let me recall who Robert Redford is. He\\'s an American actor, director, and producer known for his roles in films like \"Butch Cassidy and the Sundance Kid\" and \"All the President\\'s Men.\" I need to check if he\\'s still alive.\\n\\nLooking at the first resource: \"Robert Redford passed away last month.\" If this is true, then the answer would be No. The second resource talks about Albert Einstein\\'s theory of relativity, which isn\\'t relevant to Redford\\'s current status. The third resource is about Newton\\'s laws of motion, also unrelated. \\n\\nSince the first resource directly states that Redford passed away, that\\'s the key information needed here. The other two sentences don\\'t provide any information about his current life or death. Therefore, only the first resource is useful in answering the question.\\n</think>\\n\\n[Yes]  \\nThe first resource is useful because it directly states that Robert Redford passed away, answering the query.'}]\n"
     ]
    }
   ],
   "source": [
    "context_chunks = data[\"chunk\"].tolist()\n",
    "\n",
    "#print(context_chunks)\n",
    "\n",
    "#prompt = print(f\"Given [{query}] tell me among the elements of [{context_chunks}] which ones are the more relevant to [{query}]; give me the answer as a list of elements of [{context_chunks}]\")\n",
    "\n",
    "prompt=f\"\"\" You are a clever assistant. Given the query: \"{query}\"; here is a list of resources \"{context_chunks}\". For each of those say which is useful to answer the query. Give your result by only listing the useful results.\n",
    "\"\"\"\n",
    "\n",
    "print(judge_llm.generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f8c5f",
   "metadata": {},
   "source": [
    "## The personalities experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46847609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who introduced the theory of relativity?', 'Who was the first computer programmer?', 'What did Isaac Newton contribute to science?', 'Who won two Nobel Prizes for research on radioactivity?', 'What is the theory of evolution by natural selection?']\n",
      "['Albert Einstein proposed the theory of relativity, which transformed our understanding of time, space and gravity.', \"Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine.\", 'Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.', 'Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.', \"Charles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'.\"]\n"
     ]
    }
   ],
   "source": [
    "# You can extend the study by now looking to questions that still address one chunk but which are related semantically. \n",
    "# We introduce 5 queries; each one is related to one and only one personality\n",
    "\n",
    "queries = [\n",
    "\"Who introduced the theory of relativity?\",\n",
    "\"Who was the first computer programmer?\",\n",
    "\"What did Isaac Newton contribute to science?\",\n",
    "\"Who won two Nobel Prizes for research on radioactivity?\",\n",
    "\"What is the theory of evolution by natural selection?\"\n",
    "]\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914de101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly to before, test the index and the impact of the RAG on the queries above based on the reference (contrary to the previous situation the Open Source model already has a sufficient knowledge on these questions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDDLRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
