{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt9mGio-2Mmx"
      },
      "source": [
        "# Part I: Conformal Regression\n",
        "\n",
        "We consider a simple regression problem on heteroskedastic data. We want to evaluate the uncertainty associated with the prediction using various conformal prediction methods. The main objective of this first part is to get a better grasp of how Conformal Prediction works, and to *visualize* the effect of the different algorithms on the coverage rate and the size of the prediction intervals. We will code most of the algorithms from scratch, and compare our results with those obtained with help of the PUNCC library for verification purposes.\n",
        "\n",
        "**Links**\n",
        "- [PUNCC Github](https://github.com/deel-ai/puncc)\n",
        "- [PUNCC Documentation](https://deel-ai.github.io/puncc/index.html)\n",
        "\n",
        "**Warning!** We will be working on tensorflow and not on pytorch. Make sure to use the jupyter kernel associated with the tensorflow environment !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnzN8HZq2Mmz"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "**Exercise.** Install the puncc library using pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_V2OZPz2Mm0"
      },
      "outputs": [],
      "source": [
        "# TODO: install the PUNCC library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTOFV7ls2Mm1"
      },
      "source": [
        "We import some of the libraries that we will be using throughout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wODidhUv2Mm1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjsP1eXm2Mm2"
      },
      "source": [
        "## 1. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d-qM8ND2Mm2"
      },
      "source": [
        "We consider a synthetic 1D heteroskedastic dataset, where the variance of the noise increases with the value of the input feature.\n",
        "We generate $N$ samples as follows:\n",
        "\n",
        "- Inputs $X$ are uniformly distributed on $[0, 20]$\n",
        "- Outputs are given by $Y = (1+\\epsilon)\\cdot X, $\n",
        "\n",
        "Such that $\\epsilon \\sim {\\cal N}(\\mu=0,\\sigma=1)$ is standard gaussian noise.\n",
        "\n",
        "**Exercise.**\n",
        "1. Complete the function `heteroskedastic_data` that takes the number of samples `n_samples` to be generated as an arguments, and outputs numpy arrays `X` and `y` of size `n_samples` according to the procedure described above.\n",
        "2. Use the `heteroskedastic_data` function to generate 4000 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faWFg9ev2Mm2"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic 1D heteroskedastic data\n",
        "\n",
        "def heteroskedastic_data(n_samples):\n",
        "    # TODO: complete\n",
        "\n",
        "\n",
        "# TODO: generate 4000 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/data/heteroskedastic_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSQw03buBa5r"
      },
      "source": [
        "**Exercise.** Complete the function `plot_data below`, and use it to plot the synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2er3CFeA_gD"
      },
      "outputs": [],
      "source": [
        "def plot_data(X, y):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # TODO: plot the data, what plot type would you choose?\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# TODO: plot the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/data/plot_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ijwE1xBrMj"
      },
      "source": [
        "**Exercise.** Split the data into a training set and a test set by using the `train_test_split` function of the `scikit-learn` library. Split the data randomly and leaving out 25% of the data for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NevHayn9_FuA"
      },
      "outputs": [],
      "source": [
        "# TODO: Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/data/split_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zM53vaUHacq"
      },
      "source": [
        "## 2. Split Conformal Regression\n",
        "\n",
        "In order to perform the *split conformal regression* algorithm, we need to split our training data into a *proper training set* (which we will store in the variables `X_fit` and `y_fit`) and a *calibration set*.\n",
        "\n",
        "**Exercise.** Further split the training data randomly by leaving out 50% of the training data for the calibration set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9kwSokT2Mm4"
      },
      "outputs": [],
      "source": [
        "# TODO: Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/split_calib.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P15suIFMK-IS"
      },
      "source": [
        "**Exercise.** Train a prediction model using the `LinearRegression` class in the `scikit-learn` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0erSYk42Mm3"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/train_regressor.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-38rY-fcMAQQ"
      },
      "source": [
        "**Exercise.** Complete the `plot_model` function below in order to plot synthetic data along with the model predictions. Use the function `plot_model` to plot the test data along with the model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaCk0pvdMH0z"
      },
      "outputs": [],
      "source": [
        "def plot_model(model, X, y):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # TODO: plot the data\n",
        "    # TODO: plot the model predictions\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# TODO: visualize the data and model in the same plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/plot_regressor.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFdTY1rZwvOO"
      },
      "source": [
        "Now that we have a trained model, we wish to *conformalize* it. The simplest algorithm to conformalize a model is the *split conformal* algorithm.\n",
        "\n",
        "In this section, we are using the split conformal algorithm for the regression task, but as we have seen in the lectures, there are many other conformalization algorithms that rely on splitting the data into a train/fit and a calibration set. Therefore, we will coda a generic class called `SplitConformal` that we will be able to use later for other conformal prediction algorithms of the *split* type.\n",
        "\n",
        "In order to define our general `SplitConformal` class, we will rely on the following information:\n",
        "the algorithms using the `SplitConformal` class are different between each other only in two ways:\n",
        "- the way the *nonconformity scores* are computed,\n",
        "- the way the *prediction sets* are constructed.\n",
        "\n",
        "Therefore, we will build the `SplitConformal` class so that it can work with any choice of *nonconformity score function* and *prediction set contruction function*.\n",
        "\n",
        "\n",
        "**Exercise.** Code the `SplitConformal` class by implementing the following:\n",
        "1. Attributes:\n",
        "  - `score_fn`: the nonconformity score function to be used (a function that computes scores between ground truth values and model predictions).\n",
        "  - `predset_fn`: the prediction set construction function to be used (a function that builds prediction sets from model predictions and quantile values).\n",
        "  - `scores`: attribute to store the array of calibration nonconformity scores.\n",
        "  - `quantile`: attribute to store the value of the quantile of order $1-\\alpha$ (plus correction).\n",
        "\n",
        "2. Methods:\n",
        "  - `__init__`: takes as input the choice of nonconformity score function and prediction set construction function.\n",
        "  - `compute_scores`: takes as input a numpy array containing ground truth $y$-values and predicted $y$-values, computes the nonconformity scores, and stores them into the attribute `scores`.\n",
        "  - `compute_quantile`: takes as input the nominal error rate $\\alpha$ and computes the quantile of level $1-\\alpha$ (taking into account the usual finite-sample correction characteristic of conformal prediction algorithms) on the array of scores saved in the `scores` attribute. **Warning!** You may use the `quantile` function in the `numpy` library, however, make sure you choose the *right* method to compute the quantile values, otherwise the probabilistic guarantee of conformal prediction will no longer be true. (**This is an important point, try and understand the different methods and choose the correct one yourself before checking the solutions!!!**)\n",
        "  - `predict`: takes as input an array of model predictions and outputs prediction sets (in whatever format the `predset_fn` outputs prediction sets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-jBZs8aw1yd"
      },
      "outputs": [],
      "source": [
        "class SplitConformal():\n",
        "    # TODO: Complete the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/split_conformal.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhzDBc3P1fqN"
      },
      "source": [
        "We are dealing with a univariate regression problem. Thus, a natural choice for our nonconformity score is to use the absolute difference between the ground truth and the prediction. The corresponding prediction set is an interval, which is obtained by adding and substracting the empirical quantile to the model prediction :\n",
        "- $S(y,\\hat{y}) = |y-\\hat{y}|$\n",
        "- $C_{\\alpha}(x) = [\\hat{f}(x)-\\hat{q}_{1-\\alpha}, \\hat{f}(x)+\\hat{q}_{1-\\alpha}]$\n",
        "\n",
        "**Exercise.** Code the `abs_difference` and `additive_interval` functions to be used as the arguments of the `SplitConformal` class at initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjvWD01e1X82"
      },
      "outputs": [],
      "source": [
        "# TODO: code the abs_difference function\n",
        "\n",
        "# TODO: code the additive_interval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/absdiff_addint.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WkxO_jd1O_l"
      },
      "source": [
        "**Exercise.** Use the `SplitConformal` class to conformalize the Linear Regression model with a nominal error rate of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTOs_VZo2XLG"
      },
      "outputs": [],
      "source": [
        "# TODO: conformalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/conformalize_regressor.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1An-2gDM3ILU"
      },
      "source": [
        "**Exercise.** Complete the function `plot_conformalized_data` below and use it to plot the test dataset along with the model predictions and the prediction intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2foMQULL3T8M"
      },
      "outputs": [],
      "source": [
        "def plot_conformalized_data(X, y, y_pred, y_lower, y_upper):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sort_indices = np.argsort(X.flatten())\n",
        "\n",
        "    # TODO: plot the data\n",
        "    # TODO: plot the model predictions\n",
        "    # TODO: plot the prediction interval\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "plot_conformalized_data(X_test, y_test, y_pred_test, y_lower, y_upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/plot_conformalized_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o0G1uve_CNH"
      },
      "source": [
        "**Exercise.** Write a function called `evaluate_conformal_regression` that takes as input an array of ground-truth $y$-values along with an array of the lower limits and an array of the upper limits of the corresponding prediction intervals. It then computes and outputs the following two metrics:\n",
        "1. `coverage`: The average number of intervals that contain the ground-truth values.\n",
        "2. `avg_length`: The average length of the intervals.\n",
        "\n",
        "**Questions.**\n",
        "1. What value should we expect for the `coverage` metric?\n",
        "2. What kind of values do we desire for the `avg_length` metric?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AECk58vD-Jih"
      },
      "outputs": [],
      "source": [
        "# TODO: implement the evaluate_conformal_regression function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/split/eval_conf_regressor.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SijSFsr0l81R"
      },
      "source": [
        "**Exercise.** Evaluate the conformalized model with the help of the `evaluate_conformal_regression` function and display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0ou8AO8l3O_"
      },
      "outputs": [],
      "source": [
        "# TODO: evaluate the conformal prediction regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppq3xp7fApK-"
      },
      "source": [
        "The evaluation seems to match the desired results. \n",
        "\n",
        "**Remark.** Note however (from the plot) how the interval length is constant, and does not match the heteroskedasticity of the data. Indeed, the theoretical guarantee in the CP theorem is true for the quantity $P(Y \\in C_{\\alpha}(X))$ averaged over $X$, but is not true in general for $P(Y\\in C_{\\alpha}(X)| X=x)$!\n",
        "\n",
        "Next, we perform a similar conformalization procedure using the PUNCC library, this will allow us to compare results, but also to learn how the PUNCC library works so that we can use it in the future instead of coding the CP algorithms from scratch.\n",
        "\n",
        "**Exercise (at home or in the end if you still have time).** Find the tutorial called *Introduction Tutorial* in the Readme page of the PUNCC github repository and dollow the steps presented in the *Conformal Regression* section in order to conformalize the linar regression model above using PUNCC:\n",
        "1. Train the model using PUNCC rather than fitting it directly.\n",
        "2. Compute the prediciton intervals.\n",
        "3. Compute the evaluation metrics.\n",
        "4. Use PUNCC's visualization tools to visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/puncc/fit_reg.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/puncc/calib_reg.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/puncc/eval_reg.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/puncc/viz_reg.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLxLvISd2Mm5"
      },
      "source": [
        "## 3. Cross-Validation+ (CV+)\n",
        "As seen during the lecture, practitionners are not always willing to sacrifice part of the training data for the calibration phase. Of course, this is not a problem for our toy case, where we can generate as many extra exmaples as we wish... but we will nevertheless implement\n",
        "the *Cross-Validation+* algorithm, which allows to use the whole training dataset for training, instead of splitting it into a proper training set and a calibration set. This comes at the cost of having to train multiple models.\n",
        "\n",
        "**Exercise.** Use the `KFold` class from `scikit-learn` to:\n",
        "1. Train 10 different linear regression models according to the CV+ algorithm.\n",
        "2. Compute the nonconformity scores on each of the folds.\n",
        "\n",
        "**Exercise.** Try explaining on your own words what we are doing : \n",
        "1. How is data being used? \n",
        "2. Which data to train which model? \n",
        "3. What models are in use in the calculation of the nonconformity scores?\n",
        "\n",
        "Store the models into a list of models called `models` and the scores in a numpy array called `scores`.\n",
        "\n",
        "**Warning.** Read carefully the documentaion of the `Kfold` function, we will be using it again further down to compute the prediction sets and the point predictions on the test sets, and we need to make sure that the data is split in the exact same folds both times!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7g57twnJrtl"
      },
      "outputs": [],
      "source": [
        "# TODO: train and compute scores according to the CV+ algorithm\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# TODO: Define the k-fold cross-validation scheme\n",
        "kf = # TODO: Define the k-fold cross-validation scheme\n",
        "\n",
        "# Initialize empty lists to store models and scores\n",
        "models = []\n",
        "scores = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
        "\n",
        "    #TODO: Split the data into training and validation sets\n",
        "    \n",
        "    #TODO: Initialize the linear model\n",
        "\n",
        "    #TODO: Train the linear model and store it in the models array\n",
        "    \n",
        "    #TODO: Compute predictions on the validation set\n",
        "\n",
        "    #TODO: Compute nonconformity scores and append to lists\n",
        "\n",
        "# concatenate scores into a numpy array\n",
        "scores = np.concatenate(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cv/cv_train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z417DAEReeRs"
      },
      "source": [
        "**Exercise.** Use the CV+ algorithm to produce prediction intervals for nominal $\\alpha=0.1$. Compute point predictions on the test set by averaging accross the 10 different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFpdlWNGJrq_"
      },
      "outputs": [],
      "source": [
        "# Conformalize the model using the CV+ algorithm\n",
        "#TODO: Set value of nominal error rate alpha\n",
        "\n",
        "# Initialize lower and upper arrays\n",
        "y_pred_test = []\n",
        "y_lower = []\n",
        "y_upper = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(scores)):\n",
        "\n",
        "    y_pred = #TODO: Compute predictions on k-th model\n",
        "    y_pred_test.append(y_pred)\n",
        "\n",
        "    scores_fold = #TODO: Extract scores\n",
        "\n",
        "    #TODO: Update lower and upper arrays\n",
        "\n",
        "# Convert the lists y_lower and y_upper into numpy arrays\n",
        "y_pred_test = np.stack(y_pred_test)\n",
        "y_lower = np.concatenate(y_lower, axis=1)\n",
        "y_upper = np.concatenate(y_upper, axis=1)\n",
        "\n",
        "y_pred_test = #TODO: Compute predictions by averaging over the k models\n",
        "\n",
        "# Compute prediction intervals using the numpy quantile function\n",
        "alpha_lower_corrected = np.floor(alpha * (len(scores) + 1)) / len(scores)\n",
        "alpha_upper_corrected = np.ceil((1 - alpha) * (len(scores) + 1)) / len(scores)\n",
        "y_lower = np.quantile(y_lower, alpha_lower_corrected, axis=1, method=\"inverted_cdf\")\n",
        "y_upper = np.quantile(y_upper, alpha_upper_corrected, axis=1, method=\"inverted_cdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cv/cv_calib.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FyqypAoT6x4"
      },
      "source": [
        "**Exercise.** Evaluate and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yi_4gUnT8th"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate the results\n",
        "\n",
        "# TODO: plot the conformalized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cv/cv_eval.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX7L7o8wJTTv"
      },
      "source": [
        "**Exercise (at home or in the end if you still have time).** Perform the Cross-Validation using PUNCC and compare the result, there is no tutorial for this algorithm, but you can check the following section in the documentation to help yourself:\n",
        "\n",
        "https://deel-ai.github.io/puncc/regression.html#deel.puncc.regression.CVPlus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/puncc/cv_calib.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/puncc/cv_eval.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EGTxIRtdm3c"
      },
      "source": [
        "Note that even if the guarantee provided by the CV+ method is\n",
        "$$\\mathbb{P}(Y_{n+1}\\in \\hat{C}_\\alpha(X_{n+1})) \\geq 1-2\\alpha,$$\n",
        "the achieved coverage rate is rather close to $1-\\alpha$.\n",
        "\n",
        "**Question.** How can you explain this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwvxw_Kr2Mm6"
      },
      "source": [
        "## 4. Conformal Quantile Regression\n",
        "We now turn to the problem of the *constant* prediction intervals. As we can see in the plots above, the coverage rate of $1-\\alpha$ is obtained by over-covering in the low-variance regions and under-covering in the high variance regions. We consider the *Conformalized Quantile Regression (CQR)* algorithm, the purpose of which is to generate prediction sets that are more adapted to the heteroskedasticity of the data. CQR extends traditional quantile regression by incorporating conformal prediction techniques, allowing us to construct predictive intervals with state-of-the-art performance and guaranteed coverage (under data exchangeability).\n",
        "\n",
        "**Exercise.** Train lower and upper quantile models for a nominal eror rate $\\alpha$ of 0.1, using the `GradientBoostingRegressor` model with 10 estimators from the `sklearn.ensemble` module.\n",
        "\n",
        "**Careful!** The CQR algorithm is a kind of *split conformal* algorithm, so we need to use the *fit* data split to train the models, and hold out the *calibration* data split for the calibration phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICHEON0i2Mm6"
      },
      "outputs": [],
      "source": [
        "# TODO: train the lower and upper quantile models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cqr/train_quantiles.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwGRrhO_hVyJ"
      },
      "source": [
        "**Exercise.** Complete the `plot_quantile_model` function below and use it to plot the test examples along with the lower and upper quantile model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-8PdHTXhWWp"
      },
      "outputs": [],
      "source": [
        "def plot_quantile_models(X, y, lower_quantile_model, upper_quantile_model):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sort_indices = np.argsort(X.flatten())\n",
        "    X_sorted = X[sort_indices]\n",
        "    y_sorted = y[sort_indices]\n",
        "    # TODO: plot the data\n",
        "    # TODO: plot the lower quantile predictions\n",
        "    # TODO: plot the upper quantile predictions\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_quantile_models(X_test, y_test, lower_quantile_model, upper_quantile_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cqr/plot_quantiles.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHoDSFVRiONh"
      },
      "source": [
        "**Exercise.** Code the functions `cqr_score` and `cqr_set` so that we can use them along with the class `SplitConformal` above in order to implement the CQR algorithm. In order to keep a structure compatible with the `SplitConformal` class above, the `y_pred` input to the `cqr_score` and `cqr_set` functions is an array of length 2, where the first element contains the predictions of the lower quantile model and the second one the predictions of the upper quantile model.\n",
        "\n",
        "**Careful!** There is a typo in the lecture notes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "044bBpvrixs0"
      },
      "outputs": [],
      "source": [
        "# TODO: implement the functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cqr/cqr_functions.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWgRFmrFjbB5"
      },
      "source": [
        "**Exercise.** Conformalize the quantile regressors using the CQR algorithm and the calibration dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgurDqrkjNQD"
      },
      "outputs": [],
      "source": [
        "# TODO: Conformalize according to the CQR algorithm\n",
        "y_pred_calib_lower = # TODO: predict the upper quantiles of the calibration data\n",
        "y_pred_calib_upper = # TODO: predict the lower quantiles of the calibration data\n",
        "y_pred_calib = np.stack([y_pred_calib_lower, y_pred_calib_upper])\n",
        "\n",
        "alpha = # TODO: set the value of alpha\n",
        "\n",
        "splitcr = # TODO: instantiate the split conformal class\n",
        "\n",
        "# TODO: conformalize the quantile regression model\n",
        "\n",
        "# TODO: compute the prediction intervals on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cqr/cqr_calib.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTT_TJ3wkcQj"
      },
      "source": [
        "**Exercise.** Evaluate and visualize the results of the CQR conformalized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgiISGOWkglG"
      },
      "outputs": [],
      "source": [
        "# TODO: evaluate the results and print the evaluation metrics\n",
        "\n",
        "def plot_cqr_conformalized_data(X, y, y_pred_lower, y_pred_upper, y_lower, y_upper):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sort_indices = np.argsort(X.flatten())\n",
        "    X_sorted = X[sort_indices]\n",
        "    y_pred_lower_sorted = y_pred_lower[sort_indices]\n",
        "    y_pred_upper_sorted = y_pred_upper[sort_indices]\n",
        "    y_lower_sorted = y_lower[sort_indices]\n",
        "    y_upper_sorted = y_upper[sort_indices]\n",
        "\n",
        "    plt.scatter(X, y, alpha=0.6)\n",
        "    plt.plot(X_sorted, y_pred_lower_sorted, color=\"red\", linewidth=3)\n",
        "    plt.plot(X_sorted, y_pred_upper_sorted, color=\"red\", linewidth=3)\n",
        "    plt.fill_between(\n",
        "        X_sorted.flatten(),\n",
        "        y_lower_sorted,\n",
        "        y_upper_sorted,\n",
        "        alpha=0.3,\n",
        "        color=\"green\",\n",
        "    )\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "plot_cqr_conformalized_data(X_test, y_test, y_pred_test_lower, y_pred_test_upper, y_lower, y_upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/cqr/cqr_eval.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U9FvTqbWSOb"
      },
      "source": [
        "**Questions.**\n",
        "- What do you observe?\n",
        "- Is this the expected behavior? Why?\n",
        "- How does the average interval width compare with the previous methods?\n",
        "\n",
        "**Exercise (at home or in the end if you still have time).** Implement the CQR algorithm in with PUNCC and compare the results with the ones obtained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/puncc/cqr.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dMp2GrgbUha"
      },
      "source": [
        "# Part II: Conformal Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtowNStI71Md"
      },
      "source": [
        "The objective of this section is to train a small neural network on the MNIST dataset and apply the Conformal Classification algorithms seen during the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "694F_wwk4FN3"
      },
      "source": [
        " ## 1. Dataset\n",
        "**Exercise.**\n",
        "1. Import the MNIST dataset from keras.\n",
        "2. Split the training set into a proper training set, which we call the `fit` dataset, and a calibration set. Use the first 50_000 training points for the fit dataset and the remaining ones for the calibration dataset.\n",
        "3. Convert the labels to categorical, and save them into new arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgshPvvT3BcR"
      },
      "outputs": [],
      "source": [
        "# TODO: Import and pre-process the data according to the instructions.\n",
        "\n",
        "# TODO : Import the mnist dataset\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# TODO: Load teh MNIST data\n",
        "\n",
        "# Preprocessing: reshaping and standardization\n",
        "X_train = X_train.reshape((len(X_train), 28, 28))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((len(X_test), 28 , 28))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "X_fit, X_calib  = # TODO: Split fit and calib datasets\n",
        "y_fit, y_calib  = # TODO: Split fit and calib datasets\n",
        "\n",
        "y_fit_cat = # TODO: Perform the one hot encoding of the labels\n",
        "y_calib_cat = # TODO: Perform the one hot encoding of the labels\n",
        "y_test_cat = # TODO: Perform the one hot encoding of the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/classif/data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzozVxv54LqC"
      },
      "source": [
        "## 2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jPfgClX8vYI"
      },
      "source": [
        "**Exercise.** Define a simple convolutional neural network having the following sequential architecture:\n",
        "- A convolution with kernel size 3 and 16 channels.\n",
        "- A ReLU activation.\n",
        "- A max pooling layer with kernel size 2.\n",
        "- A convolution with kernel size 3 and 32 channels.\n",
        "- A reLU activation.\n",
        "- A max pooling layer with kernel size 2.\n",
        "- A Fully connected layer with 10 neurons.\n",
        "- A softmax activation.\n",
        "\n",
        "**Warning!** Use tensorflow, it is easier to use with PUNCC later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL4YqMVs3G-w"
      },
      "outputs": [],
      "source": [
        "from tensorflow import random\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "random.set_seed(0)\n",
        "keras.utils.set_random_seed(0)\n",
        "\n",
        "# Classification model: convnet composed of two convolution/pooling layers\n",
        "# and a dense output layer\n",
        "nn_model = ...  # TODO: define the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/classif/model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2B3TJSj4PF7"
      },
      "source": [
        "## 3. Training\n",
        "\n",
        "**Exercise.** Train the model using the Adam optimizer, and the categorical cross-entropy loss. Plot the training and validation accuracy while training, with 10% of the fit data left out for the validation set.\n",
        "\n",
        "Train the model for 2 epochs with a batch size of 256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7hfuoCp4XHi"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the model for two epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/classif/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCA-LBOpbYAA"
      },
      "source": [
        "## 4. Least Ambiguous Set-Valued Classifiers (LAC)\n",
        "We now implement the LAC algorithm seen during the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPEBOJdjn_Mp"
      },
      "source": [
        "**Exercise.** Define a `lac_score` and a `lac_set` function in order to be used with the `SplitConformal` class above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baaKnFdufW7F"
      },
      "outputs": [],
      "source": [
        "# TODO: define both functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/classif/lac_funcs.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhS730qQpDOU"
      },
      "source": [
        "**Exercise.** Conformalize the classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtHFwbLjo7_0"
      },
      "outputs": [],
      "source": [
        "y_pred_calib = nn_model.predict(X_calib)\n",
        "y_pred_test = nn_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA-j5mnqoPnh"
      },
      "outputs": [],
      "source": [
        "# TODO: Conformalize the classifier using the calibration dataset and a nominal error rate of 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/classif/lac_calib.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYVJxvgTqR5K"
      },
      "source": [
        "**Exercise.** Evaluate the results by computing the average coverage and average size of the prediction sets on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkEw0X21qeBo"
      },
      "outputs": [],
      "source": [
        "def eval_conformal_classifier(y_true, y_predset):\n",
        "    # TODO: implement the function to compute both metrics (coverage and set size)\n",
        "\n",
        "# TODO: Evaluate the conformalized model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVue3wFq1uD"
      },
      "source": [
        "**Exercise.** Plot a random image along with the prediction set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbzQskSf7aVL"
      },
      "outputs": [],
      "source": [
        "sample = 18\n",
        "\n",
        "plt.imshow(X_test[sample].reshape((28,28)))\n",
        "_ = plt.title(f\"Point prediction: {np.argmax(y_pred[sample])} \\n Prediction set: {y_predset_test[sample]}\")\n",
        "_ = plt.xticks([])\n",
        "_ = plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFxLJgQ97hNn"
      },
      "source": [
        "**Questions.**\n",
        "1. How come the average size of the prediction sets is smaller than 1?\n",
        "2. Some of the prediction sets are empty, they contain no labels. Why? Do you think that this is an Ok behavior?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t61p2NOE7Cee"
      },
      "source": [
        "## 5. Regularized Adaptive Prediction Sets\n",
        "**Exercise (at home or in the end if you still have time).** Follow the *Introduction Tutorial* in PUNCC, the section on *Conformal Classification* to implement the RAPS method as in the tutorial. Compare the results thus obtained with those obtained from the LAC method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWt0nXeK-dli"
      },
      "source": [
        "# To go further\n",
        "\n",
        "1. Code the APS and RAPS algorithms from scratch without using PUNCC.\n",
        "2. Check out the MAPIE library, which is a nice alternative to PUNCC.\n",
        "3. Check out the algorithms for Conformal Anomaly Detection and Conformal Object Detection in PUNCC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDZKVqre-zb9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
