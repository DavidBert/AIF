{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Artificial Intelligence Frameworks Hello and welcome to the AI Frameworks 2024 course! This course serves as a successor to the Machine Learning and High Dimensional & Deep Learning courses. While the aforementioned courses delved into the theory and methods of machine learning and deep learning, our current course is designed to equip you with the practical skills necessary to apply these methods in real-world scenarios. Specifically, we'll explore the following areas: Development Environment and Tools Python scripting Version control using Git and GitHub Working with Docker containers Ensuring reproducibility Session 2: Recommender Systems Collaborative filtering Content-based filtering Hybrid recommender systems Evaluating recommender systems Session 3: Natural Language Processing (NLP) Text preprocessing Bag of Words approach Word embeddings Introduction to Transformers Session 4: Out-of-Distribution and Anomaly Detection Session 5: Conformal Prediction Each session consists of three components: Theoretical Learning: This includes markdown documents and videos. Slides accompanying the videos will be provided for every lesson. Please review these materials before the practical sessions. Weekly reminders and content updates will be sent out. Should you have any queries, don't hesitate to raise them during the practical sessions or email me at david.bertoin (at) insa-toulouse.fr. You will be evaluated on your understanding of the theoretical concepts through quizzes at the beginning of each practical session. Practical Application: Comprising a series of Jupyter notebooks and Python scripts, this hands-on component requires you to code and answer questions. A teaching assistant will be available to assist and clarify any doubts. Additionally, you're not expected to complete this in a single session; feel free to continue at home. This is also the time to work on your final project. Evaluation Grading will be based on the final project (60%) and a quizzes (40%). The quizzes will be held at the beginning of each session and will test your understanding of the theoretical concepts. Knowledge requirements Python Tutorial Elementary statistic tools Data Exploration and Clustering . Machine Learning High Dimensional & Deep Learning","title":"Home"},{"location":"index.html#artificial-intelligence-frameworks","text":"Hello and welcome to the AI Frameworks 2024 course! This course serves as a successor to the Machine Learning and High Dimensional & Deep Learning courses. While the aforementioned courses delved into the theory and methods of machine learning and deep learning, our current course is designed to equip you with the practical skills necessary to apply these methods in real-world scenarios. Specifically, we'll explore the following areas: Development Environment and Tools Python scripting Version control using Git and GitHub Working with Docker containers Ensuring reproducibility Session 2: Recommender Systems Collaborative filtering Content-based filtering Hybrid recommender systems Evaluating recommender systems Session 3: Natural Language Processing (NLP) Text preprocessing Bag of Words approach Word embeddings Introduction to Transformers Session 4: Out-of-Distribution and Anomaly Detection Session 5: Conformal Prediction Each session consists of three components:","title":"Artificial Intelligence Frameworks"},{"location":"index.html#theoretical-learning","text":"This includes markdown documents and videos. Slides accompanying the videos will be provided for every lesson. Please review these materials before the practical sessions. Weekly reminders and content updates will be sent out. Should you have any queries, don't hesitate to raise them during the practical sessions or email me at david.bertoin (at) insa-toulouse.fr. You will be evaluated on your understanding of the theoretical concepts through quizzes at the beginning of each practical session.","title":"Theoretical Learning:"},{"location":"index.html#practical-application","text":"Comprising a series of Jupyter notebooks and Python scripts, this hands-on component requires you to code and answer questions. A teaching assistant will be available to assist and clarify any doubts. Additionally, you're not expected to complete this in a single session; feel free to continue at home. This is also the time to work on your final project.","title":"Practical Application:"},{"location":"index.html#evaluation","text":"Grading will be based on the final project (60%) and a quizzes (40%). The quizzes will be held at the beginning of each session and will test your understanding of the theoretical concepts.","title":"Evaluation"},{"location":"index.html#knowledge-requirements","text":"Python Tutorial Elementary statistic tools Data Exploration and Clustering . Machine Learning High Dimensional & Deep Learning","title":"Knowledge requirements"},{"location":"evaluation.html","text":"Evaluation The evaluation is associated to the DEFI-IA (Introduction video link ) Objectives You will be evaluated on your capacity of acting like a Data Scientist , i.e. Collect the data. Doing some exploratory analysis. Create new features. Write a complete pipeline to train and test your models. Justify your modelisation choices. Interpret your results. Work in group (Git). Share it and make your results easily reproducible (Docker, Gradio). Evaluation criteria You are expected to produce a code that is easily readable and reproducible. Your code should at leat contain the three following files (but you are ecouraged to add more to make it more readable): * train.py : the training script * app.py : code to launch a gradio application to test your model (see Gradio ) * analysis.ipynb : a notebook containing your exploratory analysis and interpretability results on your model. * Dockerfile : a Dockerfile to build a docker image of your application (see Docker ) You will be evaluated on the following criteria: Project - ( 70% ): You must provide a git repository with a complete history of your commits. Your capacity to work in group will be evaluated, your commit history must contain commits from several users at different dates. You must provide a Dockerfile to build a docker image that can be used to run your code (training and the Gradio application). The git should contain a clear markdown Readme, which describes: the result you achieved the commands to run for training your model or launching the gradio application (from a docker container) The code should be clear and easily readable. No notebooks exept for the exploratory analysis. * Oral presentation - ( 30% ) 15 minutes presentation + 10 minutes questions. You will be evaluated on the following criteria: Quality of the presentation. Explanations of the chosen features and algorithm. Demonstration of your application. Some insights on your model biais and interpretability. Other details Group of 4 people (DEFI IA's team).","title":"Evaluation"},{"location":"evaluation.html#evaluation","text":"The evaluation is associated to the DEFI-IA (Introduction video link )","title":"Evaluation"},{"location":"evaluation.html#objectives","text":"You will be evaluated on your capacity of acting like a Data Scientist , i.e. Collect the data. Doing some exploratory analysis. Create new features. Write a complete pipeline to train and test your models. Justify your modelisation choices. Interpret your results. Work in group (Git). Share it and make your results easily reproducible (Docker, Gradio).","title":"Objectives"},{"location":"evaluation.html#evaluation-criteria","text":"You are expected to produce a code that is easily readable and reproducible. Your code should at leat contain the three following files (but you are ecouraged to add more to make it more readable): * train.py : the training script * app.py : code to launch a gradio application to test your model (see Gradio ) * analysis.ipynb : a notebook containing your exploratory analysis and interpretability results on your model. * Dockerfile : a Dockerfile to build a docker image of your application (see Docker ) You will be evaluated on the following criteria: Project - ( 70% ): You must provide a git repository with a complete history of your commits. Your capacity to work in group will be evaluated, your commit history must contain commits from several users at different dates. You must provide a Dockerfile to build a docker image that can be used to run your code (training and the Gradio application). The git should contain a clear markdown Readme, which describes: the result you achieved the commands to run for training your model or launching the gradio application (from a docker container) The code should be clear and easily readable. No notebooks exept for the exploratory analysis. * Oral presentation - ( 30% ) 15 minutes presentation + 10 minutes questions. You will be evaluated on the following criteria: Quality of the presentation. Explanations of the chosen features and algorithm. Demonstration of your application. Some insights on your model biais and interpretability.","title":"Evaluation criteria"},{"location":"evaluation.html#other-details","text":"Group of 4 people (DEFI IA's team).","title":"Other details"},{"location":"introduction.html","text":"Course Introduction: Hi, welcome to the AI frameworks 2024 course.","title":"Course Introduction:"},{"location":"introduction.html#course-introduction","text":"Hi, welcome to the AI frameworks 2024 course.","title":"Course Introduction:"},{"location":"schedule.html","text":"Schedule Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures. Session 0 Course Introduction Session 1: Developpement tools 09/10/2023 (9h30-12h15 & 13h30-16h15) Introduction to Pytorch and Python scripts Github Reminder Introduction to Docker Practical session 1 Practical session 2 Session 2: Introduction to recommender systems 13/11/2023 (9h30-12h15 & 13h30-16h15) Session 3: Introduction to Natural language processing 27/11/2023 (9h30-12h15 & 13h30-16h15) Session 4: Introduction to Reinforcement Learning 18/12/2023 (9h30-12h15 & 13h30-16h15) Session 5: Explainability methods 08/01/2024 (9h30-12h15 & 13h30-16h15)","title":"Schedule"},{"location":"schedule.html#schedule","text":"Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures.","title":"Schedule"},{"location":"schedule.html#session-0","text":"Course Introduction","title":"Session 0"},{"location":"schedule.html#session-1-developpement-tools","text":"","title":"Session 1: Developpement tools"},{"location":"schedule.html#09102023-9h30-12h15-13h30-16h15","text":"Introduction to Pytorch and Python scripts Github Reminder Introduction to Docker Practical session 1 Practical session 2","title":"09/10/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-2-introduction-to-recommender-systems","text":"","title":"Session 2: Introduction to recommender systems"},{"location":"schedule.html#13112023-9h30-12h15-13h30-16h15","text":"","title":"13/11/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-3-introduction-to-natural-language-processing","text":"","title":"Session 3: Introduction to Natural language processing"},{"location":"schedule.html#27112023-9h30-12h15-13h30-16h15","text":"","title":"27/11/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-4-introduction-to-reinforcement-learning","text":"","title":"Session 4: Introduction to Reinforcement Learning"},{"location":"schedule.html#18122023-9h30-12h15-13h30-16h15","text":"","title":"18/12/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-5-explainability-methods","text":"","title":"Session 5: Explainability methods"},{"location":"schedule.html#08012024-9h30-12h15-13h30-16h15","text":"","title":"08/01/2024 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"conformal/conformal.html","text":"Conformal Prediction By Joseba Dalmau Practical session: Notebook: Solution:","title":"Course and practical session"},{"location":"conformal/conformal.html#conformal-prediction","text":"","title":"Conformal Prediction"},{"location":"conformal/conformal.html#by-joseba-dalmau","text":"","title":"By Joseba Dalmau"},{"location":"conformal/conformal.html#practical-session","text":"Notebook: Solution:","title":"Practical session:"},{"location":"developpement/api.html","text":"Development for Data Scientist: Deploying a Machine Learning Model using a REST API in Python In this lesson, we'll learn about REST APIs and how to deploy a machine learning model using Flask in Python . What is Model Deployment? Model deployment refers to the process of integrating a trained machine learning or statistical model into a production environment. The goal is to make the model's capabilities available to end-users, applications, or services. In the context of data science, deploying a model often means offering it as a service where applications send data to the model and receive predictions in return. Why is Model Deployment Crucial? Jupyter notebook or a local environment doesn't add business value. To realize its potential value, it must be made available where it's needed. Model deployment is the bridge between building a model and getting it into the hands of users. It's the last step in the data science pipeline but often one of the most complex. Without effective deployment, even the most sophisticated models are of little use. Challenges in Model Deployment: Model deployment is a complex process that involves several challenges. Some of the most common ones are: Scalability : Models may need to handle thousands of requests per second in some applications. It's crucial to ensure that deployment solutions can scale as needed. Latency : Especially in real-time applications, the time taken for a model to provide a prediction (latency) can be critical. Version Control : Models evolve over time. Deployment strategies need to account for versioning to ensure consistent and reproducible results. Dependency Management : Models might rely on specific libraries and versions, making dependency management a significant concern during deployment. While understanding the importance and intricacies of model deployment is crucial, the practical aspect involves choosing a suitable method for deployment. One of the most popular and effective ways to deploy our models is by using REST APIs . REST (Representational State Transfer) APIs provide a standardized way to make our models accessible over the internet. This method not only makes it easier to integrate our models into different applications but also offers scalability and flexibility. With REST APIs, we can encapsulate our models as services that can be consumed by various applications, be it web, mobile, or other services. What is an API? An API, or Application Programming Interface, is a set of rules and protocols that allow different software entities to communicate with each other. It specifies how software components should interact, enabling the integration of different systems and sharing of data and functionality. Key Concepts in API: Endpoint : A specific address or URL where an API can be accessed. Request & Response : The essential interaction in an API involves sending a request and receiving a response. Rate Limiting : Restrictions on how many API calls a user or system can make within a specified time frame. While various types of APIs exist, such as SOAP , GraphQL , and RPC , we'll be concentrating on REST APIs in this course due to their ubiquity and relevance in deploying machine learning models. Overview of RESTful Services: REST , or Representational State Transfer, is an architectural style for designing networked applications. RESTful services or APIs are designed around the principles of REST and use standard HTTP methods . Principles of REST: Statelessness : Each request from a client contains all the information needed to process the request. The server retains no session information. Client-Server Architecture : REST APIs follow a client-server model where the client is responsible for the user interface and user experience, and the server manages the data. Cacheability : Responses from the server can be cached on the client side to improve performance. Uniform Interface : A consistent and unified interface simplifies and decouples the architecture. Deploying a machine learning model using REST APIs is a common practice in the industry. It allows us to encapsulate our model as a service that can be consumed by other applications. This approach offers several advantages: Platform Independence : RESTful APIs can be consumed by any client that understands HTTP, regardless of the platform or language. Scalability : RESTful services are stateless, making it easier to scale by simply adding more servers. Performance : With the ability to cache responses, REST APIs can reduce the number of requests, improving the performance. Flexibility : Data can be returned in multiple formats, such as JSON or XML, allowing for flexibility in how it's consumed. APIs, especially RESTful APIs, are essential tools in the world of software integration. In the context of deploying data science models, they provide a mechanism to make the model accessible to other systems and applications in a standardized manner. As we delve deeper into this course, we'll see how to harness the power of REST APIs to deploy and serve our machine learning models efficiently. Components of REST APIs: Endpoints: An endpoint refers to a specific address (URL) in an API where particular functions can be accessed. For example, /predict might be an endpoint for a machine learning model where you send data for predictions. For example, https://api.example.com/predict could be an endpoint for a machine learning model where you send data for predictions. HTTP Methods: RESTful APIs use standard HTTP methods to denote the type of action to be performed: - GET : Retrieve data. - POST : Send data for processing or create a new resource. - PUT : Update an existing resource. - DELETE : Remove a resource. Status Codes: HTTP status codes are standard response codes given by web servers on the Internet. They help identify the success or failure of an API request. - 200 OK : Successful request. - 201 Created : Request was successful, and a resource was created. - 400 Bad Request : The server could not understand the request. - 401 Unauthorized : The client must authenticate itself. - 404 Not Found : The requested resource could not be found. - 500 Internal Server Error : A generic error message when an unexpected condition was encountered. Requests & Responses: Interacting with REST APIs involves sending requests and receiving responses. The request and response formats are standardized and follow a specific structure: - Request : Comprises the endpoint, method, headers, and any data sent to the server. - Response : Includes the status code, headers, and any data sent back from the server. Understanding the foundational elements of REST APIs is crucial for effectively designing, consuming, and deploying services on the web. As we transition to building our own APIs for model deployment, this knowledge will ensure we create efficient, scalable, and user-friendly interfaces for our models. Interacting with REST APIs using Python Python has a powerful library called requests that simplifies making requests to REST APIs. In this section, we will explore how to use this library to interact with an example API. For our hands-on learning, we'll fetch weather data using the requests library in Python. The requests library is a de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a simple API. In the following example, we'll use the requests library to fetch weather data from an API. OpenWeatherMap offers weather data, which is free for limited access. Although you typically need an API key, for brevity, we're using a mock API endpoint for our exercise. Crafting a GET Request for Weather Data We can use the requests library to make a GET request to fetch weather data from the API. We'll fetch the current weather for a city. For our example, let's use London. import requests # Mock URL for London's weather (no real API key needed) url = 'https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22' # Send the GET request response = requests.get(url) # Process and display the result if response.status_code == 200: data = response.json() print(f\"Weather in {data['name']} - {data['weather'][0]['description']}\") print(f\"Temperature: {data['main']['temp']}K\") This script will print the current weather description and temperature in Kelvin for London. Note : The data returned is in JSON format. It's structured and easy to parse in Python, which makes it a popular choice for APIs. APIs can return a lot of data. Here, besides the weather description and temperature, you can also access humidity, pressure, wind speed, and much more. Explore the data dictionary to uncover these details. Sending Data with POST Requests in Python While GET requests are primarily used to retrieve information from a server without causing any side effects, POST requests serve a different purpose. The POST method is designed to submit data to a server, usually resulting in a change in the server's state, such as creating a new record, updating data, or triggering an action. In essence, while GET is about asking the server for specific data, POST is about sending data to the server. With that understanding, let's delve into how to send data using POST requests in Python. For the sake of our pedagogical example, let's use JSONPlaceholder , a free fake online REST API used for testing and prototyping. Specifically, we'll be simulating the process of creating a new post. Before sending data, we must prepare it. Let's consider we're creating a new blog post: # The data we want to send post_data = { \"title\": \"Understanding REST APIs\", \"body\": \"REST APIs are fundamental in web services...\", \"userId\": 1 } This is our blog post with a title, body, and an associated user ID. With our data ready, we can send it using the requests library: import requests # The API endpoint where we want to create the new post url = 'https://jsonplaceholder.typicode.com/posts' # Sending the POST request response = requests.post(url, json=post_data) # Output the result if response.status_code == 201: print(f\"Successfully created! New Post ID: {response.json()['id']}\") else: print(\"Failed to create the post.\") Here, we've specified the URL to which we want to send the data and provided our post data in JSON format. Note : It's essential to check the response status code. A 201 status indicates that our data was successfully received and a new resource was created on the server. When you send a POST request, the server typically responds with details about the newly created resource. In our example, the server returns the ID of the newly created post, which we then print. POST requests are crucial when we want to send or update data on a server. With the requests library in Python, this process is streamlined, making data submission and interactions with web services smooth and efficient. Now that we've learned how to interact with REST APIs using Python, let's explore how to build our own API using Flask. Building a Simple Flask API Flask is a lightweight web framework for Python, making it easy to build web applications and RESTful services. In this section, we'll set up a simple Flask API that counts API requests and provides a method to determine the number of letters in a given name. First, install Flask: pip install flask Create a new file named app.py . This will be our main application file. For our example, we'll use the following code: from flask import Flask, jsonify, request app = Flask(name) # Initialize a counter request_count = 0 @app.route('/api/count', methods=['GET']) def count(): global request_count request_count += 1 return jsonify({\"count\": request_count}) @app.route('/api/letter_count', methods=['POST']) def letter_count(): global request_count request_count += 1 data = request.json name = data.get(\"name\", \"\") return jsonify({\"name\": name, \"letter_count\": len(name)}) if name == 'main': app.run(debug=True) This code sets up a Flask application with two routes: 1. /api/count : When accessed, it increases a counter and returns the current count. 2. /api/letter_count : Accepts a POST request with a JSON payload containing a name and returns the number of letters in the name. In your terminal or command prompt, navigate to the directory containing app.py and run: python app.py The Flask server should start, and by default, it'll be accessible at http://127.0.0.1:5000/. Requesting the Flask API using Python With our Flask API running, let's now query it using Python: Create a new file named client.py and add the following code: import requests # Making a GET request to the count endpoint count_response = requests.get('http://127.0.0.1:5000/api/count') print(count_response.json()) # Making a POST request to the letter_count endpoint data = {\"name\": \"Alice\"} letter_count_response = requests.post('http://127.0.0.1:5000/api/letter_count', json=data) print(letter_count_response.json()) The first request will return the current request count, while the second one will tell us the number of letters in the name \"Alice\". Flask provides an intuitive way to set up RESTful APIs quickly. With just a few lines of code, we've set up a server that can handle requests, perform operations, and return data. By understanding these basics, you can extend the functionality and integrate more complex operations, such as serving machine learning models.","title":"Introduction to REST APIs"},{"location":"developpement/api.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/api.html#deploying-a-machine-learning-model-using-a-rest-api-in-python","text":"In this lesson, we'll learn about REST APIs and how to deploy a machine learning model using Flask in Python .","title":"Deploying a Machine Learning Model using a REST API in Python"},{"location":"developpement/api.html#what-is-model-deployment","text":"Model deployment refers to the process of integrating a trained machine learning or statistical model into a production environment. The goal is to make the model's capabilities available to end-users, applications, or services. In the context of data science, deploying a model often means offering it as a service where applications send data to the model and receive predictions in return.","title":"What is Model Deployment?"},{"location":"developpement/api.html#why-is-model-deployment-crucial","text":"Jupyter notebook or a local environment doesn't add business value. To realize its potential value, it must be made available where it's needed. Model deployment is the bridge between building a model and getting it into the hands of users. It's the last step in the data science pipeline but often one of the most complex. Without effective deployment, even the most sophisticated models are of little use.","title":"Why is Model Deployment Crucial?"},{"location":"developpement/api.html#challenges-in-model-deployment","text":"Model deployment is a complex process that involves several challenges. Some of the most common ones are: Scalability : Models may need to handle thousands of requests per second in some applications. It's crucial to ensure that deployment solutions can scale as needed. Latency : Especially in real-time applications, the time taken for a model to provide a prediction (latency) can be critical. Version Control : Models evolve over time. Deployment strategies need to account for versioning to ensure consistent and reproducible results. Dependency Management : Models might rely on specific libraries and versions, making dependency management a significant concern during deployment. While understanding the importance and intricacies of model deployment is crucial, the practical aspect involves choosing a suitable method for deployment. One of the most popular and effective ways to deploy our models is by using REST APIs . REST (Representational State Transfer) APIs provide a standardized way to make our models accessible over the internet. This method not only makes it easier to integrate our models into different applications but also offers scalability and flexibility. With REST APIs, we can encapsulate our models as services that can be consumed by various applications, be it web, mobile, or other services.","title":"Challenges in Model Deployment:"},{"location":"developpement/api.html#what-is-an-api","text":"An API, or Application Programming Interface, is a set of rules and protocols that allow different software entities to communicate with each other. It specifies how software components should interact, enabling the integration of different systems and sharing of data and functionality.","title":"What is an API?"},{"location":"developpement/api.html#key-concepts-in-api","text":"Endpoint : A specific address or URL where an API can be accessed. Request & Response : The essential interaction in an API involves sending a request and receiving a response. Rate Limiting : Restrictions on how many API calls a user or system can make within a specified time frame. While various types of APIs exist, such as SOAP , GraphQL , and RPC , we'll be concentrating on REST APIs in this course due to their ubiquity and relevance in deploying machine learning models.","title":"Key Concepts in API:"},{"location":"developpement/api.html#overview-of-restful-services","text":"REST , or Representational State Transfer, is an architectural style for designing networked applications. RESTful services or APIs are designed around the principles of REST and use standard HTTP methods .","title":"Overview of RESTful Services:"},{"location":"developpement/api.html#principles-of-rest","text":"Statelessness : Each request from a client contains all the information needed to process the request. The server retains no session information. Client-Server Architecture : REST APIs follow a client-server model where the client is responsible for the user interface and user experience, and the server manages the data. Cacheability : Responses from the server can be cached on the client side to improve performance. Uniform Interface : A consistent and unified interface simplifies and decouples the architecture. Deploying a machine learning model using REST APIs is a common practice in the industry. It allows us to encapsulate our model as a service that can be consumed by other applications. This approach offers several advantages: Platform Independence : RESTful APIs can be consumed by any client that understands HTTP, regardless of the platform or language. Scalability : RESTful services are stateless, making it easier to scale by simply adding more servers. Performance : With the ability to cache responses, REST APIs can reduce the number of requests, improving the performance. Flexibility : Data can be returned in multiple formats, such as JSON or XML, allowing for flexibility in how it's consumed. APIs, especially RESTful APIs, are essential tools in the world of software integration. In the context of deploying data science models, they provide a mechanism to make the model accessible to other systems and applications in a standardized manner. As we delve deeper into this course, we'll see how to harness the power of REST APIs to deploy and serve our machine learning models efficiently.","title":"Principles of REST:"},{"location":"developpement/api.html#components-of-rest-apis","text":"","title":"Components of REST APIs:"},{"location":"developpement/api.html#endpoints","text":"An endpoint refers to a specific address (URL) in an API where particular functions can be accessed. For example, /predict might be an endpoint for a machine learning model where you send data for predictions. For example, https://api.example.com/predict could be an endpoint for a machine learning model where you send data for predictions.","title":"Endpoints:"},{"location":"developpement/api.html#http-methods","text":"RESTful APIs use standard HTTP methods to denote the type of action to be performed: - GET : Retrieve data. - POST : Send data for processing or create a new resource. - PUT : Update an existing resource. - DELETE : Remove a resource.","title":"HTTP Methods:"},{"location":"developpement/api.html#status-codes","text":"HTTP status codes are standard response codes given by web servers on the Internet. They help identify the success or failure of an API request. - 200 OK : Successful request. - 201 Created : Request was successful, and a resource was created. - 400 Bad Request : The server could not understand the request. - 401 Unauthorized : The client must authenticate itself. - 404 Not Found : The requested resource could not be found. - 500 Internal Server Error : A generic error message when an unexpected condition was encountered.","title":"Status Codes:"},{"location":"developpement/api.html#requests-responses","text":"Interacting with REST APIs involves sending requests and receiving responses. The request and response formats are standardized and follow a specific structure: - Request : Comprises the endpoint, method, headers, and any data sent to the server. - Response : Includes the status code, headers, and any data sent back from the server. Understanding the foundational elements of REST APIs is crucial for effectively designing, consuming, and deploying services on the web. As we transition to building our own APIs for model deployment, this knowledge will ensure we create efficient, scalable, and user-friendly interfaces for our models.","title":"Requests &amp; Responses:"},{"location":"developpement/api.html#interacting-with-rest-apis-using-python","text":"Python has a powerful library called requests that simplifies making requests to REST APIs. In this section, we will explore how to use this library to interact with an example API. For our hands-on learning, we'll fetch weather data using the requests library in Python. The requests library is a de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a simple API. In the following example, we'll use the requests library to fetch weather data from an API. OpenWeatherMap offers weather data, which is free for limited access. Although you typically need an API key, for brevity, we're using a mock API endpoint for our exercise.","title":"Interacting with REST APIs using Python"},{"location":"developpement/api.html#crafting-a-get-request-for-weather-data","text":"We can use the requests library to make a GET request to fetch weather data from the API. We'll fetch the current weather for a city. For our example, let's use London. import requests # Mock URL for London's weather (no real API key needed) url = 'https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22' # Send the GET request response = requests.get(url) # Process and display the result if response.status_code == 200: data = response.json() print(f\"Weather in {data['name']} - {data['weather'][0]['description']}\") print(f\"Temperature: {data['main']['temp']}K\") This script will print the current weather description and temperature in Kelvin for London. Note : The data returned is in JSON format. It's structured and easy to parse in Python, which makes it a popular choice for APIs. APIs can return a lot of data. Here, besides the weather description and temperature, you can also access humidity, pressure, wind speed, and much more. Explore the data dictionary to uncover these details.","title":"Crafting a GET Request for Weather Data"},{"location":"developpement/api.html#sending-data-with-post-requests-in-python","text":"While GET requests are primarily used to retrieve information from a server without causing any side effects, POST requests serve a different purpose. The POST method is designed to submit data to a server, usually resulting in a change in the server's state, such as creating a new record, updating data, or triggering an action. In essence, while GET is about asking the server for specific data, POST is about sending data to the server. With that understanding, let's delve into how to send data using POST requests in Python. For the sake of our pedagogical example, let's use JSONPlaceholder , a free fake online REST API used for testing and prototyping. Specifically, we'll be simulating the process of creating a new post. Before sending data, we must prepare it. Let's consider we're creating a new blog post: # The data we want to send post_data = { \"title\": \"Understanding REST APIs\", \"body\": \"REST APIs are fundamental in web services...\", \"userId\": 1 } This is our blog post with a title, body, and an associated user ID. With our data ready, we can send it using the requests library: import requests # The API endpoint where we want to create the new post url = 'https://jsonplaceholder.typicode.com/posts' # Sending the POST request response = requests.post(url, json=post_data) # Output the result if response.status_code == 201: print(f\"Successfully created! New Post ID: {response.json()['id']}\") else: print(\"Failed to create the post.\") Here, we've specified the URL to which we want to send the data and provided our post data in JSON format. Note : It's essential to check the response status code. A 201 status indicates that our data was successfully received and a new resource was created on the server. When you send a POST request, the server typically responds with details about the newly created resource. In our example, the server returns the ID of the newly created post, which we then print. POST requests are crucial when we want to send or update data on a server. With the requests library in Python, this process is streamlined, making data submission and interactions with web services smooth and efficient. Now that we've learned how to interact with REST APIs using Python, let's explore how to build our own API using Flask.","title":"Sending Data with POST Requests in Python"},{"location":"developpement/api.html#building-a-simple-flask-api","text":"Flask is a lightweight web framework for Python, making it easy to build web applications and RESTful services. In this section, we'll set up a simple Flask API that counts API requests and provides a method to determine the number of letters in a given name. First, install Flask: pip install flask Create a new file named app.py . This will be our main application file. For our example, we'll use the following code: from flask import Flask, jsonify, request app = Flask(name) # Initialize a counter request_count = 0 @app.route('/api/count', methods=['GET']) def count(): global request_count request_count += 1 return jsonify({\"count\": request_count}) @app.route('/api/letter_count', methods=['POST']) def letter_count(): global request_count request_count += 1 data = request.json name = data.get(\"name\", \"\") return jsonify({\"name\": name, \"letter_count\": len(name)}) if name == 'main': app.run(debug=True) This code sets up a Flask application with two routes: 1. /api/count : When accessed, it increases a counter and returns the current count. 2. /api/letter_count : Accepts a POST request with a JSON payload containing a name and returns the number of letters in the name. In your terminal or command prompt, navigate to the directory containing app.py and run: python app.py The Flask server should start, and by default, it'll be accessible at http://127.0.0.1:5000/.","title":"Building a Simple Flask API"},{"location":"developpement/api.html#requesting-the-flask-api-using-python","text":"With our Flask API running, let's now query it using Python: Create a new file named client.py and add the following code: import requests # Making a GET request to the count endpoint count_response = requests.get('http://127.0.0.1:5000/api/count') print(count_response.json()) # Making a POST request to the letter_count endpoint data = {\"name\": \"Alice\"} letter_count_response = requests.post('http://127.0.0.1:5000/api/letter_count', json=data) print(letter_count_response.json()) The first request will return the current request count, while the second one will tell us the number of letters in the name \"Alice\". Flask provides an intuitive way to set up RESTful APIs quickly. With just a few lines of code, we've set up a server that can handle requests, perform operations, and return data. By understanding these basics, you can extend the functionality and integrate more complex operations, such as serving machine learning models.","title":"Requesting the Flask API using Python"},{"location":"developpement/colorize.html","text":"Development for Data Scientist: Practical session 2: Deploying a digit classifier Now that you have a good understanding of the PyTorch framework and how to deploy your model through a REST API or a web application, you will develop an application that will colorize black and white images. Once again, you are expected to use Python scripts to train your model and to deploy it. Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. Then navigate to the developpement/colorize folder. Your working directory should look like this: code/ \u251c\u2500\u2500 data_utils.py \u251c\u2500\u2500 model.py \u251c\u2500\u2500 train.py \u251c\u2500\u2500 colorize_api.py \u251c\u2500\u2500 colorize_webapp.py \u251c\u2500\u2500 test_api.ipynb sample_images/ \u251c\u2500\u2500 img1.jpg \u251c\u2500\u2500 img2.jpg \u251c\u2500\u2500 img3.jpg \u251c\u2500\u2500 img4.jpg \u251c\u2500\u2500 img5.jpg requirements.txt download_landscapes.sh Data We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh Here, we only have access to color images, so we will have to generate our own black and white images. The file data_utils.py contains some useful functions to load the dataset. In particuler given a dataset containing landscape images, the function get_colorized_dataset_loader returns a PyTorch DataLoader object that can be used to iterate over the dataset yielding batches of black and white images and their corresponding colorized version. The network architecture We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = F.sigmoid(self.last_conv(x)) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python model.py Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the train.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Some of you may have GPUs on their local machine. If that is the case, you can use them to train your model. If not, you can use Google Colab to train your model on a GPU for free. If you are using Google Colab, you are expected to do all the code development on your local machine and then send your code to collab to train your model. Try to run your code on your local machine for one or two minibatches to check that everything is working. If it is the case, you can send your code to Google Colab to train your model. To do so: - Open the run_in_colab.ipynb notebook in Google Colab. - Make sure you are connected to a GPU runtime. - Run the first cell to download the dataset. - Upload the files data_utils.py , model.py and train.py to the code folder in Google Colab. - Run the second cell to launch a tensorboard instance. - Run the third cell to launch the training. - Download the trained model and the tensorboard logs to your local machine. API, Web app and deployment Complete the colorize_api.py file to create a Flask API that will colorize images. The API should have a /colorize endpoint that will take a black and white image as input and return the colorized version of the image. You can use the test_colorize_api.py file to test your API. You can test your app with random balck and white images from the net. For exemple one of these . You can also test your api using Postman. To do so: - Install Postman . - Launch your API. - Open Postman. - Create a new request. - Set the request type to POST. - Set the request URL to your API URL. - Go to the Body tab. - Select binary as the body type. - Select a black and white image on your computer. - Click on send. Do you have any idea why the colors are so dull? You can also complete the colorize_web_app.py file to create a web app that will colorize images. Finally complete the Dockerfile file to deploy your app on a local server. DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE. Solutions: model.py train.py colorize_api.py colorize_web_app.py","title":"Development for Data Scientist:"},{"location":"developpement/colorize.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/colorize.html#practical-session-2-deploying-a-digit-classifier","text":"Now that you have a good understanding of the PyTorch framework and how to deploy your model through a REST API or a web application, you will develop an application that will colorize black and white images. Once again, you are expected to use Python scripts to train your model and to deploy it.","title":"Practical session 2: Deploying a digit classifier"},{"location":"developpement/colorize.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. Then navigate to the developpement/colorize folder. Your working directory should look like this: code/ \u251c\u2500\u2500 data_utils.py \u251c\u2500\u2500 model.py \u251c\u2500\u2500 train.py \u251c\u2500\u2500 colorize_api.py \u251c\u2500\u2500 colorize_webapp.py \u251c\u2500\u2500 test_api.ipynb sample_images/ \u251c\u2500\u2500 img1.jpg \u251c\u2500\u2500 img2.jpg \u251c\u2500\u2500 img3.jpg \u251c\u2500\u2500 img4.jpg \u251c\u2500\u2500 img5.jpg requirements.txt download_landscapes.sh","title":"Practical session repository:"},{"location":"developpement/colorize.html#data","text":"We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh Here, we only have access to color images, so we will have to generate our own black and white images. The file data_utils.py contains some useful functions to load the dataset. In particuler given a dataset containing landscape images, the function get_colorized_dataset_loader returns a PyTorch DataLoader object that can be used to iterate over the dataset yielding batches of black and white images and their corresponding colorized version.","title":"Data"},{"location":"developpement/colorize.html#the-network-architecture","text":"We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = F.sigmoid(self.last_conv(x)) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python model.py","title":"The network architecture"},{"location":"developpement/colorize.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the train.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Some of you may have GPUs on their local machine. If that is the case, you can use them to train your model. If not, you can use Google Colab to train your model on a GPU for free. If you are using Google Colab, you are expected to do all the code development on your local machine and then send your code to collab to train your model. Try to run your code on your local machine for one or two minibatches to check that everything is working. If it is the case, you can send your code to Google Colab to train your model. To do so: - Open the run_in_colab.ipynb notebook in Google Colab. - Make sure you are connected to a GPU runtime. - Run the first cell to download the dataset. - Upload the files data_utils.py , model.py and train.py to the code folder in Google Colab. - Run the second cell to launch a tensorboard instance. - Run the third cell to launch the training. - Download the trained model and the tensorboard logs to your local machine.","title":"Training script"},{"location":"developpement/colorize.html#api-web-app-and-deployment","text":"Complete the colorize_api.py file to create a Flask API that will colorize images. The API should have a /colorize endpoint that will take a black and white image as input and return the colorized version of the image. You can use the test_colorize_api.py file to test your API. You can test your app with random balck and white images from the net. For exemple one of these . You can also test your api using Postman. To do so: - Install Postman . - Launch your API. - Open Postman. - Create a new request. - Set the request type to POST. - Set the request URL to your API URL. - Go to the Body tab. - Select binary as the body type. - Select a black and white image on your computer. - Click on send. Do you have any idea why the colors are so dull? You can also complete the colorize_web_app.py file to create a web app that will colorize images. Finally complete the Dockerfile file to deploy your app on a local server. DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE.","title":"API, Web app and deployment"},{"location":"developpement/colorize.html#solutions","text":"","title":"Solutions:"},{"location":"developpement/colorize.html#modelpy","text":"","title":"model.py"},{"location":"developpement/colorize.html#trainpy","text":"","title":"train.py"},{"location":"developpement/colorize.html#colorize_apipy","text":"","title":"colorize_api.py"},{"location":"developpement/colorize.html#colorize_web_apppy","text":"","title":"colorize_web_app.py"},{"location":"developpement/course.html","text":"Course slides:","title":"Course slides"},{"location":"developpement/course.html#course-slides","text":"","title":"Course slides:"},{"location":"developpement/docker.html","text":"Development for Data Scientist: Docker In this practical session, you will now learn how to run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. Make sure to have finished the previous practical session before starting this one. To make our full application work, we want to package both the Gradio application and the API into a two containers. To do so, we will create two Dockerfiles, one for the API and one for the Gradio application. We will then use docker-compose to run both the API and the Gradio application in a single container. This is the plan, but it's easier to make and test both containers separately when debugging. We will then create independent containers for each application and test them separately and then we will use docker-compose to run them together. Let's start by creating the Dockerfile corresponding to the API. API container Requirements Our API just need the following packages: torch , torchvision , flask , pillow , numpy . Create a new file named requirements-api.txt containing the following code: torch==2.0.1 torchvision==0.15.2 flask==2.3.2 pillow==10.0.0 numpy==1.24.4 It is a good practice to specify the version of the packages you are using to make sure to get the same environment eveytime you build the image. Dockerfile We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile-api containing the following code (update the path to the model file if needed): # Use an official Python runtime as the parent image FROM python:3.10-slim # Set the working directory in the container to /app WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app # Install any needed packages specified in requirements.txt RUN pip install --trusted-host pypi.python.org -r requirements-api.txt # Make port 5075 available to the world outside this container EXPOSE 5075 # Define environment variable for Flask to run in production mode ENV FLASK_ENV=production # Run mnist_api.py when the container launches CMD [\"python\", \"mnist_api.py\", \"--model_path\", \"weights/mnist_net.pth\"] Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image with python 3.10. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The python 3.10 image we will be using is available here . You can have a look at its corresponding dockerfile here . Remmeber that docker images are organized by layers. This means that our image will be built upon the python 3.10 image and will contain all the dependencies you could find in the base image. API Image If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -f Dockerfile-api -t mnist-flask-app . -f is used to specify the Dockerfile to use. -t is used to specify the name of the image. . is used to specify the context of the build. In our case, the context is the current directory. The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? If you remember, this is not the ideal way to build a dockerfile. If we need to modify our code, we will have to rebuild the image and it will be re-downloaded all the dependencies. Try to add a comment in the mnist_api.py file with an empty line and to rebuild the image. This is probably quite long. Try to solve this problem by moving the part where you install the dependencies to the Dockerfile. API Container Now that our images are built, we can use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: sudo docker run --rm -p 5075:5075 --name mnist mnist-flask-app -p is used to specify the port mapping. This means that the container port 5075 (the port on which the API is running) will be accessible on port 5075 of the host machine. --rm is used to remove the container when it stops. This is useful to avoid leaving orphaned containers behind. --name is used to specify the name of the container. mnist-flask-app is the name of the image we built in the previous step. mnist is the name of the container. On a separate terminal, you can list all your running containers using the following command: sudo docker container ls Quit the container using ctrl+c . Your container is closed and does not appear when you list all the containers. Restart a container using the following command: sudo docker run --rm -p 5075:5075 --name mnist mnist-flask-app We will now test the API from the container. If everything is working, you should be able to request your API using the notebook test_api.ipynb . Check that everything is working is OK. Gradio container Requirements Our Gradio application just need the following packages: pillow , gradio , numpy . Create a new file named requirements-gradio.txt containing the following code: pillow==10.3.0 gradio==5.49 numpy==1.26.4 Dockerfile We will now create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile-gradio containing the following code: FROM python:3.10-slim WORKDIR /app COPY requirements_gradio.txt . RUN pip install --no-cache-dir -r requirements_gradio.txt # App code COPY . /app # Gradio uses 7860 by default EXPOSE 7860 # Immediate logging without -t ENV PYTHONUNBUFFERED=1 # (FLASK_ENV isn't used by Gradio; safe to drop) # ENV FLASK_ENV=production CMD [\"python\", \"mnist_gradio.py\"] Take a moment to analyze this dockerfile. Gradio Image Let's modify a little bit the gradio application to use the API from the localhost. To do so, we need to change the base URL for API requests in the mnist_gradio.py file. Change the following line: response = requests.post(\"http://:5075/predict\", data=img_binary.getvalue()) to response = requests.post(\"http://host.docker.internal:5075/predict\", data=img_binary.getvalue()) By doing so, the API will be accessible to the container from the host machine. Now you can build the image using the following command: sudo docker build -f Dockerfile-gradio -t mnist-gradio-app . Gradio Container Once the image is built, you can run the container using the following command (windows and macos): sudo docker run --rm -p 7860:7860 --name mnist mnist-gradio-app For Linux users: sudo docker run --rm --add-host=host.docker.internal:host-gateway -p 7860:7860 --name mnist mnist-gradio-app On another terminal, run the API locally and check that the Gradio application is working ( http://localhost:7860 ): python mnist_api.py Docker compose At this point, we have two containers running well independently. It's time to run them together. To do so, we will use docker-compose. Create a new file named docker-compose.yml containing the following code: version: '3.8' # specify docker-compose version services: # services to run api: # name of the first service build: context: . # specify the directory of the Dockerfile dockerfile: Dockerfile-api # specify the Dockerfile name ports: - \"5075:5075\" # specify port mapping gradio-app: build: context: . # specify the directory of the Dockerfile dockerfile: Dockerfile-gradio # specify the Dockerfile name ports: - \"7860:7860\" # specify port mapping depends_on: - api # specify service dependencies We are creating two services: - api : the API service - gradio-app : the Gradio application service The depends_on directive is used to specify that the Gradio application service depends on the API service. This means that the Gradio application service will not start until the API service is running. Both containers are exposed on the host machine on the ports 5075 and 7860 and will be able to communicate with each other. Thus let's revert the change we made in the mnist_gradio.py file to use the API from the localhost. Change the following line: response = requests.post(\"http://host.docker.internal:5075/predict\", data=img_binary.getvalue()) to response = requests.post(\"http://0.0.0.0:5075/predict\", data=img_binary.getvalue()) To run the application, run the following command: sudo docker-compose up If everything is working, you should be able to access the Gradio application at http://localhost:7860 . To delete the containers, run the following command: sudo docker-compose down That's it! Now you have a dockerized application that you can deploy on any machine that has docker installed. Do not hesitate to play a little more with Docker. For instance try to train the MNIST classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Practical session 2"},{"location":"developpement/docker.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/docker.html#docker","text":"In this practical session, you will now learn how to run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. Make sure to have finished the previous practical session before starting this one. To make our full application work, we want to package both the Gradio application and the API into a two containers. To do so, we will create two Dockerfiles, one for the API and one for the Gradio application. We will then use docker-compose to run both the API and the Gradio application in a single container. This is the plan, but it's easier to make and test both containers separately when debugging. We will then create independent containers for each application and test them separately and then we will use docker-compose to run them together. Let's start by creating the Dockerfile corresponding to the API.","title":"Docker"},{"location":"developpement/docker.html#api-container","text":"","title":"API container"},{"location":"developpement/docker.html#requirements","text":"Our API just need the following packages: torch , torchvision , flask , pillow , numpy . Create a new file named requirements-api.txt containing the following code: torch==2.0.1 torchvision==0.15.2 flask==2.3.2 pillow==10.0.0 numpy==1.24.4 It is a good practice to specify the version of the packages you are using to make sure to get the same environment eveytime you build the image.","title":"Requirements"},{"location":"developpement/docker.html#dockerfile","text":"We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile-api containing the following code (update the path to the model file if needed): # Use an official Python runtime as the parent image FROM python:3.10-slim # Set the working directory in the container to /app WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app # Install any needed packages specified in requirements.txt RUN pip install --trusted-host pypi.python.org -r requirements-api.txt # Make port 5075 available to the world outside this container EXPOSE 5075 # Define environment variable for Flask to run in production mode ENV FLASK_ENV=production # Run mnist_api.py when the container launches CMD [\"python\", \"mnist_api.py\", \"--model_path\", \"weights/mnist_net.pth\"] Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image with python 3.10. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The python 3.10 image we will be using is available here . You can have a look at its corresponding dockerfile here . Remmeber that docker images are organized by layers. This means that our image will be built upon the python 3.10 image and will contain all the dependencies you could find in the base image.","title":"Dockerfile"},{"location":"developpement/docker.html#api-image","text":"If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -f Dockerfile-api -t mnist-flask-app . -f is used to specify the Dockerfile to use. -t is used to specify the name of the image. . is used to specify the context of the build. In our case, the context is the current directory. The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? If you remember, this is not the ideal way to build a dockerfile. If we need to modify our code, we will have to rebuild the image and it will be re-downloaded all the dependencies. Try to add a comment in the mnist_api.py file with an empty line and to rebuild the image. This is probably quite long. Try to solve this problem by moving the part where you install the dependencies to the Dockerfile.","title":"API Image"},{"location":"developpement/docker.html#api-container_1","text":"Now that our images are built, we can use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: sudo docker run --rm -p 5075:5075 --name mnist mnist-flask-app -p is used to specify the port mapping. This means that the container port 5075 (the port on which the API is running) will be accessible on port 5075 of the host machine. --rm is used to remove the container when it stops. This is useful to avoid leaving orphaned containers behind. --name is used to specify the name of the container. mnist-flask-app is the name of the image we built in the previous step. mnist is the name of the container. On a separate terminal, you can list all your running containers using the following command: sudo docker container ls Quit the container using ctrl+c . Your container is closed and does not appear when you list all the containers. Restart a container using the following command: sudo docker run --rm -p 5075:5075 --name mnist mnist-flask-app We will now test the API from the container. If everything is working, you should be able to request your API using the notebook test_api.ipynb . Check that everything is working is OK.","title":"API Container"},{"location":"developpement/docker.html#gradio-container","text":"","title":"Gradio container"},{"location":"developpement/docker.html#requirements_1","text":"Our Gradio application just need the following packages: pillow , gradio , numpy . Create a new file named requirements-gradio.txt containing the following code: pillow==10.3.0 gradio==5.49 numpy==1.26.4","title":"Requirements"},{"location":"developpement/docker.html#dockerfile_1","text":"We will now create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile-gradio containing the following code: FROM python:3.10-slim WORKDIR /app COPY requirements_gradio.txt . RUN pip install --no-cache-dir -r requirements_gradio.txt # App code COPY . /app # Gradio uses 7860 by default EXPOSE 7860 # Immediate logging without -t ENV PYTHONUNBUFFERED=1 # (FLASK_ENV isn't used by Gradio; safe to drop) # ENV FLASK_ENV=production CMD [\"python\", \"mnist_gradio.py\"] Take a moment to analyze this dockerfile.","title":"Dockerfile"},{"location":"developpement/docker.html#gradio-image","text":"Let's modify a little bit the gradio application to use the API from the localhost. To do so, we need to change the base URL for API requests in the mnist_gradio.py file. Change the following line: response = requests.post(\"http://:5075/predict\", data=img_binary.getvalue()) to response = requests.post(\"http://host.docker.internal:5075/predict\", data=img_binary.getvalue()) By doing so, the API will be accessible to the container from the host machine. Now you can build the image using the following command: sudo docker build -f Dockerfile-gradio -t mnist-gradio-app .","title":"Gradio Image"},{"location":"developpement/docker.html#gradio-container_1","text":"Once the image is built, you can run the container using the following command (windows and macos): sudo docker run --rm -p 7860:7860 --name mnist mnist-gradio-app For Linux users: sudo docker run --rm --add-host=host.docker.internal:host-gateway -p 7860:7860 --name mnist mnist-gradio-app On another terminal, run the API locally and check that the Gradio application is working ( http://localhost:7860 ): python mnist_api.py","title":"Gradio Container"},{"location":"developpement/docker.html#docker-compose","text":"At this point, we have two containers running well independently. It's time to run them together. To do so, we will use docker-compose. Create a new file named docker-compose.yml containing the following code: version: '3.8' # specify docker-compose version services: # services to run api: # name of the first service build: context: . # specify the directory of the Dockerfile dockerfile: Dockerfile-api # specify the Dockerfile name ports: - \"5075:5075\" # specify port mapping gradio-app: build: context: . # specify the directory of the Dockerfile dockerfile: Dockerfile-gradio # specify the Dockerfile name ports: - \"7860:7860\" # specify port mapping depends_on: - api # specify service dependencies We are creating two services: - api : the API service - gradio-app : the Gradio application service The depends_on directive is used to specify that the Gradio application service depends on the API service. This means that the Gradio application service will not start until the API service is running. Both containers are exposed on the host machine on the ports 5075 and 7860 and will be able to communicate with each other. Thus let's revert the change we made in the mnist_gradio.py file to use the API from the localhost. Change the following line: response = requests.post(\"http://host.docker.internal:5075/predict\", data=img_binary.getvalue()) to response = requests.post(\"http://0.0.0.0:5075/predict\", data=img_binary.getvalue()) To run the application, run the following command: sudo docker-compose up If everything is working, you should be able to access the Gradio application at http://localhost:7860 . To delete the containers, run the following command: sudo docker-compose down That's it! Now you have a dockerized application that you can deploy on any machine that has docker installed. Do not hesitate to play a little more with Docker. For instance try to train the MNIST classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Docker compose"},{"location":"developpement/git_intro.html","text":"Introduction to git Slides A useful cheatsheet: https://training.github.com/downloads/github-git-cheat-sheet.pdf A useful reference: https://git-scm.com/docs Practical session: Basic Git Workflow and Introduction to and CI/CD Introduction: This practical session is designed for small groups of 2-3 people to work through together. It is very important that every member of the group runs the different commands in this practical session all shared with the same repository (we will see later what this means). During this project, you will learn if not already, how to use Git and GitHub to manage your code and to collaborate with your team. You will also learn how to use GitHub Actions to deploy your code and to run tests. This practical session is not a full Git and GitHub course, but rather a practical introduction to these tools providing you with the basic knowledge to be able to use them during this project. If you want to learn more about Git and GitHub, you can refer to the official documentation . Understanding Version Control Version control is a system that helps track and manage changes to files over time. It's an essential tool for software development, enabling developers to collaborate effectively, maintain a history of changes, and manage different versions of their code. With version control, you can revert files to a previous state, compare changes over time, see who last modified something that might be causing issues, and more. It acts as a safety net, allowing developers to experiment with new ideas without fear of losing their original work. Example: Imagine you're working on a Python application for a bookstore. Over time, you add several features, but one of them turns out to be problematic. With version control, you can easily remove just the unwanted feature while keeping the other improvements. Let's see how this might look: Day 1: You start with a basic Book class. class Book: def __init__(self, title, author, price): self.title = title self.author = author self.price = price def display_info(self): return f\"{self.title} by {self.author}: ${self.price}\" Day 3: You add a discount feature. class Book: def __init__(self, title, author, price): self.title = title self.author = author self.price = price def display_info(self): return f\"{self.title} by {self.author}: ${self.price}\" def apply_discount(self, discount): self.price = self.price * (1 - discount) Day 5: You add a feature to track stock, but it has a bug. class Book: def __init__(self, title, author, price, stock): self.title = title self.author = author self.price = price self.stock = stock def display_info(self): return f\"{self.title} by {self.author}: ${self.price} (In stock: {self.stock})\" def apply_discount(self, discount): self.price = self.price * (1 - discount) def update_stock(self, quantity): self.stock += quantity # Bug: This allows negative stock! Day 7: You add a method to calculate total value. class Book: def __init__(self, title, author, price, stock): self.title = title self.author = author self.price = price self.stock = stock def display_info(self): return f\"{self.title} by {self.author}: ${self.price} (In stock: {self.stock})\" def apply_discount(self, discount): self.price = self.price * (1 - discount) def update_stock(self, quantity): self.stock += quantity # Bug: This allows negative stock! def total_value(self): return self.price * self.stock Day 10: You realize the stock tracking feature has a bug allowing negative stock. With version control, you can: 1. Review the history of changes to identify when the bug was introduced. 2. Revert just the problematic update_stock method while keeping the other beneficial changes. 3. Fix the bug and commit the corrected version. Modern development teams use version control systems to manage their code, track changes, and collaborate effectively. Git is one of the most popular version control systems, providing a robust set of features for managing codebases of all sizes. Git Git is a distributed version control system that has revolutionized how developers manage and track changes in their code. Created by Linus Torvalds in 2005, Git was born out of the need for a fast, efficient, and reliable system to manage the development of the Linux kernel. Unlike centralized version control systems that preceded it, Git allows developers to have a complete copy of the project history on their local machines, enabling offline work and providing a safeguard against data loss. Git's distributed nature facilitates collaboration among developers, allowing them to work on different features simultaneously and merge their changes seamlessly. Its branching and merging capabilities make it easy to experiment with new ideas without affecting the main codebase. Since its inception, Git has become the de facto standard for version control in software development, used by millions of developers and organizations worldwide. Git's Basic Paradigms Snapshots, Not Differences : Unlike other version control systems that store data as changes to a base version of each file, Git thinks of its data more like a series of snapshots of a miniature filesystem. Every time you commit, or save the state of your project in Git, it basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot. Local Operations : Most operations in Git need only local files and resources to operate. This means you can work on your project even when you're offline or not on a VPN, unlike centralized systems that need to communicate with a server for almost every operation. Integrity : Everything in Git is check-summed before it is stored and is then referred to by that checksum. This means it's impossible to change the contents of any file or directory without Git knowing about it. This functionality is built into Git at the lowest levels and is integral to its philosophy. Branching Model : Git's branching model is its \"killer feature.\" Unlike many other VCSs, Git encourages workflows that branch and merge often. This allows for feature branches, experimentation, and parallel development streams that can be easily merged when ready. Distributed Development : In Git, every developer's working copy of the code is also a repository that can contain the full history of all changes. This allows for multiple backup copies and various collaborative development models. Fast and Lightweight : Git is designed to be fast and efficient with large projects. Most operations are local, reducing the overhead of communicating with a centralized server. The Three States in Git In Git, files can exist in three states: Modified : You have changed the file but have not committed it to your database yet. Staged : You have marked a modified file in its current version to go into your next commit snapshot. Committed : The data is safely stored in your local database. These three states correspond to the three main sections of a Git project. When not tracked by Git, a file is in the Untracked state. Git Areas and Workflow Understanding the different areas in Git is crucial for mastering its workflow. Git manages your project's files through four main areas: Working Directory (Working Tree) : This is where you actually work on your project files. It's a single checkout of one version of the project. These files are pulled out of the compressed database in the Git directory and placed on disk for you to use or modify. Staging Area (Index) : This is a file, generally contained in your Git directory, that stores information about what will go into your next commit. It's sometimes referred to as the \"Index\". Think of it as a prep area for your next commit. Files are added to this area with the git add command. Local Repository : This is where Git stores the metadata and object database for your project. It's what's copied when you clone a repository from another computer. The local repository contains all of your committed changes. It's located in the .git directory of your project. Remote Repository : This is a version of your project that is hosted on the Internet or network somewhere (like GitHub, GitLab, or Bitbucket). You can have several of them, each of which generally is either read-only or read/write for you. Collaborating with others involves managing your remote repositories and pushing and pulling data to and from them when you need to share work. Basic Git Workflow: You modify files in your Working Directory. You stage the files, adding snapshots of them to your Staging Area. You do a commit , which takes the files as they are in the Staging Area and stores that snapshot permanently to your Local Repository. You push your changes to a Remote Repository to share with others or as a backup. Understanding these areas and how they interact is key to understanding Git's workflow and effectively managing your projects with version control. GitHub GitHub is a web-based hosting service for Git repositories. Launched in 2008, it has become the world's largest host of source code and a central hub for collaboration among developers. While Git is a command-line tool, GitHub provides a web-based graphical interface. It also offers access control and several collaboration features, such as bug tracking, feature requests, task management, and wikis for every project. Key features of GitHub include: Repository Hosting : GitHub can host your Git repositories in the cloud, making it easy to share and collaborate on code. Fork and Pull Request : Users can \"fork\" an existing repository (creating their own copy), make changes, and then submit a \"pull request\" to propose those changes back to the original project. Issue Tracking : GitHub provides a system for reporting and managing bugs, feature requests, and other tasks related to a project. Project Management Tools : Including project boards, milestones, and other tools to help manage and organize work on repositories. Social Coding : Users can follow repositories and other users, star repositories they like, and see a feed of activity from repositories and users they're interested in. GitHub Pages : A feature that allows hosting of static websites directly from a GitHub repository. Integrations : GitHub can integrate with many third-party services, enhancing its capabilities for things like continuous integration and deployment. GitHub has played a significant role in the growth of open-source software, providing a platform where developers from around the world can collaborate on projects. It's used not only by individual developers and open-source projects but also by large companies to host and manage their code. Setting up your environment Anaconda and Python: Download and install Anaconda from the official website: https://www.anaconda.com/products/distribution During installation, make sure to add Anaconda to your PATH environment variable when prompted. Open an Anaconda Prompt (on Windows) or a terminal (on macOS/Linux). Create a new environment for this project: conda create --name gitproject python=3.8 Activate the environment: conda activate gitproject Install Flask (we'll use it for examples during the session): pip install flask Git 1. Download Git from the official website: https://git-scm.com/downloads 2. Follow the installation instructions for your operating system. 3. After installation, open a new terminal or command prompt and verify the installation: git --version Configure your Git username and email: git config --global user.name \"Your Name\" git config --global user.email \"your.email@example.com\" GitHub We will use GitHub as our remote repository for this project. If you don't have a GitHub account, you can create one for free at https://github.com/. Remote repository (Just one person in the group should do this and invite others to collaborate) Log in to GitHub. Click the \"new\" button on the left menu. Name your repository (e.g., \"flask-git-demo\"). Choose to make it public. Initialize the repository with a README file. Click \"Create project\". To invite collaborators: Go to your repository's page. On the left menu, click on \"Members\" and invite the other members of the group. Give them the \"Developer\" role. Local repository Open a terminal or command prompt. Navigate to the directory where you want to create your project: cd path/to/your/project/directory Clone the remote repository: git clone https://github.com/your-username/flask-git-demo.git (Replace 'your-username' with the GitHub username of the person who created the repository) 4. Navigate into the cloned repository: cd flask-git-demo Now your environment is set up and ready for the Git and GitHub practical session. Each member of the group should have Python, Flask, and Git installed, a GitHub account, and a local copy of the repository. The repository owner has set up the remote repository and invited other group members as collaborators. Practical session: A Flask Web App Development with Git and GitHub We'll build a Flask web app that collects a user's name and date of birth, then displays various information based on this input. We'll develop this app in stages, using Git and GitHub to manage our development process. Stage 1: Basic Setup and First Commit - Explained Guide In this stage, we'll set up a basic Flask app within our existing Git repository. To avoid conflicts, only one person (the team lead) should perform the initial setup and push the changes. Other team members will then pull these changes. For the Team Lead: 1. Navigate to the Project Directory First, we need to ensure we're in the correct directory for our project. cd path/to/flask-git-demo This command changes the current directory to your project folder. 2. Create the Flask App Now, we'll create the basic structure for our Flask application. Create a new file called app.py : touch app.py # touch is a command that creates an empty file # on windows just create it manually This command creates an empty file named app.py . Open app.py in your preferred text editor and add the following code: from flask import Flask, render_template, request from datetime import datetime app = Flask(__name__) @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] welcome_message = f\"Welcome, {name}! Your date of birth is {dob}.\" return render_template('result.html', message=welcome_message) return render_template('index.html') if __name__ == '__main__': app.run(debug=True) This code sets up a basic Flask app with a route that handles both GET and POST requests. Create a new directory for our HTML templates: mkdir templates # mkdir is a command that creates a new directory # on windows just create it manually This command creates a new directory named \"templates\", where Flask will look for our HTML files. Create two HTML files in the templates directory: For index.html : touch templates/index.html # on windows just create it manually Then add the HTML content for the form in your text editor. <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Birthday App</title> <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"> </head> <body> <h1>Welcome to the Birthday App</h1> <form method=\"POST\"> <label for=\"name\">Name:</label> <input type=\"text\" id=\"name\" name=\"name\" required> <label for=\"dob\">Date of Birth:</label> <input type=\"date\" id=\"dob\" name=\"dob\" required> <input type=\"submit\" value=\"Submit\"> </form> </body> </html> For result.html : touch templates/result.html # on windows just create it manually Then add the HTML content for the result page in your text editor. <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Birthday App Result</title> <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"> </head> <body> <h1>Your Birthday Information</h1> <p>{{ message }}</p> <a href=\"{{ url_for('index') }}\">Go back</a> </body> </html> 3. Update .gitignore To keep our repository clean, we'll update the .gitignore file to exclude unnecessary files. echo \"venv/\" >> .gitignore echo \"__pycache__/\" >> .gitignore echo \"*.pyc\" >> .gitignore These commands append new lines to the .gitignore file, telling Git to ignore the virtual environment directory, Python cache files, and compiled Python files. By adding these files to the .gitignore file, we ensure that they are not tracked by Git, meaning they won't be committed to the repository. 4. Git Operations Now we'll use Git to track our new files and push them to the remote repository. Check the status of your repository: git status This command shows you which files have been changed or are untracked. Add the new files to the staging area: git add . This stages all new and modified files, preparing them for commit. Check the status again to verify that the files have been staged: git status Commit the changes: git commit -m \"Add basic Flask app structure\" This creates a new commit with the staged changes and adds a descriptive message. Check the status again to verify that the commit has been created: git status Push the changes to the remote repository: git push origin main This uploads your local commits to the remote repository on GitHub. Check the status again to verify that the commit has been pushed: git status 5. Testing the App Before notifying your team, make sure the app works as expected: python app.py This command runs your Flask application. Open a web browser and go to http://127.0.0.1:5000/ to verify the app is functioning correctly. 6. Notify Team Members Once you've successfully pushed the changes and tested the app, notify your team members that the initial setup is complete and they can proceed with their steps. For Other Team Members: After the team lead has completed the setup and pushed the changes, follow these steps: Pull the latest changes from the remote repository: git pull origin main This command fetches the latest changes from the remote repository and merges them into your local branch. Test the app to ensure it's working on your local machine: python app.py This runs the Flask application. Check http://127.0.0.1:5000/ in your browser to verify it's working correctly. Collaboration Notes Always pull the latest changes before starting work each day: git pull origin main This ensures you're working with the most up-to-date version of the project. If you encounter any issues or merge conflicts, communicate with your team to resolve them. Remember to commit your changes frequently with meaningful commit messages as you start working on new features in the upcoming stages. In the next stages, we'll add more features to this app and explore more Git and GitHub concepts as a team. Stage 2: Implementing Age Calculation - Direct Implementation In this stage, we'll add a feature to calculate the user's age based on their date of birth. We'll implement this directly on the main branch to practice basic Git workflows. For the Student Lead of Stage 2: 1. Prepare Your Local Repository Ensure your local repository is up-to-date: git pull origin main This fetches and merges the latest changes from the main branch. 2. Implement Age Calculation Open app.py and add a new function to calculate age: from datetime import datetime def calculate_age(dob): today = datetime.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\") age = today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day)) return age Modify the index function to use this new calculation: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) welcome_message = f\"Welcome, {name}! You are {age} years old.\" return render_template('result.html', message=welcome_message) return render_template('index.html') 3. Test Your Changes Run the Flask application to test your changes: python app.py Verify that the age calculation works correctly by submitting the form and checking the result. 4. Commit Your Changes After confirming that everything works: git add app.py git commit -m \"Add age calculation feature\" This stages and commits your changes to your local main branch. 5. Push Your Changes Push your commits to the remote repository: git push origin main This updates the main branch on GitHub with your new changes. 6. Notify Team Members Let your team know that you've pushed new changes to the main branch. For Other Team Members: After the student lead has pushed the changes: Pull the latest changes from the remote repository: git pull origin main This updates your local main branch with the new changes. Test the updated application to ensure everything works correctly: python app.py If you encounter any issues, communicate with the team to resolve them. Stage 3: Adding Zodiac Sign Feature - Branching and Pull Requests Now that we've implemented a feature directly on the main branch, let's discuss the concept of branching and why it's useful: Branching in Git allows developers to diverge from the main line of development and work independently on features or experiments without affecting the main codebase. This has several advantages: Isolation : You can work on different features or experiments without interfering with the main codebase or other developers' work. Easier collaboration : Multiple developers can work on different features simultaneously without conflicts. Code review : Branches facilitate code reviews through pull requests before merging changes into the main codebase. Experimentation : You can try out ideas without the risk of breaking the main codebase. In this stage, we'll add a feature to determine the user's zodiac sign based on their date of birth. We'll use Git branching and create a pull request to implement this feature, demonstrating a more advanced Git workflow. For the Student Lead of Stage 3: 1. Ensure Your Repository is Up-to-Date First, make sure you're on the main branch and it's up-to-date: git checkout main git pull origin main 2. Create a New Branch Create and switch to a new branch for the zodiac sign feature: git checkout -b feature/zodiac-sign 3. Implement the Zodiac Sign Feature In app.py , add a new function to determine the zodiac sign: def get_zodiac_sign(dob): month, day = map(int, dob.split('-')[1:]) zodiac_signs = [ (1, 20, \"Capricorn\"), (2, 19, \"Aquarius\"), (3, 20, \"Pisces\"), (4, 20, \"Aries\"), (5, 21, \"Taurus\"), (6, 21, \"Gemini\"), (7, 22, \"Cancer\"), (8, 23, \"Leo\"), (9, 23, \"Virgo\"), (10, 23, \"Libra\"), (11, 22, \"Scorpio\"), (12, 22, \"Sagittarius\"), (12, 31, \"Capricorn\") ] for m, d, sign in zodiac_signs: if (month, day) <= (m, d): return sign Modify the index function to include the zodiac sign: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) message = f\"Welcome, {name}! You are {age} years old. Your zodiac sign is {zodiac}.\" return render_template('result.html', message=message) return render_template('index.html') 4. Test Your Changes Run the Flask application and test the new feature: python app.py 5. Commit Your Changes After ensuring everything works: git add app.py git commit -m \"Add zodiac sign feature\" 6. Push Your Branch to GitHub Push your feature branch to the remote repository: git push -u origin feature/zodiac-sign 7. Create a Pull Request Go to your repository on GitHub. You should see a prompt to create a pull request for your recently pushed branch. Click on it. Fill in the details of your pull request, describing the new zodiac sign feature. Assign team members to review your pull request. For Other Team Members (Reviewers): Reviewing the Pull Request Go to the repository on GitHub and navigate to the Pull Requests tab. Click on the pull request for the zodiac sign feature. Review the changes: Check the code for correctness and style. Consider how this feature integrates with the existing codebase. To test the changes locally: git fetch origin # to update the local repository with the latest changes from the remote repository git checkout feature/zodiac-sign # to switch to the feature branch python app.py # to test the changes locally Leave comments or request changes if necessary. If everything looks good, approve the pull request. After the Pull Request is Merged Once the pull request is approved and merged: Switch back to the main branch: git checkout main Pull the latest changes: git pull origin main Test the updated application to ensure everything works correctly. In the next stage, we'll continue to build on our app and explore more advanced Git and GitHub features, such as handling merge conflicts. Stage 4: Enhancing the UI - Git Stash and GitHub Issues In this stage, we'll improve the user interface of our Flask application by adding some basic CSS. We'll also learn about Git stash for managing temporary changes and use GitHub Issues for task tracking. For the Project Manager (can be any team member): 1. Create GitHub Issues Go to your GitHub repository and navigate to the \"Issues\" tab. Create a new issue titled \"Enhance UI with CSS\". In the description, outline the following tasks: Add a CSS file for styling Style the form on the index page Improve the layout of the result page Add labels like \"enhancement\" and \"ui\". Assign the issue to a team member. For the Assigned Team Member: 1. Set Up Your Work Environment Ensure your local repository is up-to-date: git checkout main git pull origin main 2. Create a New Branch Create a branch for the UI enhancements: git checkout -b feature/ui-enhancement 3. Add CSS File Create a new directory named static in your project root: mkdir static Create a new CSS file: touch static/style.css Add some basic CSS to style.css : body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; } h1 { color: #333; } form { background-color: #fff; padding: 20px; border-radius: 5px; box-shadow: 0 0 10px rgba(0,0,0,0.1); } input[type=\"text\"], input[type=\"date\"] { width: 100%; padding: 8px; margin-bottom: 10px; border: 1px solid #ddd; border-radius: 4px; } input[type=\"submit\"] { background-color: #333; color: #fff; border: none; padding: 10px 20px; cursor: pointer; border-radius: 4px; } input[type=\"submit\"]:hover { background-color: #555; } 4. Test Your Changes Run the Flask application and verify the UI improvements: python app.py 5. Demonstrate Git Stash Let's say you notice a small bug in the age calculation while working on the UI. Here's how to use Git stash: Make a small change in app.py to fix the bug. Instead of committing this change, use Git stash: git stash save \"Fix age calculation bug\" Your working directory is now clean and back to the state of the last commit. To apply the stashed changes later: git stash pop 6. Commit UI Changes Now, commit your UI enhancements: git add static/style.css templates/index.html templates/result.html git commit -m \"Enhance UI with CSS styling\" 7. Push and Create Pull Request Push your branch and create a merge request: git push -u origin feature/ui-enhancement Create a pull request on GitHub, referencing the issue number in the description (e.g., \"Closes #1\"). For Reviewers: Review the pull request, checking both the code and the visual changes. Test the changes locally if necessary. Provide feedback or approve the changes. After Merging: Close the GitHub issue once the pull request is merged. All team members should pull the latest changes: git checkout main git pull origin main Discuss as a team: 1. How did using GitHub Issues help in organizing the task? 2. What was your experience with Git stash? How might it be useful in other scenarios? 3. How has the workflow evolved from the earlier stages of the project? In the next stage, we'll implement a more complex feature and explore handling merge conflicts. Stage 5: Merging and Rebasing with Conflicts Git offers two primary ways to integrate changes from one branch into another: merging and rebasing. Merge Merging creates a new commit that combines the tips of two branches. This method preserves the entire history of both branches, providing a non-destructive operation that maintains a clear record of when branches diverged and were integrated. The resulting history shows the parallel development that occurred, with a visible branching structure. Merging is particularly safe for shared branches as it doesn't rewrite history. However, in projects with frequent merges, this can lead to a more complex history that may be harder to follow. Rebase Rebasing, on the other hand, moves the entire feature branch to begin on the tip of the main branch, effectively replaying your work on top of it. This results in a linear project history, as if the work was done sequentially rather than in parallel. Rebasing creates a cleaner, more streamlined history which can make it easier to track features. However, it achieves this by rewriting the project history, which can be problematic if the rebased branch has been shared with others. As such, rebasing requires more care when used on public or shared branches. The choice between merging and rebasing often depends on the specific needs of your project and team. Merging is generally preferred for preserving complete history and for work on public branches, while rebasing is often used to maintain a cleaner history, especially for local branches or before merging a feature branch into the main line of development. In this stage, we'll practice merging and rebasing, including handling conflicts. We'll do this by creating a new feature branch, making changes to the main branch, and then exploring both merge and rebase workflows. Part 1: Merging with Conflicts For Developer A: Ensure your main branch is up-to-date: git checkout main git pull origin main Create a new branch for a feature: git checkout -b feature/birthday-countdown Implement the birthday countdown feature in app.py : from datetime import datetime, date def days_to_birthday(dob): today = date.today() dob = datetime.strptime(dob, \"%Y-%m-%d\").date() next_birthday = date(today.year, dob.month, dob.day) if next_birthday < today: next_birthday = date(today.year + 1, dob.month, dob.day) return (next_birthday - today).days @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) message = f\"Welcome, {name}! You are {age} years old. Your zodiac sign is {zodiac}. There are {days_to_bday} days until your next birthday!\" return render_template('result.html', message=message) return render_template('index.html') Commit your changes: git add app.py git commit -m \"Add birthday countdown feature\" For Developer B: Make sure you're on the main branch and it's up-to-date: git checkout main git pull origin main Make a change to the index function in app.py : @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) message = f\"Hello, {name}! Your age is {age} and your zodiac sign is {zodiac}.\" return render_template('result.html', message=message) return render_template('index.html') Commit and push this change: git add app.py git commit -m \"Update welcome message format\" git push origin main Back to Developer A: Try to merge the main branch into your feature branch: git checkout feature/birthday-countdown git merge main You'll encounter a merge conflict. Open app.py and you'll see something like: <<<<<<< HEAD message = f\"Welcome, {name}! You are {age} years old. Your zodiac sign is {zodiac}. There are {days_to_bday} days until your next birthday!\" ======= message = f\"Hello, {name}! Your age is {age} and your zodiac sign is {zodiac}.\" >>>>>>> main Resolve the conflict by combining both changes: message = f\"Hello, {name}! Your age is {age} and your zodiac sign is {zodiac}. There are {days_to_bday} days until your next birthday!\" Stage the resolved file, commit the merge, and push: git add app.py git commit -m \"Merge main into feature/birthday-countdown and resolve conflicts\" git push origin feature/birthday-countdown Before moving to the next part, you can merge the feature branch into the main branch, either using a merge request or if you have the right to do it directly: git checkout main git merge feature/birthday-countdown Part 2: Rebasing with Conflicts Now, let's practice rebasing with a similar scenario. For Developer A: Create a new feature branch from main: git checkout main git pull origin main git checkout -b feature/lucky-number Add a lucky number feature to app.py : import random def get_lucky_number(): return random.randint(1, 100) @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) lucky_number = get_lucky_number() message = f\"Hello, {name}! Your age is {age}, your zodiac sign is {zodiac}, and there are {days_to_bday} days until your next birthday. Your lucky number is {lucky_number}!\" return render_template('result.html', message=message) return render_template('index.html') Commit your changes: git add app.py git commit -m \"Add lucky number feature\" For Developer B: Make another change to the main branch: git checkout main git pull origin main Update the index function in app.py : @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) message = f\"Greetings, {name}! You're {age} years old with the zodiac sign {zodiac}. Your next birthday is in {days_to_bday} days.\" return render_template('result.html', message=message) return render_template('index.html') Commit and push this change: git add app.py git commit -m \"Refine welcome message\" git push origin main Back to Developer A: Try to rebase your feature branch onto the updated main: git checkout feature/lucky-number git rebase main You'll encounter a rebase conflict. Open app.py and resolve the conflict: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) lucky_number = get_lucky_number() message = f\"Greetings, {name}! You're {age} years old with the zodiac sign {zodiac}. Your next birthday is in {days_to_bday} days. Your lucky number is {lucky_number}!\" return render_template('result.html', message=message) return render_template('index.html') After resolving the conflict: git add app.py git rebase --continue Force push your rebased branch: git push origin feature/lucky-number --force Make a merge request to merge the feature branch into the main branch. Stage 6: Implementing Tests for the Flask Application In this stage, we'll add tests to our Flask application to ensure its functionality and to practice test-driven development (TDD). We'll write both unit tests for individual functions and integration tests for the application routes. 1. Set Up Testing Environment First, we need to set up our testing environment: Install pytest, a popular testing framework for Python: pip install pytest Create a new file called test_app.py in your project root directory. 2. Writing Unit Tests Let's start by writing unit tests for our existing functions: Open test_app.py and add the following code: from app import calculate_age, get_zodiac_sign, days_to_birthday from datetime import date def test_calculate_age(): assert calculate_age(\"1990-01-01\") == date.today().year - 1990 def test_get_zodiac_sign(): assert get_zodiac_sign(\"1990-01-01\") == \"Capricorn\" assert get_zodiac_sign(\"1990-07-01\") == \"Cancer\" def test_days_to_birthday(): today = date.today() next_birthday = date(today.year, 12, 31) if next_birthday < today: next_birthday = date(today.year + 1, 12, 31) expected_days = (next_birthday - today).days assert days_to_birthday(\"2000-12-31\") == expected_days These tests check the core functionality of our utility functions. 3. Writing Integration Tests Now, let's add integration tests for our Flask routes: Add the following code to test_app.py : import pytest from app import app @pytest.fixture def client(): app.config['TESTING'] = True with app.test_client() as client: yield client def test_home_page(client): response = client.get('/') assert response.status_code == 200 assert b\"Welcome to the Birthday App\" in response.data def test_form_submission(client): response = client.post('/', data={ 'name': 'John Doe', 'dob': '1990-01-01' }, follow_redirects=True) assert response.status_code == 200 assert b\"John Doe\" in response.data assert b\"Your age is\" in response.data assert b\"Your zodiac sign is\" in response.data These tests check that our application's routes are working correctly. 4. Running the Tests To run the tests: In your terminal, navigate to your project directory. Run the following command: pytest You should see output indicating which tests passed or failed. In our case you should see that test_form_submission failed can you see why, and how to fix it? 5. Test-Driven Development: Adding a New Feature Test-Driven Development (TDD) is a software development approach where tests are written before the actual code. The TDD cycle, often referred to as Red-Green-Refactor, consists of three steps: Red: Write a test that fails. This test describes a desired functionality that doesn't exist yet. Green: Write the minimal amount of code necessary to make the test pass. The focus here is on making the test pass, not on writing perfect code. Refactor: Improve the code without changing its functionality. This step is about cleaning up the code, removing duplication, and ensuring it follows good design principles. The benefits of TDD include: Improved code quality: By thinking about how to test the code before writing it, developers often create more modular, flexible, and easier-to-maintain code. Better understanding of requirements: Writing tests first forces developers to clearly understand what the code should do before implementing it. Built-in regression testing: As features are added, the growing suite of tests helps ensure that new changes don't break existing functionality. Documentation: Tests serve as a form of documentation, showing how the code is expected to behave in various scenarios. Confidence in refactoring: With a comprehensive test suite, developers can refactor code with confidence, knowing that if they break something, a test will fail. Let's practice TDD by adding a new feature to determine if it's the user's birthday today. First, write a test for the new function in test_app.py : from app import is_birthday_today def test_is_birthday_today(): today = date.today() assert is_birthday_today(f\"{today.year}-{today.month:02d}-{today.day:02d}\") == True assert is_birthday_today(\"1990-01-01\") == (date.today().month == 1 and date.today().day == 1) Run the tests. The new test should fail because we haven't implemented the function yet. Now, implement the function in app.py : def is_birthday_today(dob): today = date.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\").date() return (today.month, today.day) == (birth_date.month, birth_date.day) Run the tests again. They should all pass now. Finally, update your index route to use this new function: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) is_birthday = is_birthday_today(dob) lucky_number = get_lucky_number() message = f\"Greetings, {name}! You're {age} years old with the zodiac sign {zodiac}. Your next birthday is in {days_to_bday} days. Your lucky number is {lucky_number}!\" if is_birthday: message += \"Happy Birthday!\" else: message += f\"There are {days_to_bday} days until your next birthday.\" return render_template('result.html', message=message) return render_template('index.html') Add a test for this new route behavior in test_app.py : def test_birthday_today(client): today = date.today() response = client.post('/', data={ 'name': 'John Doe', 'dob': f\"{today.year}-{today.month:02d}-{today.day:02d}\" }, follow_redirects=True) assert b\"Happy Birthday!\" in response.data Run the tests one final time to ensure everything is working. Stage 7: Introduction to CI/CD Continuous Integration (CI) and Continuous Deployment (CD) are practices in software development that aim to improve the process of building, testing, and releasing software. Continuous Integration (CI) Continuous Integration is the practice of frequently merging code changes into a shared repository. Each integration is verified by automated builds and tests. The main goals of CI are: Detect and address integration issues early Improve software quality Reduce the time to validate and release new updates With CI, developers integrate their work frequently, usually daily, leading to multiple integrations per day. Each integration triggers automated builds and tests to detect issues quickly. Continuous Deployment (CD) Continuous Deployment takes CI one step further. In CD, every change that passes the automated tests is automatically deployed to production. The main benefits of CD are: Faster release cycles Reduced manual processes and human error More frequent user feedback Improved developer productivity CD can also refer to Continuous Delivery, where changes are automatically deployed to a staging environment but require manual approval for production deployment. Why CI/CD is Useful Faster Bug Detection and Resolution : Issues are caught earlier in the development process, making them easier and less expensive to fix. Improved Collaboration : Frequent integration encourages communication between team members and keeps everyone up to date with changes. Higher Quality Software : Automated testing ensures that tests are run consistently and frequently, catching bugs that might be missed in manual testing. Faster Time to Market : Automating the build, test, and deployment processes reduces the time between writing code and using it in production. Reduced Risk : Smaller, more frequent updates are less risky and easier to roll back if issues occur. Increased Confidence : With a robust CI/CD pipeline, teams can be more confident in the stability and quality of their code. In the following section, we'll implement a basic CI/CD pipeline using GitHub Actions, experiencing firsthand how these practices can improve our development workflow. Implementing CI/CD with GitHub In this final part, we'll set up a Continuous Integration/Continuous Deployment (CI/CD) pipeline using GitHub Actions. We'll deliberately introduce a failing test, observe it fail both locally and in the CI pipeline, and then fix it. 1. Setting Up GitHub CI/CD In your local repository, create a new file .github/workflows/python-app.yml with the following content: name: Python Test on: push: branches: [ main ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python 3.8 uses: actions/setup-python@v2 with: python-version: 3.8 - name: Install dependencies run: | python -m pip install --upgrade pip pip install flask pytest - name: Run tests run: pytest This GitHub CI/CD configuration does the following: Uses a Python 3.8 Docker image for the CI environment Defines a single stage called \"test\" Installs the necessary dependencies (Flask and pytest) Runs the pytest command to execute the tests Triggers the pipeline on pushes to the main branch and for merge requests Commit and push this new file: git add .github/workflows/python-app.yml git commit -m \"Add GitHub CI/CD configuration\" git push origin main 2. Introducing a Failing Test Let's modify our calculate_age function to introduce a bug, and update its test to catch this bug. In app.py , change the calculate_age function: def calculate_age(dob): today = date.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\").date() age = today.year - birth_date.year # Introduce a bug: forget to check if birthday has occurred this year return age # This might be off by one year In test_app.py , update the test_calculate_age function: def test_calculate_age(): today = date.today() assert calculate_age(f\"{today.year-30}-{today.month:02d}-{today.day:02d}\") == 30 assert calculate_age(f\"{today.year-30}-{today.month:02d}-{today.day+1:02d}\") == 29 # This will fail 3. Running Tests Locally Run the tests locally to see the failure: pytest You should see that the second assertion in test_calculate_age fails. 4. Pushing to GitHub and Observing CI Failure Commit and push these changes: git add app.py test_app.py git commit -m \"Update calculate_age function and its test\" git push origin main Go to your GitHub repository, click on the \"Build/Pipelines\" tab, and you should see the pipeline running. It will fail due to the failing test. Because of that failure, the code won't be deployed to the production environment. You can think of the pipeline as a way to ensure that the code is working as expected before it is deployed to the production environment. 5. Fixing the Bug Now, let's fix the bug in the calculate_age function: In app.py , correct the calculate_age function: def calculate_age(dob): today = date.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\").date() age = today.year - birth_date.year # Check if birthday has occurred this year if today < date(today.year, birth_date.month, birth_date.day): age -= 1 return age Commit and push the fix: git add app.py git commit -m \"Fix calculate_age function\" git push origin main Go back to the GitHub tab and watch the new workflow run. It should pass all tests now. By following these steps, you've implemented a basic CI/CD pipeline using GitHub. This pipeline will automatically run your tests whenever you push changes to the repository, helping you catch issues early and ensure the quality of your code.","title":"Introduction to Git"},{"location":"developpement/git_intro.html#introduction-to-git","text":"Slides A useful cheatsheet: https://training.github.com/downloads/github-git-cheat-sheet.pdf A useful reference: https://git-scm.com/docs","title":"Introduction to git"},{"location":"developpement/git_intro.html#practical-session-basic-git-workflow-and-introduction-to-and-cicd","text":"","title":"Practical session: Basic Git Workflow and Introduction to and CI/CD"},{"location":"developpement/git_intro.html#introduction","text":"This practical session is designed for small groups of 2-3 people to work through together. It is very important that every member of the group runs the different commands in this practical session all shared with the same repository (we will see later what this means). During this project, you will learn if not already, how to use Git and GitHub to manage your code and to collaborate with your team. You will also learn how to use GitHub Actions to deploy your code and to run tests. This practical session is not a full Git and GitHub course, but rather a practical introduction to these tools providing you with the basic knowledge to be able to use them during this project. If you want to learn more about Git and GitHub, you can refer to the official documentation .","title":"Introduction:"},{"location":"developpement/git_intro.html#understanding-version-control","text":"Version control is a system that helps track and manage changes to files over time. It's an essential tool for software development, enabling developers to collaborate effectively, maintain a history of changes, and manage different versions of their code. With version control, you can revert files to a previous state, compare changes over time, see who last modified something that might be causing issues, and more. It acts as a safety net, allowing developers to experiment with new ideas without fear of losing their original work.","title":"Understanding Version Control"},{"location":"developpement/git_intro.html#example","text":"Imagine you're working on a Python application for a bookstore. Over time, you add several features, but one of them turns out to be problematic. With version control, you can easily remove just the unwanted feature while keeping the other improvements. Let's see how this might look: Day 1: You start with a basic Book class. class Book: def __init__(self, title, author, price): self.title = title self.author = author self.price = price def display_info(self): return f\"{self.title} by {self.author}: ${self.price}\" Day 3: You add a discount feature. class Book: def __init__(self, title, author, price): self.title = title self.author = author self.price = price def display_info(self): return f\"{self.title} by {self.author}: ${self.price}\" def apply_discount(self, discount): self.price = self.price * (1 - discount) Day 5: You add a feature to track stock, but it has a bug. class Book: def __init__(self, title, author, price, stock): self.title = title self.author = author self.price = price self.stock = stock def display_info(self): return f\"{self.title} by {self.author}: ${self.price} (In stock: {self.stock})\" def apply_discount(self, discount): self.price = self.price * (1 - discount) def update_stock(self, quantity): self.stock += quantity # Bug: This allows negative stock! Day 7: You add a method to calculate total value. class Book: def __init__(self, title, author, price, stock): self.title = title self.author = author self.price = price self.stock = stock def display_info(self): return f\"{self.title} by {self.author}: ${self.price} (In stock: {self.stock})\" def apply_discount(self, discount): self.price = self.price * (1 - discount) def update_stock(self, quantity): self.stock += quantity # Bug: This allows negative stock! def total_value(self): return self.price * self.stock Day 10: You realize the stock tracking feature has a bug allowing negative stock. With version control, you can: 1. Review the history of changes to identify when the bug was introduced. 2. Revert just the problematic update_stock method while keeping the other beneficial changes. 3. Fix the bug and commit the corrected version. Modern development teams use version control systems to manage their code, track changes, and collaborate effectively. Git is one of the most popular version control systems, providing a robust set of features for managing codebases of all sizes.","title":"Example:"},{"location":"developpement/git_intro.html#git","text":"Git is a distributed version control system that has revolutionized how developers manage and track changes in their code. Created by Linus Torvalds in 2005, Git was born out of the need for a fast, efficient, and reliable system to manage the development of the Linux kernel. Unlike centralized version control systems that preceded it, Git allows developers to have a complete copy of the project history on their local machines, enabling offline work and providing a safeguard against data loss. Git's distributed nature facilitates collaboration among developers, allowing them to work on different features simultaneously and merge their changes seamlessly. Its branching and merging capabilities make it easy to experiment with new ideas without affecting the main codebase. Since its inception, Git has become the de facto standard for version control in software development, used by millions of developers and organizations worldwide.","title":"Git"},{"location":"developpement/git_intro.html#gits-basic-paradigms","text":"Snapshots, Not Differences : Unlike other version control systems that store data as changes to a base version of each file, Git thinks of its data more like a series of snapshots of a miniature filesystem. Every time you commit, or save the state of your project in Git, it basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot. Local Operations : Most operations in Git need only local files and resources to operate. This means you can work on your project even when you're offline or not on a VPN, unlike centralized systems that need to communicate with a server for almost every operation. Integrity : Everything in Git is check-summed before it is stored and is then referred to by that checksum. This means it's impossible to change the contents of any file or directory without Git knowing about it. This functionality is built into Git at the lowest levels and is integral to its philosophy. Branching Model : Git's branching model is its \"killer feature.\" Unlike many other VCSs, Git encourages workflows that branch and merge often. This allows for feature branches, experimentation, and parallel development streams that can be easily merged when ready. Distributed Development : In Git, every developer's working copy of the code is also a repository that can contain the full history of all changes. This allows for multiple backup copies and various collaborative development models. Fast and Lightweight : Git is designed to be fast and efficient with large projects. Most operations are local, reducing the overhead of communicating with a centralized server.","title":"Git's Basic Paradigms"},{"location":"developpement/git_intro.html#the-three-states-in-git","text":"In Git, files can exist in three states: Modified : You have changed the file but have not committed it to your database yet. Staged : You have marked a modified file in its current version to go into your next commit snapshot. Committed : The data is safely stored in your local database. These three states correspond to the three main sections of a Git project. When not tracked by Git, a file is in the Untracked state.","title":"The Three States in Git"},{"location":"developpement/git_intro.html#git-areas-and-workflow","text":"Understanding the different areas in Git is crucial for mastering its workflow. Git manages your project's files through four main areas: Working Directory (Working Tree) : This is where you actually work on your project files. It's a single checkout of one version of the project. These files are pulled out of the compressed database in the Git directory and placed on disk for you to use or modify. Staging Area (Index) : This is a file, generally contained in your Git directory, that stores information about what will go into your next commit. It's sometimes referred to as the \"Index\". Think of it as a prep area for your next commit. Files are added to this area with the git add command. Local Repository : This is where Git stores the metadata and object database for your project. It's what's copied when you clone a repository from another computer. The local repository contains all of your committed changes. It's located in the .git directory of your project. Remote Repository : This is a version of your project that is hosted on the Internet or network somewhere (like GitHub, GitLab, or Bitbucket). You can have several of them, each of which generally is either read-only or read/write for you. Collaborating with others involves managing your remote repositories and pushing and pulling data to and from them when you need to share work.","title":"Git Areas and Workflow"},{"location":"developpement/git_intro.html#basic-git-workflow","text":"You modify files in your Working Directory. You stage the files, adding snapshots of them to your Staging Area. You do a commit , which takes the files as they are in the Staging Area and stores that snapshot permanently to your Local Repository. You push your changes to a Remote Repository to share with others or as a backup. Understanding these areas and how they interact is key to understanding Git's workflow and effectively managing your projects with version control.","title":"Basic Git Workflow:"},{"location":"developpement/git_intro.html#github","text":"GitHub is a web-based hosting service for Git repositories. Launched in 2008, it has become the world's largest host of source code and a central hub for collaboration among developers. While Git is a command-line tool, GitHub provides a web-based graphical interface. It also offers access control and several collaboration features, such as bug tracking, feature requests, task management, and wikis for every project. Key features of GitHub include: Repository Hosting : GitHub can host your Git repositories in the cloud, making it easy to share and collaborate on code. Fork and Pull Request : Users can \"fork\" an existing repository (creating their own copy), make changes, and then submit a \"pull request\" to propose those changes back to the original project. Issue Tracking : GitHub provides a system for reporting and managing bugs, feature requests, and other tasks related to a project. Project Management Tools : Including project boards, milestones, and other tools to help manage and organize work on repositories. Social Coding : Users can follow repositories and other users, star repositories they like, and see a feed of activity from repositories and users they're interested in. GitHub Pages : A feature that allows hosting of static websites directly from a GitHub repository. Integrations : GitHub can integrate with many third-party services, enhancing its capabilities for things like continuous integration and deployment. GitHub has played a significant role in the growth of open-source software, providing a platform where developers from around the world can collaborate on projects. It's used not only by individual developers and open-source projects but also by large companies to host and manage their code.","title":"GitHub"},{"location":"developpement/git_intro.html#setting-up-your-environment","text":"","title":"Setting up your environment"},{"location":"developpement/git_intro.html#anaconda-and-python","text":"Download and install Anaconda from the official website: https://www.anaconda.com/products/distribution During installation, make sure to add Anaconda to your PATH environment variable when prompted. Open an Anaconda Prompt (on Windows) or a terminal (on macOS/Linux). Create a new environment for this project: conda create --name gitproject python=3.8 Activate the environment: conda activate gitproject Install Flask (we'll use it for examples during the session): pip install flask","title":"Anaconda and Python:"},{"location":"developpement/git_intro.html#git_1","text":"1. Download Git from the official website: https://git-scm.com/downloads 2. Follow the installation instructions for your operating system. 3. After installation, open a new terminal or command prompt and verify the installation: git --version Configure your Git username and email: git config --global user.name \"Your Name\" git config --global user.email \"your.email@example.com\"","title":"Git"},{"location":"developpement/git_intro.html#github_1","text":"We will use GitHub as our remote repository for this project. If you don't have a GitHub account, you can create one for free at https://github.com/.","title":"GitHub"},{"location":"developpement/git_intro.html#remote-repository","text":"(Just one person in the group should do this and invite others to collaborate) Log in to GitHub. Click the \"new\" button on the left menu. Name your repository (e.g., \"flask-git-demo\"). Choose to make it public. Initialize the repository with a README file. Click \"Create project\". To invite collaborators: Go to your repository's page. On the left menu, click on \"Members\" and invite the other members of the group. Give them the \"Developer\" role.","title":"Remote repository"},{"location":"developpement/git_intro.html#local-repository","text":"Open a terminal or command prompt. Navigate to the directory where you want to create your project: cd path/to/your/project/directory Clone the remote repository: git clone https://github.com/your-username/flask-git-demo.git (Replace 'your-username' with the GitHub username of the person who created the repository) 4. Navigate into the cloned repository: cd flask-git-demo Now your environment is set up and ready for the Git and GitHub practical session. Each member of the group should have Python, Flask, and Git installed, a GitHub account, and a local copy of the repository. The repository owner has set up the remote repository and invited other group members as collaborators.","title":"Local repository"},{"location":"developpement/git_intro.html#practical-session-a-flask-web-app-development-with-git-and-github","text":"We'll build a Flask web app that collects a user's name and date of birth, then displays various information based on this input. We'll develop this app in stages, using Git and GitHub to manage our development process.","title":"Practical session: A Flask Web App Development with Git and GitHub"},{"location":"developpement/git_intro.html#stage-1-basic-setup-and-first-commit-explained-guide","text":"In this stage, we'll set up a basic Flask app within our existing Git repository. To avoid conflicts, only one person (the team lead) should perform the initial setup and push the changes. Other team members will then pull these changes.","title":"Stage 1: Basic Setup and First Commit - Explained Guide"},{"location":"developpement/git_intro.html#for-the-team-lead","text":"","title":"For the Team Lead:"},{"location":"developpement/git_intro.html#1-navigate-to-the-project-directory","text":"First, we need to ensure we're in the correct directory for our project. cd path/to/flask-git-demo This command changes the current directory to your project folder.","title":"1. Navigate to the Project Directory"},{"location":"developpement/git_intro.html#2-create-the-flask-app","text":"Now, we'll create the basic structure for our Flask application. Create a new file called app.py : touch app.py # touch is a command that creates an empty file # on windows just create it manually This command creates an empty file named app.py . Open app.py in your preferred text editor and add the following code: from flask import Flask, render_template, request from datetime import datetime app = Flask(__name__) @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] welcome_message = f\"Welcome, {name}! Your date of birth is {dob}.\" return render_template('result.html', message=welcome_message) return render_template('index.html') if __name__ == '__main__': app.run(debug=True) This code sets up a basic Flask app with a route that handles both GET and POST requests. Create a new directory for our HTML templates: mkdir templates # mkdir is a command that creates a new directory # on windows just create it manually This command creates a new directory named \"templates\", where Flask will look for our HTML files. Create two HTML files in the templates directory: For index.html : touch templates/index.html # on windows just create it manually Then add the HTML content for the form in your text editor. <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Birthday App</title> <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"> </head> <body> <h1>Welcome to the Birthday App</h1> <form method=\"POST\"> <label for=\"name\">Name:</label> <input type=\"text\" id=\"name\" name=\"name\" required> <label for=\"dob\">Date of Birth:</label> <input type=\"date\" id=\"dob\" name=\"dob\" required> <input type=\"submit\" value=\"Submit\"> </form> </body> </html> For result.html : touch templates/result.html # on windows just create it manually Then add the HTML content for the result page in your text editor. <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Birthday App Result</title> <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"> </head> <body> <h1>Your Birthday Information</h1> <p>{{ message }}</p> <a href=\"{{ url_for('index') }}\">Go back</a> </body> </html>","title":"2. Create the Flask App"},{"location":"developpement/git_intro.html#3-update-gitignore","text":"To keep our repository clean, we'll update the .gitignore file to exclude unnecessary files. echo \"venv/\" >> .gitignore echo \"__pycache__/\" >> .gitignore echo \"*.pyc\" >> .gitignore These commands append new lines to the .gitignore file, telling Git to ignore the virtual environment directory, Python cache files, and compiled Python files. By adding these files to the .gitignore file, we ensure that they are not tracked by Git, meaning they won't be committed to the repository.","title":"3. Update .gitignore"},{"location":"developpement/git_intro.html#4-git-operations","text":"Now we'll use Git to track our new files and push them to the remote repository. Check the status of your repository: git status This command shows you which files have been changed or are untracked. Add the new files to the staging area: git add . This stages all new and modified files, preparing them for commit. Check the status again to verify that the files have been staged: git status Commit the changes: git commit -m \"Add basic Flask app structure\" This creates a new commit with the staged changes and adds a descriptive message. Check the status again to verify that the commit has been created: git status Push the changes to the remote repository: git push origin main This uploads your local commits to the remote repository on GitHub. Check the status again to verify that the commit has been pushed: git status","title":"4. Git Operations"},{"location":"developpement/git_intro.html#5-testing-the-app","text":"Before notifying your team, make sure the app works as expected: python app.py This command runs your Flask application. Open a web browser and go to http://127.0.0.1:5000/ to verify the app is functioning correctly.","title":"5. Testing the App"},{"location":"developpement/git_intro.html#6-notify-team-members","text":"Once you've successfully pushed the changes and tested the app, notify your team members that the initial setup is complete and they can proceed with their steps.","title":"6. Notify Team Members"},{"location":"developpement/git_intro.html#for-other-team-members","text":"After the team lead has completed the setup and pushed the changes, follow these steps: Pull the latest changes from the remote repository: git pull origin main This command fetches the latest changes from the remote repository and merges them into your local branch. Test the app to ensure it's working on your local machine: python app.py This runs the Flask application. Check http://127.0.0.1:5000/ in your browser to verify it's working correctly.","title":"For Other Team Members:"},{"location":"developpement/git_intro.html#collaboration-notes","text":"Always pull the latest changes before starting work each day: git pull origin main This ensures you're working with the most up-to-date version of the project. If you encounter any issues or merge conflicts, communicate with your team to resolve them. Remember to commit your changes frequently with meaningful commit messages as you start working on new features in the upcoming stages. In the next stages, we'll add more features to this app and explore more Git and GitHub concepts as a team.","title":"Collaboration Notes"},{"location":"developpement/git_intro.html#stage-2-implementing-age-calculation-direct-implementation","text":"In this stage, we'll add a feature to calculate the user's age based on their date of birth. We'll implement this directly on the main branch to practice basic Git workflows.","title":"Stage 2: Implementing Age Calculation - Direct Implementation"},{"location":"developpement/git_intro.html#for-the-student-lead-of-stage-2","text":"","title":"For the Student Lead of Stage 2:"},{"location":"developpement/git_intro.html#1-prepare-your-local-repository","text":"Ensure your local repository is up-to-date: git pull origin main This fetches and merges the latest changes from the main branch.","title":"1. Prepare Your Local Repository"},{"location":"developpement/git_intro.html#2-implement-age-calculation","text":"Open app.py and add a new function to calculate age: from datetime import datetime def calculate_age(dob): today = datetime.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\") age = today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day)) return age Modify the index function to use this new calculation: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) welcome_message = f\"Welcome, {name}! You are {age} years old.\" return render_template('result.html', message=welcome_message) return render_template('index.html')","title":"2. Implement Age Calculation"},{"location":"developpement/git_intro.html#3-test-your-changes","text":"Run the Flask application to test your changes: python app.py Verify that the age calculation works correctly by submitting the form and checking the result.","title":"3. Test Your Changes"},{"location":"developpement/git_intro.html#4-commit-your-changes","text":"After confirming that everything works: git add app.py git commit -m \"Add age calculation feature\" This stages and commits your changes to your local main branch.","title":"4. Commit Your Changes"},{"location":"developpement/git_intro.html#5-push-your-changes","text":"Push your commits to the remote repository: git push origin main This updates the main branch on GitHub with your new changes.","title":"5. Push Your Changes"},{"location":"developpement/git_intro.html#6-notify-team-members_1","text":"Let your team know that you've pushed new changes to the main branch.","title":"6. Notify Team Members"},{"location":"developpement/git_intro.html#for-other-team-members_1","text":"After the student lead has pushed the changes: Pull the latest changes from the remote repository: git pull origin main This updates your local main branch with the new changes. Test the updated application to ensure everything works correctly: python app.py If you encounter any issues, communicate with the team to resolve them.","title":"For Other Team Members:"},{"location":"developpement/git_intro.html#stage-3-adding-zodiac-sign-feature-branching-and-pull-requests","text":"Now that we've implemented a feature directly on the main branch, let's discuss the concept of branching and why it's useful: Branching in Git allows developers to diverge from the main line of development and work independently on features or experiments without affecting the main codebase. This has several advantages: Isolation : You can work on different features or experiments without interfering with the main codebase or other developers' work. Easier collaboration : Multiple developers can work on different features simultaneously without conflicts. Code review : Branches facilitate code reviews through pull requests before merging changes into the main codebase. Experimentation : You can try out ideas without the risk of breaking the main codebase. In this stage, we'll add a feature to determine the user's zodiac sign based on their date of birth. We'll use Git branching and create a pull request to implement this feature, demonstrating a more advanced Git workflow.","title":"Stage 3: Adding Zodiac Sign Feature - Branching and Pull Requests"},{"location":"developpement/git_intro.html#for-the-student-lead-of-stage-3","text":"","title":"For the Student Lead of Stage 3:"},{"location":"developpement/git_intro.html#1-ensure-your-repository-is-up-to-date","text":"First, make sure you're on the main branch and it's up-to-date: git checkout main git pull origin main","title":"1. Ensure Your Repository is Up-to-Date"},{"location":"developpement/git_intro.html#2-create-a-new-branch","text":"Create and switch to a new branch for the zodiac sign feature: git checkout -b feature/zodiac-sign","title":"2. Create a New Branch"},{"location":"developpement/git_intro.html#3-implement-the-zodiac-sign-feature","text":"In app.py , add a new function to determine the zodiac sign: def get_zodiac_sign(dob): month, day = map(int, dob.split('-')[1:]) zodiac_signs = [ (1, 20, \"Capricorn\"), (2, 19, \"Aquarius\"), (3, 20, \"Pisces\"), (4, 20, \"Aries\"), (5, 21, \"Taurus\"), (6, 21, \"Gemini\"), (7, 22, \"Cancer\"), (8, 23, \"Leo\"), (9, 23, \"Virgo\"), (10, 23, \"Libra\"), (11, 22, \"Scorpio\"), (12, 22, \"Sagittarius\"), (12, 31, \"Capricorn\") ] for m, d, sign in zodiac_signs: if (month, day) <= (m, d): return sign Modify the index function to include the zodiac sign: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) message = f\"Welcome, {name}! You are {age} years old. Your zodiac sign is {zodiac}.\" return render_template('result.html', message=message) return render_template('index.html')","title":"3. Implement the Zodiac Sign Feature"},{"location":"developpement/git_intro.html#4-test-your-changes","text":"Run the Flask application and test the new feature: python app.py","title":"4. Test Your Changes"},{"location":"developpement/git_intro.html#5-commit-your-changes","text":"After ensuring everything works: git add app.py git commit -m \"Add zodiac sign feature\"","title":"5. Commit Your Changes"},{"location":"developpement/git_intro.html#6-push-your-branch-to-github","text":"Push your feature branch to the remote repository: git push -u origin feature/zodiac-sign","title":"6. Push Your Branch to GitHub"},{"location":"developpement/git_intro.html#7-create-a-pull-request","text":"Go to your repository on GitHub. You should see a prompt to create a pull request for your recently pushed branch. Click on it. Fill in the details of your pull request, describing the new zodiac sign feature. Assign team members to review your pull request.","title":"7. Create a Pull Request"},{"location":"developpement/git_intro.html#for-other-team-members-reviewers","text":"","title":"For Other Team Members (Reviewers):"},{"location":"developpement/git_intro.html#reviewing-the-pull-request","text":"Go to the repository on GitHub and navigate to the Pull Requests tab. Click on the pull request for the zodiac sign feature. Review the changes: Check the code for correctness and style. Consider how this feature integrates with the existing codebase. To test the changes locally: git fetch origin # to update the local repository with the latest changes from the remote repository git checkout feature/zodiac-sign # to switch to the feature branch python app.py # to test the changes locally Leave comments or request changes if necessary. If everything looks good, approve the pull request.","title":"Reviewing the Pull Request"},{"location":"developpement/git_intro.html#after-the-pull-request-is-merged","text":"Once the pull request is approved and merged: Switch back to the main branch: git checkout main Pull the latest changes: git pull origin main Test the updated application to ensure everything works correctly. In the next stage, we'll continue to build on our app and explore more advanced Git and GitHub features, such as handling merge conflicts.","title":"After the Pull Request is Merged"},{"location":"developpement/git_intro.html#stage-4-enhancing-the-ui-git-stash-and-github-issues","text":"In this stage, we'll improve the user interface of our Flask application by adding some basic CSS. We'll also learn about Git stash for managing temporary changes and use GitHub Issues for task tracking.","title":"Stage 4: Enhancing the UI - Git Stash and GitHub Issues"},{"location":"developpement/git_intro.html#for-the-project-manager-can-be-any-team-member","text":"","title":"For the Project Manager (can be any team member):"},{"location":"developpement/git_intro.html#1-create-github-issues","text":"Go to your GitHub repository and navigate to the \"Issues\" tab. Create a new issue titled \"Enhance UI with CSS\". In the description, outline the following tasks: Add a CSS file for styling Style the form on the index page Improve the layout of the result page Add labels like \"enhancement\" and \"ui\". Assign the issue to a team member.","title":"1. Create GitHub Issues"},{"location":"developpement/git_intro.html#for-the-assigned-team-member","text":"","title":"For the Assigned Team Member:"},{"location":"developpement/git_intro.html#1-set-up-your-work-environment","text":"Ensure your local repository is up-to-date: git checkout main git pull origin main","title":"1. Set Up Your Work Environment"},{"location":"developpement/git_intro.html#2-create-a-new-branch_1","text":"Create a branch for the UI enhancements: git checkout -b feature/ui-enhancement","title":"2. Create a New Branch"},{"location":"developpement/git_intro.html#3-add-css-file","text":"Create a new directory named static in your project root: mkdir static Create a new CSS file: touch static/style.css Add some basic CSS to style.css : body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; } h1 { color: #333; } form { background-color: #fff; padding: 20px; border-radius: 5px; box-shadow: 0 0 10px rgba(0,0,0,0.1); } input[type=\"text\"], input[type=\"date\"] { width: 100%; padding: 8px; margin-bottom: 10px; border: 1px solid #ddd; border-radius: 4px; } input[type=\"submit\"] { background-color: #333; color: #fff; border: none; padding: 10px 20px; cursor: pointer; border-radius: 4px; } input[type=\"submit\"]:hover { background-color: #555; }","title":"3. Add CSS File"},{"location":"developpement/git_intro.html#4-test-your-changes_1","text":"Run the Flask application and verify the UI improvements: python app.py","title":"4. Test Your Changes"},{"location":"developpement/git_intro.html#5-demonstrate-git-stash","text":"Let's say you notice a small bug in the age calculation while working on the UI. Here's how to use Git stash: Make a small change in app.py to fix the bug. Instead of committing this change, use Git stash: git stash save \"Fix age calculation bug\" Your working directory is now clean and back to the state of the last commit. To apply the stashed changes later: git stash pop","title":"5. Demonstrate Git Stash"},{"location":"developpement/git_intro.html#6-commit-ui-changes","text":"Now, commit your UI enhancements: git add static/style.css templates/index.html templates/result.html git commit -m \"Enhance UI with CSS styling\"","title":"6. Commit UI Changes"},{"location":"developpement/git_intro.html#7-push-and-create-pull-request","text":"Push your branch and create a merge request: git push -u origin feature/ui-enhancement Create a pull request on GitHub, referencing the issue number in the description (e.g., \"Closes #1\").","title":"7. Push and Create Pull Request"},{"location":"developpement/git_intro.html#for-reviewers","text":"Review the pull request, checking both the code and the visual changes. Test the changes locally if necessary. Provide feedback or approve the changes.","title":"For Reviewers:"},{"location":"developpement/git_intro.html#after-merging","text":"Close the GitHub issue once the pull request is merged. All team members should pull the latest changes: git checkout main git pull origin main Discuss as a team: 1. How did using GitHub Issues help in organizing the task? 2. What was your experience with Git stash? How might it be useful in other scenarios? 3. How has the workflow evolved from the earlier stages of the project? In the next stage, we'll implement a more complex feature and explore handling merge conflicts.","title":"After Merging:"},{"location":"developpement/git_intro.html#stage-5-merging-and-rebasing-with-conflicts","text":"Git offers two primary ways to integrate changes from one branch into another: merging and rebasing.","title":"Stage 5: Merging and Rebasing with Conflicts"},{"location":"developpement/git_intro.html#merge","text":"Merging creates a new commit that combines the tips of two branches. This method preserves the entire history of both branches, providing a non-destructive operation that maintains a clear record of when branches diverged and were integrated. The resulting history shows the parallel development that occurred, with a visible branching structure. Merging is particularly safe for shared branches as it doesn't rewrite history. However, in projects with frequent merges, this can lead to a more complex history that may be harder to follow.","title":"Merge"},{"location":"developpement/git_intro.html#rebase","text":"Rebasing, on the other hand, moves the entire feature branch to begin on the tip of the main branch, effectively replaying your work on top of it. This results in a linear project history, as if the work was done sequentially rather than in parallel. Rebasing creates a cleaner, more streamlined history which can make it easier to track features. However, it achieves this by rewriting the project history, which can be problematic if the rebased branch has been shared with others. As such, rebasing requires more care when used on public or shared branches. The choice between merging and rebasing often depends on the specific needs of your project and team. Merging is generally preferred for preserving complete history and for work on public branches, while rebasing is often used to maintain a cleaner history, especially for local branches or before merging a feature branch into the main line of development. In this stage, we'll practice merging and rebasing, including handling conflicts. We'll do this by creating a new feature branch, making changes to the main branch, and then exploring both merge and rebase workflows.","title":"Rebase"},{"location":"developpement/git_intro.html#part-1-merging-with-conflicts","text":"","title":"Part 1: Merging with Conflicts"},{"location":"developpement/git_intro.html#for-developer-a","text":"Ensure your main branch is up-to-date: git checkout main git pull origin main Create a new branch for a feature: git checkout -b feature/birthday-countdown Implement the birthday countdown feature in app.py : from datetime import datetime, date def days_to_birthday(dob): today = date.today() dob = datetime.strptime(dob, \"%Y-%m-%d\").date() next_birthday = date(today.year, dob.month, dob.day) if next_birthday < today: next_birthday = date(today.year + 1, dob.month, dob.day) return (next_birthday - today).days @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) message = f\"Welcome, {name}! You are {age} years old. Your zodiac sign is {zodiac}. There are {days_to_bday} days until your next birthday!\" return render_template('result.html', message=message) return render_template('index.html') Commit your changes: git add app.py git commit -m \"Add birthday countdown feature\"","title":"For Developer A:"},{"location":"developpement/git_intro.html#for-developer-b","text":"Make sure you're on the main branch and it's up-to-date: git checkout main git pull origin main Make a change to the index function in app.py : @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) message = f\"Hello, {name}! Your age is {age} and your zodiac sign is {zodiac}.\" return render_template('result.html', message=message) return render_template('index.html') Commit and push this change: git add app.py git commit -m \"Update welcome message format\" git push origin main","title":"For Developer B:"},{"location":"developpement/git_intro.html#back-to-developer-a","text":"Try to merge the main branch into your feature branch: git checkout feature/birthday-countdown git merge main You'll encounter a merge conflict. Open app.py and you'll see something like: <<<<<<< HEAD message = f\"Welcome, {name}! You are {age} years old. Your zodiac sign is {zodiac}. There are {days_to_bday} days until your next birthday!\" ======= message = f\"Hello, {name}! Your age is {age} and your zodiac sign is {zodiac}.\" >>>>>>> main Resolve the conflict by combining both changes: message = f\"Hello, {name}! Your age is {age} and your zodiac sign is {zodiac}. There are {days_to_bday} days until your next birthday!\" Stage the resolved file, commit the merge, and push: git add app.py git commit -m \"Merge main into feature/birthday-countdown and resolve conflicts\" git push origin feature/birthday-countdown Before moving to the next part, you can merge the feature branch into the main branch, either using a merge request or if you have the right to do it directly: git checkout main git merge feature/birthday-countdown","title":"Back to Developer A:"},{"location":"developpement/git_intro.html#part-2-rebasing-with-conflicts","text":"Now, let's practice rebasing with a similar scenario.","title":"Part 2: Rebasing with Conflicts"},{"location":"developpement/git_intro.html#for-developer-a_1","text":"Create a new feature branch from main: git checkout main git pull origin main git checkout -b feature/lucky-number Add a lucky number feature to app.py : import random def get_lucky_number(): return random.randint(1, 100) @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) lucky_number = get_lucky_number() message = f\"Hello, {name}! Your age is {age}, your zodiac sign is {zodiac}, and there are {days_to_bday} days until your next birthday. Your lucky number is {lucky_number}!\" return render_template('result.html', message=message) return render_template('index.html') Commit your changes: git add app.py git commit -m \"Add lucky number feature\"","title":"For Developer A:"},{"location":"developpement/git_intro.html#for-developer-b_1","text":"Make another change to the main branch: git checkout main git pull origin main Update the index function in app.py : @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) message = f\"Greetings, {name}! You're {age} years old with the zodiac sign {zodiac}. Your next birthday is in {days_to_bday} days.\" return render_template('result.html', message=message) return render_template('index.html') Commit and push this change: git add app.py git commit -m \"Refine welcome message\" git push origin main","title":"For Developer B:"},{"location":"developpement/git_intro.html#back-to-developer-a_1","text":"Try to rebase your feature branch onto the updated main: git checkout feature/lucky-number git rebase main You'll encounter a rebase conflict. Open app.py and resolve the conflict: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) lucky_number = get_lucky_number() message = f\"Greetings, {name}! You're {age} years old with the zodiac sign {zodiac}. Your next birthday is in {days_to_bday} days. Your lucky number is {lucky_number}!\" return render_template('result.html', message=message) return render_template('index.html') After resolving the conflict: git add app.py git rebase --continue Force push your rebased branch: git push origin feature/lucky-number --force Make a merge request to merge the feature branch into the main branch.","title":"Back to Developer A:"},{"location":"developpement/git_intro.html#stage-6-implementing-tests-for-the-flask-application","text":"In this stage, we'll add tests to our Flask application to ensure its functionality and to practice test-driven development (TDD). We'll write both unit tests for individual functions and integration tests for the application routes.","title":"Stage 6: Implementing Tests for the Flask Application"},{"location":"developpement/git_intro.html#1-set-up-testing-environment","text":"First, we need to set up our testing environment: Install pytest, a popular testing framework for Python: pip install pytest Create a new file called test_app.py in your project root directory.","title":"1. Set Up Testing Environment"},{"location":"developpement/git_intro.html#2-writing-unit-tests","text":"Let's start by writing unit tests for our existing functions: Open test_app.py and add the following code: from app import calculate_age, get_zodiac_sign, days_to_birthday from datetime import date def test_calculate_age(): assert calculate_age(\"1990-01-01\") == date.today().year - 1990 def test_get_zodiac_sign(): assert get_zodiac_sign(\"1990-01-01\") == \"Capricorn\" assert get_zodiac_sign(\"1990-07-01\") == \"Cancer\" def test_days_to_birthday(): today = date.today() next_birthday = date(today.year, 12, 31) if next_birthday < today: next_birthday = date(today.year + 1, 12, 31) expected_days = (next_birthday - today).days assert days_to_birthday(\"2000-12-31\") == expected_days These tests check the core functionality of our utility functions.","title":"2. Writing Unit Tests"},{"location":"developpement/git_intro.html#3-writing-integration-tests","text":"Now, let's add integration tests for our Flask routes: Add the following code to test_app.py : import pytest from app import app @pytest.fixture def client(): app.config['TESTING'] = True with app.test_client() as client: yield client def test_home_page(client): response = client.get('/') assert response.status_code == 200 assert b\"Welcome to the Birthday App\" in response.data def test_form_submission(client): response = client.post('/', data={ 'name': 'John Doe', 'dob': '1990-01-01' }, follow_redirects=True) assert response.status_code == 200 assert b\"John Doe\" in response.data assert b\"Your age is\" in response.data assert b\"Your zodiac sign is\" in response.data These tests check that our application's routes are working correctly.","title":"3. Writing Integration Tests"},{"location":"developpement/git_intro.html#4-running-the-tests","text":"To run the tests: In your terminal, navigate to your project directory. Run the following command: pytest You should see output indicating which tests passed or failed. In our case you should see that test_form_submission failed can you see why, and how to fix it?","title":"4. Running the Tests"},{"location":"developpement/git_intro.html#5-test-driven-development-adding-a-new-feature","text":"Test-Driven Development (TDD) is a software development approach where tests are written before the actual code. The TDD cycle, often referred to as Red-Green-Refactor, consists of three steps: Red: Write a test that fails. This test describes a desired functionality that doesn't exist yet. Green: Write the minimal amount of code necessary to make the test pass. The focus here is on making the test pass, not on writing perfect code. Refactor: Improve the code without changing its functionality. This step is about cleaning up the code, removing duplication, and ensuring it follows good design principles. The benefits of TDD include: Improved code quality: By thinking about how to test the code before writing it, developers often create more modular, flexible, and easier-to-maintain code. Better understanding of requirements: Writing tests first forces developers to clearly understand what the code should do before implementing it. Built-in regression testing: As features are added, the growing suite of tests helps ensure that new changes don't break existing functionality. Documentation: Tests serve as a form of documentation, showing how the code is expected to behave in various scenarios. Confidence in refactoring: With a comprehensive test suite, developers can refactor code with confidence, knowing that if they break something, a test will fail. Let's practice TDD by adding a new feature to determine if it's the user's birthday today. First, write a test for the new function in test_app.py : from app import is_birthday_today def test_is_birthday_today(): today = date.today() assert is_birthday_today(f\"{today.year}-{today.month:02d}-{today.day:02d}\") == True assert is_birthday_today(\"1990-01-01\") == (date.today().month == 1 and date.today().day == 1) Run the tests. The new test should fail because we haven't implemented the function yet. Now, implement the function in app.py : def is_birthday_today(dob): today = date.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\").date() return (today.month, today.day) == (birth_date.month, birth_date.day) Run the tests again. They should all pass now. Finally, update your index route to use this new function: @app.route('/', methods=['GET', 'POST']) def index(): if request.method == 'POST': name = request.form['name'] dob = request.form['dob'] age = calculate_age(dob) zodiac = get_zodiac_sign(dob) days_to_bday = days_to_birthday(dob) is_birthday = is_birthday_today(dob) lucky_number = get_lucky_number() message = f\"Greetings, {name}! You're {age} years old with the zodiac sign {zodiac}. Your next birthday is in {days_to_bday} days. Your lucky number is {lucky_number}!\" if is_birthday: message += \"Happy Birthday!\" else: message += f\"There are {days_to_bday} days until your next birthday.\" return render_template('result.html', message=message) return render_template('index.html') Add a test for this new route behavior in test_app.py : def test_birthday_today(client): today = date.today() response = client.post('/', data={ 'name': 'John Doe', 'dob': f\"{today.year}-{today.month:02d}-{today.day:02d}\" }, follow_redirects=True) assert b\"Happy Birthday!\" in response.data Run the tests one final time to ensure everything is working.","title":"5. Test-Driven Development: Adding a New Feature"},{"location":"developpement/git_intro.html#stage-7-introduction-to-cicd","text":"Continuous Integration (CI) and Continuous Deployment (CD) are practices in software development that aim to improve the process of building, testing, and releasing software.","title":"Stage 7: Introduction to CI/CD"},{"location":"developpement/git_intro.html#continuous-integration-ci","text":"Continuous Integration is the practice of frequently merging code changes into a shared repository. Each integration is verified by automated builds and tests. The main goals of CI are: Detect and address integration issues early Improve software quality Reduce the time to validate and release new updates With CI, developers integrate their work frequently, usually daily, leading to multiple integrations per day. Each integration triggers automated builds and tests to detect issues quickly.","title":"Continuous Integration (CI)"},{"location":"developpement/git_intro.html#continuous-deployment-cd","text":"Continuous Deployment takes CI one step further. In CD, every change that passes the automated tests is automatically deployed to production. The main benefits of CD are: Faster release cycles Reduced manual processes and human error More frequent user feedback Improved developer productivity CD can also refer to Continuous Delivery, where changes are automatically deployed to a staging environment but require manual approval for production deployment.","title":"Continuous Deployment (CD)"},{"location":"developpement/git_intro.html#why-cicd-is-useful","text":"Faster Bug Detection and Resolution : Issues are caught earlier in the development process, making them easier and less expensive to fix. Improved Collaboration : Frequent integration encourages communication between team members and keeps everyone up to date with changes. Higher Quality Software : Automated testing ensures that tests are run consistently and frequently, catching bugs that might be missed in manual testing. Faster Time to Market : Automating the build, test, and deployment processes reduces the time between writing code and using it in production. Reduced Risk : Smaller, more frequent updates are less risky and easier to roll back if issues occur. Increased Confidence : With a robust CI/CD pipeline, teams can be more confident in the stability and quality of their code. In the following section, we'll implement a basic CI/CD pipeline using GitHub Actions, experiencing firsthand how these practices can improve our development workflow.","title":"Why CI/CD is Useful"},{"location":"developpement/git_intro.html#implementing-cicd-with-github","text":"In this final part, we'll set up a Continuous Integration/Continuous Deployment (CI/CD) pipeline using GitHub Actions. We'll deliberately introduce a failing test, observe it fail both locally and in the CI pipeline, and then fix it.","title":"Implementing CI/CD with GitHub"},{"location":"developpement/git_intro.html#1-setting-up-github-cicd","text":"In your local repository, create a new file .github/workflows/python-app.yml with the following content: name: Python Test on: push: branches: [ main ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python 3.8 uses: actions/setup-python@v2 with: python-version: 3.8 - name: Install dependencies run: | python -m pip install --upgrade pip pip install flask pytest - name: Run tests run: pytest This GitHub CI/CD configuration does the following: Uses a Python 3.8 Docker image for the CI environment Defines a single stage called \"test\" Installs the necessary dependencies (Flask and pytest) Runs the pytest command to execute the tests Triggers the pipeline on pushes to the main branch and for merge requests Commit and push this new file: git add .github/workflows/python-app.yml git commit -m \"Add GitHub CI/CD configuration\" git push origin main","title":"1. Setting Up GitHub CI/CD"},{"location":"developpement/git_intro.html#2-introducing-a-failing-test","text":"Let's modify our calculate_age function to introduce a bug, and update its test to catch this bug. In app.py , change the calculate_age function: def calculate_age(dob): today = date.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\").date() age = today.year - birth_date.year # Introduce a bug: forget to check if birthday has occurred this year return age # This might be off by one year In test_app.py , update the test_calculate_age function: def test_calculate_age(): today = date.today() assert calculate_age(f\"{today.year-30}-{today.month:02d}-{today.day:02d}\") == 30 assert calculate_age(f\"{today.year-30}-{today.month:02d}-{today.day+1:02d}\") == 29 # This will fail","title":"2. Introducing a Failing Test"},{"location":"developpement/git_intro.html#3-running-tests-locally","text":"Run the tests locally to see the failure: pytest You should see that the second assertion in test_calculate_age fails.","title":"3. Running Tests Locally"},{"location":"developpement/git_intro.html#4-pushing-to-github-and-observing-ci-failure","text":"Commit and push these changes: git add app.py test_app.py git commit -m \"Update calculate_age function and its test\" git push origin main Go to your GitHub repository, click on the \"Build/Pipelines\" tab, and you should see the pipeline running. It will fail due to the failing test. Because of that failure, the code won't be deployed to the production environment. You can think of the pipeline as a way to ensure that the code is working as expected before it is deployed to the production environment.","title":"4. Pushing to GitHub and Observing CI Failure"},{"location":"developpement/git_intro.html#5-fixing-the-bug","text":"Now, let's fix the bug in the calculate_age function: In app.py , correct the calculate_age function: def calculate_age(dob): today = date.today() birth_date = datetime.strptime(dob, \"%Y-%m-%d\").date() age = today.year - birth_date.year # Check if birthday has occurred this year if today < date(today.year, birth_date.month, birth_date.day): age -= 1 return age Commit and push the fix: git add app.py git commit -m \"Fix calculate_age function\" git push origin main Go back to the GitHub tab and watch the new workflow run. It should pass all tests now. By following these steps, you've implemented a basic CI/CD pipeline using GitHub. This pipeline will automatically run your tests whenever you push changes to the repository, helping you catch issues early and ensure the quality of your code.","title":"5. Fixing the Bug"},{"location":"developpement/mnist.html","text":"Development for Data Scientist: Practical session 1: Deploying a digit classifier For this session, your task is to create a script for training a simple neural network on the MNIST dataset using PyTorch. Throughout the training process, you'll utilize TensorBoard for the following purposes: Keeping an eye on your network's performance as epochs progress. Organizing your different experiments and hyperparameters. Generating visualizations to aid in analysis. Once the training process is complete, you'll also learn how to export your model in a format that can be used for inference. Finally, you will learn how to deploy your model on a REST API using Flask and how to request it using a Python script. Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. The network class: Using the figure above, fill in the following code, in the model.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x The training script The earlier file included your model class. Now, you will proceed to finalize the training script, named train.py . This script will serve as a Python script for training a neural network on the MNIST Dataset. Both the train() and test() methods have already been implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from model import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total Now you will implement the main method, which will be executed each time the Python script is run. You'd like to offer users the flexibility to adjust certain learning process parameters, specifically: Batch size Learning rate Number of training epochs To achieve this, the Python argparse module will be employed. This module simplifies the creation of user-friendly command-line interfaces. Incorporating arguments into a Python script through argparse is a straightforward process. To begin, you'll need to import the argparse module and create a parser instance within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments as follows: args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments. Call the train method to train your network and the test method to evaluate it. Finally, print the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Save your model using the torch.save method. This method takes two arguments: the first one is the object to save, the second one is the path to the file where the object will be saved. Here, you will save the model's state dictionary ( net.state_dict() ) in a file named mnist_net.pth . The state dictionary is a Python dictionary containing all the weights and biases of the network. torch.save(net.state_dict(), 'weights/mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train.py --epochs=5 --lr=1e-3 --batch_size=64 Monitoring and experiment management Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network Deploying your model with Flask: Now that your model is trained, you will deploy it using a simple Flask application. Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries. The following code is a simple Flask application that will load your model and given an image, it will return the predicted class. The application will listen on port 5075 and will have a single route /predict that will accept a POST request with an image as payload. The image will be received as a byte stream and will first be converted to a PIL image, then will be transformed using the same transformation as during training to be fed to the model. The model will return a tensor containing the probabilities for each class. The class with the highest probability will be returned as a JSON object. Complete the following code to take the path of your model as an argument and load it in the model variable. import argparse import torch import torchvision.transforms as transforms from flask import Flask, jsonify, request from PIL import Image import io from models import MNISTNet device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') app = Flask(__name__) parser = ... ... model_path = ... model = MNISTNet().to(device) # Load the model model.load_state_dict(torch.load(model_path)) model.eval() transform = transforms.Compose([ transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) @app.route('/predict', methods=['POST']) def predict(): img_binary = request.data img_pil = Image.open(io.BytesIO(img_binary)) # Transform the PIL image tensor = transform(img_pil).to(device) tensor = tensor.unsqueeze(0) # Add batch dimension # Make prediction with torch.no_grad(): outputs = model(tensor) _, predicted = outputs.max(1) return jsonify({\"prediction\": int(predicted[0])}) if __name__ == \"__main__\": app.run(host='0.0.0.0', port=5075, debug=True) Save the code in a file named mnist_api.py and run it with: python mnist_api.py --model_path [PATH_TO_YOUR_MODEL] Now run the test_api.ipynb notebook to test your API. We requested the api one image at a time. As you may already know, neural networks are much more efficient when they are fed with a batch of images. Modify the mnist_api.py by adding a new route /batch_predict that will accept a batch of images and return a batch of predictions and test it with the last cell of the test_api.ipynb notebook. @app.route('/batch_predict', methods=['POST']) def batch_predict(): # Get the image data from the request images_binary = request.files.getlist(\"images[]\") tensors = [] for img_binary in images_binary: img_pil = Image.open(img_binary.stream) tensor = transform(img_pil) tensors.append(tensor) # Stack tensors to form a batch tensor batch_tensor = torch.stack(tensors, dim=0) # Make prediction with torch.no_grad(): outputs = model(batch_tensor.to(device)) _, predictions = outputs.max(1) return jsonify({\"predictions\": predictions.tolist()}) ``` ## A simple GUI with tkinter The file `mnist_gui.py` contains a simple GUI that will allow you to draw a digit and send it to the API to get a prediction. Run the script with: ```bash python mnist_gui.py --model_path [PATH_TO_YOUR_MODEL] and provide some of the images in the MNIST_sample folder as input to your model. Deploying your model with Gradio As you can see, the GUI is very simple and not very user friendly. Gradio is a library that allows you to quickly create a user friendly web interface for your model. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs='label', live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_gradio.py to either load your model weights or use your api to perform the predictions and run your app with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST? Git Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository. Solutions: model.py train.py mnist_api.py mnist_gradio.py","title":"Practical session 1"},{"location":"developpement/mnist.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/mnist.html#practical-session-1-deploying-a-digit-classifier","text":"For this session, your task is to create a script for training a simple neural network on the MNIST dataset using PyTorch. Throughout the training process, you'll utilize TensorBoard for the following purposes: Keeping an eye on your network's performance as epochs progress. Organizing your different experiments and hyperparameters. Generating visualizations to aid in analysis. Once the training process is complete, you'll also learn how to export your model in a format that can be used for inference. Finally, you will learn how to deploy your model on a REST API using Flask and how to request it using a Python script.","title":"Practical session 1: Deploying a digit classifier"},{"location":"developpement/mnist.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer.","title":"Practical session repository:"},{"location":"developpement/mnist.html#the-network-class","text":"Using the figure above, fill in the following code, in the model.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x","title":"The network class:"},{"location":"developpement/mnist.html#the-training-script","text":"The earlier file included your model class. Now, you will proceed to finalize the training script, named train.py . This script will serve as a Python script for training a neural network on the MNIST Dataset. Both the train() and test() methods have already been implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from model import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total Now you will implement the main method, which will be executed each time the Python script is run. You'd like to offer users the flexibility to adjust certain learning process parameters, specifically: Batch size Learning rate Number of training epochs To achieve this, the Python argparse module will be employed. This module simplifies the creation of user-friendly command-line interfaces. Incorporating arguments into a Python script through argparse is a straightforward process. To begin, you'll need to import the argparse module and create a parser instance within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments as follows: args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments. Call the train method to train your network and the test method to evaluate it. Finally, print the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Save your model using the torch.save method. This method takes two arguments: the first one is the object to save, the second one is the path to the file where the object will be saved. Here, you will save the model's state dictionary ( net.state_dict() ) in a file named mnist_net.pth . The state dictionary is a Python dictionary containing all the weights and biases of the network. torch.save(net.state_dict(), 'weights/mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train.py --epochs=5 --lr=1e-3 --batch_size=64","title":"The training script"},{"location":"developpement/mnist.html#monitoring-and-experiment-management","text":"Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network","title":"Monitoring and experiment management"},{"location":"developpement/mnist.html#deploying-your-model-with-flask","text":"Now that your model is trained, you will deploy it using a simple Flask application. Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries. The following code is a simple Flask application that will load your model and given an image, it will return the predicted class. The application will listen on port 5075 and will have a single route /predict that will accept a POST request with an image as payload. The image will be received as a byte stream and will first be converted to a PIL image, then will be transformed using the same transformation as during training to be fed to the model. The model will return a tensor containing the probabilities for each class. The class with the highest probability will be returned as a JSON object. Complete the following code to take the path of your model as an argument and load it in the model variable. import argparse import torch import torchvision.transforms as transforms from flask import Flask, jsonify, request from PIL import Image import io from models import MNISTNet device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') app = Flask(__name__) parser = ... ... model_path = ... model = MNISTNet().to(device) # Load the model model.load_state_dict(torch.load(model_path)) model.eval() transform = transforms.Compose([ transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) @app.route('/predict', methods=['POST']) def predict(): img_binary = request.data img_pil = Image.open(io.BytesIO(img_binary)) # Transform the PIL image tensor = transform(img_pil).to(device) tensor = tensor.unsqueeze(0) # Add batch dimension # Make prediction with torch.no_grad(): outputs = model(tensor) _, predicted = outputs.max(1) return jsonify({\"prediction\": int(predicted[0])}) if __name__ == \"__main__\": app.run(host='0.0.0.0', port=5075, debug=True) Save the code in a file named mnist_api.py and run it with: python mnist_api.py --model_path [PATH_TO_YOUR_MODEL] Now run the test_api.ipynb notebook to test your API. We requested the api one image at a time. As you may already know, neural networks are much more efficient when they are fed with a batch of images. Modify the mnist_api.py by adding a new route /batch_predict that will accept a batch of images and return a batch of predictions and test it with the last cell of the test_api.ipynb notebook. @app.route('/batch_predict', methods=['POST']) def batch_predict(): # Get the image data from the request images_binary = request.files.getlist(\"images[]\") tensors = [] for img_binary in images_binary: img_pil = Image.open(img_binary.stream) tensor = transform(img_pil) tensors.append(tensor) # Stack tensors to form a batch tensor batch_tensor = torch.stack(tensors, dim=0) # Make prediction with torch.no_grad(): outputs = model(batch_tensor.to(device)) _, predictions = outputs.max(1) return jsonify({\"predictions\": predictions.tolist()}) ``` ## A simple GUI with tkinter The file `mnist_gui.py` contains a simple GUI that will allow you to draw a digit and send it to the API to get a prediction. Run the script with: ```bash python mnist_gui.py --model_path [PATH_TO_YOUR_MODEL] and provide some of the images in the MNIST_sample folder as input to your model.","title":"Deploying your model with Flask:"},{"location":"developpement/mnist.html#deploying-your-model-with-gradio","text":"As you can see, the GUI is very simple and not very user friendly. Gradio is a library that allows you to quickly create a user friendly web interface for your model. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs='label', live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_gradio.py to either load your model weights or use your api to perform the predictions and run your app with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST?","title":"Deploying your model with Gradio"},{"location":"developpement/mnist.html#git","text":"Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository.","title":"Git"},{"location":"developpement/mnist.html#solutions","text":"","title":"Solutions:"},{"location":"developpement/mnist.html#modelpy","text":"","title":"model.py"},{"location":"developpement/mnist.html#trainpy","text":"","title":"train.py"},{"location":"developpement/mnist.html#mnist_apipy","text":"","title":"mnist_api.py"},{"location":"developpement/mnist.html#mnist_gradiopy","text":"","title":"mnist_gradio.py"},{"location":"developpement/pytorch.html","text":"Development for Data Scientist: Pytorch and Python Script Course notebook: Notebook","title":"Introduction to Pytorch"},{"location":"developpement/pytorch.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/pytorch.html#pytorch-and-python-script","text":"","title":"Pytorch and Python Script"},{"location":"developpement/pytorch.html#course-notebook","text":"Notebook","title":"Course notebook:"},{"location":"developpement/quizz.html","text":"Quizz: Developpement tools for Data Scientist Chargement\u2026","title":"Quizz"},{"location":"developpement/quizz.html#quizz-developpement-tools-for-data-scientist","text":"Chargement\u2026","title":"Quizz: Developpement tools for Data Scientist"},{"location":"nlp/nlp.html","text":"Introduction to natural language processing Slides Practical session Practical session Solution Project: During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use.","title":"Course and practical session"},{"location":"nlp/nlp.html#introduction-to-natural-language-processing","text":"Slides","title":"Introduction to natural language processing"},{"location":"nlp/nlp.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"nlp/nlp.html#project","text":"During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use.","title":"Project:"},{"location":"nlp/quizz.html","text":"Chargement\u2026","title":"Quizz"},{"location":"old/docker_2021.html","text":"Development for Data Scientist: Docker Course Slides Practical Session In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Development for Data Scientist:"},{"location":"old/docker_2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"old/docker_2021.html#docker","text":"","title":"Docker"},{"location":"old/docker_2021.html#course","text":"Slides","title":"Course"},{"location":"old/docker_2021.html#practical-session","text":"In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Practical Session"},{"location":"old/gcloud2021.html","text":"Development for Data Scientist: Introduction to Google Cloud Computing Course Slides Practical Session In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it! Set up your virtual machine First follow the GCloud setup process described here . The python script Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Training on GCloud You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Bonus synchronize with Rsync An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Development for Data Scientist:"},{"location":"old/gcloud2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"old/gcloud2021.html#introduction-to-google-cloud-computing","text":"","title":"Introduction to Google Cloud Computing"},{"location":"old/gcloud2021.html#course","text":"Slides","title":"Course"},{"location":"old/gcloud2021.html#practical-session","text":"In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it!","title":"Practical Session"},{"location":"old/gcloud2021.html#set-up-your-virtual-machine","text":"First follow the GCloud setup process described here .","title":"Set up your virtual machine"},{"location":"old/gcloud2021.html#the-python-script","text":"Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The","title":"The python script"},{"location":"old/gcloud2021.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth')","title":"Training script"},{"location":"old/gcloud2021.html#training-on-gcloud","text":"You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab.","title":"Training on GCloud"},{"location":"old/gcloud2021.html#bonus-synchronize-with-rsync","text":"An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Bonus synchronize with Rsync"},{"location":"old/gcloud_set_up.html","text":"Set up GCloud: Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCloud:"},{"location":"old/gcloud_set_up.html#set-up-gcloud","text":"Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCloud:"},{"location":"old/policy_gradient.html","text":"Introduction to Reinforcement Learning: Policy Gradient Slides Practical session","title":"Introduction to Reinforcement Learning:"},{"location":"old/policy_gradient.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"old/policy_gradient.html#policy-gradient","text":"Slides Practical session","title":"Policy Gradient"},{"location":"old/q_learning.html","text":"Introduction to Reinforcement Learning: From policy iteration to Deep Q-Learning Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"Introduction to Reinforcement Learning:"},{"location":"old/q_learning.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"old/q_learning.html#from-policy-iteration-to-deep-q-learning","text":"Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"From policy iteration to Deep Q-Learning"},{"location":"old/text1.html","text":"Text Cleaning and Text Vectorization Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"old/text1.html#text-cleaning-and-text-vectorization","text":"Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"old/text2.html","text":"Words Embedding Slides Practical session","title":"Words Embedding"},{"location":"old/text2.html#words-embedding","text":"Slides Practical session","title":"Words Embedding"},{"location":"old/text3.html","text":"Text Recurrent Network Slides Practical session","title":"Text Recurrent Network"},{"location":"old/text3.html#text-recurrent-network","text":"Slides Practical session","title":"Text Recurrent Network"},{"location":"ood/ood.html","text":"Out-of-Distribution and Anomaly Detection By Joseba Dalmau Course slides: Quiz: OOD and Anomaly Detection Quiz","title":"Course and practical session"},{"location":"ood/ood.html#out-of-distribution-and-anomaly-detection","text":"","title":"Out-of-Distribution and Anomaly Detection"},{"location":"ood/ood.html#by-joseba-dalmau","text":"","title":"By Joseba Dalmau"},{"location":"ood/ood.html#course-slides","text":"","title":"Course slides:"},{"location":"ood/ood.html#quiz","text":"OOD and Anomaly Detection Quiz","title":"Quiz:"},{"location":"project/Assignment.html","text":"Project assignment You now have finished the course. Congratulations! This class was split in 5 different sections. I tried to make the project assignment a combination of the different sections. You had to build a recommender system for movies first based on movie Posters and then based on movie plots and provide a webapp to use it. This project was supposed to cover the first 3 sections of the course, being the development and deployment of an AI model, recommendation systems and natural language processing. Since I saw that some of you were still struggling with the deployment part, I decided to ease the second part of the project assignment. I provided you with a pretrained reinforcement learning agent and you have to build an XAI tool to explain its decisions. For the first part of your project, you are suppose to invite me (DavidBert) to a private Github repository that contains all the code and Dockerfiles needed to run your project. You are also supposed to provide a Docker compose file that allows to run your project. I do not want to see any dataset, model weights, pickle files or annoy indexes in your repository that weights more than 100MB. Instead of that I propose that you store them on a shared Google Drive provide in your readme file a link to download them. I will manually download them and just extract the zip file in the root directory of your project. This means that if you worked correctly on the project, I should be able to run it with a single command: docker-compose up . If the project is not working, I will not try to fix it, I will just grade it as it is so make sure that it is working before submitting it ;-). Concerning the last part of the project, you are supposed to provide a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. I want to be able to run your notebook directly on Google Colab without modifying it. Finally, the project is due on February 2nd at 23:59 for INSA students and march 3rd at 23:59 for Valdom students. (INSA students were supposed to work in groups while Valdom students were supposed to work alone that is why the deadline is different). Send me an e-mail when you are done with the link to your repository to verify that everything is ok. I won't accept any project after the deadline no matter what the reason is so make sure to finish it on time. Good luck with your exams and good luck with your project!","title":"Project assignment"},{"location":"project/Assignment.html#project-assignment","text":"You now have finished the course. Congratulations! This class was split in 5 different sections. I tried to make the project assignment a combination of the different sections. You had to build a recommender system for movies first based on movie Posters and then based on movie plots and provide a webapp to use it. This project was supposed to cover the first 3 sections of the course, being the development and deployment of an AI model, recommendation systems and natural language processing. Since I saw that some of you were still struggling with the deployment part, I decided to ease the second part of the project assignment. I provided you with a pretrained reinforcement learning agent and you have to build an XAI tool to explain its decisions. For the first part of your project, you are suppose to invite me (DavidBert) to a private Github repository that contains all the code and Dockerfiles needed to run your project. You are also supposed to provide a Docker compose file that allows to run your project. I do not want to see any dataset, model weights, pickle files or annoy indexes in your repository that weights more than 100MB. Instead of that I propose that you store them on a shared Google Drive provide in your readme file a link to download them. I will manually download them and just extract the zip file in the root directory of your project. This means that if you worked correctly on the project, I should be able to run it with a single command: docker-compose up . If the project is not working, I will not try to fix it, I will just grade it as it is so make sure that it is working before submitting it ;-). Concerning the last part of the project, you are supposed to provide a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. I want to be able to run your notebook directly on Google Colab without modifying it. Finally, the project is due on February 2nd at 23:59 for INSA students and march 3rd at 23:59 for Valdom students. (INSA students were supposed to work in groups while Valdom students were supposed to work alone that is why the deadline is different). Send me an e-mail when you are done with the link to your repository to verify that everything is ok. I won't accept any project after the deadline no matter what the reason is so make sure to finish it on time. Good luck with your exams and good luck with your project!","title":"Project assignment"},{"location":"project/Docker_compose_annoy.html","text":"Docker compose example Since it is not that easy to run Gradio and Annoy in a Docker container, I created an example to show how to do it to help you get started on the project. It is available here . The annoy-api and Dockefile-api files shows how to putt an Annoy index in a Docker container and expose it as a REST API. The Dockerfile-gradio and the gradio-webapp files show how to put a Gradio webapp in a Docker container. You could easily start from this example to build your recommender system. In the gradio-webapp I generate a random vector to show how to use the API. You can replace it with your own vector, computed by your model. In the annoy-api I use a fake index to show how to use the API. You can replace it with your own index, computed by your model and return as many results as you want. How to run Install Docker and Docker compose Clone this repository: git clone https://github.com/DavidBert/docker_compose_example.git Run docker-compose up in the root directory of this repository Open localhost:7860 in your browser Upload an image and see the results To stop the server, run docker-compose down in the root directory of this repository To remove the containers, run docker-compose rm in the root directory of this repository To remove the images, run docker image prune -a in the root directory of this repository In case you want remove everything docker system prune -a --volumes (carrefull it removes every docker images and containers present in the sytem)","title":"Docker compose example"},{"location":"project/Docker_compose_annoy.html#docker-compose-example","text":"Since it is not that easy to run Gradio and Annoy in a Docker container, I created an example to show how to do it to help you get started on the project. It is available here . The annoy-api and Dockefile-api files shows how to putt an Annoy index in a Docker container and expose it as a REST API. The Dockerfile-gradio and the gradio-webapp files show how to put a Gradio webapp in a Docker container. You could easily start from this example to build your recommender system. In the gradio-webapp I generate a random vector to show how to use the API. You can replace it with your own vector, computed by your model. In the annoy-api I use a fake index to show how to use the API. You can replace it with your own index, computed by your model and return as many results as you want.","title":"Docker compose example"},{"location":"project/Docker_compose_annoy.html#how-to-run","text":"Install Docker and Docker compose Clone this repository: git clone https://github.com/DavidBert/docker_compose_example.git Run docker-compose up in the root directory of this repository Open localhost:7860 in your browser Upload an image and see the results To stop the server, run docker-compose down in the root directory of this repository To remove the containers, run docker-compose rm in the root directory of this repository To remove the images, run docker image prune -a in the root directory of this repository In case you want remove everything docker system prune -a --volumes (carrefull it removes every docker images and containers present in the sytem)","title":"How to run"},{"location":"project/project.html","text":"Project: Welcome to the final project presentation page! In the following, you will find the instructions for the project that you will have to complete during the course. This project will be composed of 4 parts. I tried to build it in a way that would make you use all the tools and frameworks that we will see in class and that would let you start working each individual part once you have completed the corresponding practical session. We will build a web application that will help users to perform many operations about movies like: - predicting the genre of a movie according to its plot or its poster - recommending movies according to the posters or their plots - detect wether an image is a poster of a movie During the last hour of every second practical session, you are allowed to start working on the project. I really recommend you to do so, but I also encourage you to finish the corresponding practical session before starting the project. I will provide you with a solution for the practical session that you can use as a reference. Part 1: Development tools for Data Scientist You should now have completed practical sessions associated to the project. If not I would recommend to do them before continuing. In the practical sessions, you should have learned how to use docker and how to create a rest API using flask. In parallel, you should have learned in HDDL how to use a pre-trained model and how to fine-tune it to perform a new task. For this first part of the project, I would like you to train a pretrained model to predict the genre of a movie according to its poster. Then you will have to create a rest API to predict the genre of a movie according to its poster. Then use gradio to create a web interface that calls this API and allows users to upload a movie poster and get the predicted genre. Once all this is done, you will have to create a docker file to package your API and web interface and push it to a github repository that you will have to share with me. Please do not store the models weights in your repository, but rather on a cloud like Google drive and make your docker file download them at runtime. Dataset: I created a dataset of movie posters and their corresponding genres. Posters are stored in their corresponding genre folder in such a way that it is easy for you to load with the torchvision.datasets.ImageFolder class. You can download it here . Do not store it in your Github repository! Additional help: An easy way to run both the web app and the REST API in a single container would consist to create a bash script that runs both the web app and the API service. This is not the best practice, but it might be easier for you to do so. Thus you can start with this solution and then try to run the web app and the annoy index in two different containers. Here is an example of a bash script that runs both the web app and the rest API: #!/bin/bash python python api.py & gradio_app.py The & operator is used to put jobs in the background. Here the API service is run in the background and the web app is run in the foreground. Call this script in your docker file to run the application. The good practice consists in runnnig the web app in a docker container and the API in another container. To do so you can use docker-compose. Look at the docker-compose documentation to learn how to use it. Here are the theroritical steps to follow to run the web app and the API in two different containers using docker-compose. First, you need to create Dockerfiles for both the Gradio web app and the API. Then create a docker-compose.yml file to define and run the multi-container Docker applications. For exemple something like: version: '3.8' services: gradio-app: build: context: . dockerfile: Dockerfile-gradio ports: - \"7860:7860\" depends_on: - model_api model_api: build: context: . dockerfile: Dockerfile-api ports: - \"5000:5000\" Make sure in your gradio app to call the API through the url http://model_api:5000/ as the base URL for API requests. To run the application, run the following command in the same directory as the docker-compose.yml file: docker-compose up The Gradio web app should be accessible at http://localhost:7860. The REST API, if it has endpoints exposed, will be accessible at http://localhost:5000. To stop and remove the containers, networks, and volumes created by docker-compose up, run: docker-compose down Part 2: Recommendation systems based on posters During the practical session, you saw how to build a recommender system based on content using the movie posters. Use Gradio to build a web app that takes as input a movie poster and returns the images of the 5 most similar movies according to their poster. I would like you to mimic a real recommender system using a vector database. To do so I want the database to be requested by the web app through a REST API. The web app should be light and fast. Use a pre-trained network only to extract the vector representation of the input image and call through the REST API the annoy index you built during the practical session to find the 5 most similar movies. For the sake of simplicity, I would recommend you to use the same network you used in the previous part and just adding a new route to the API to return the 5 most similar movies. Dataset: Use the same dataset as in the practical session. Part 3: Recommendation systems based on plots During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use. Part 4 XAI: Modify the part 1 to show the user the explanation of the prediction. Provide 3 different explanations using 3 different XAI methods of your choice. What you are going to submit: Make a private repository on github and share it with me. In this repository, you will have to store the following: - The code for the 4 parts of the project - The corresponding docker files and docker-compose file if any - A readme file explaining how to run the project. I will make nothing more than running the commands written in the readme file. I suggest you to try to run the entire project from a clean environment to make sure that it works. - Do not store the models weights in your repository, but rather on a cloud like Google drive and make your docker file download them at runtime. - Same for the dataset, not stored on a drive if it is not necessary but also downloaded at runtime. Evaluation criteria: The project is working and runs without errors The readme file is well written and explains how to run the project Clarity of the code The project is well organized No unecessary libraries are installed in each container Deadline: The project is due on the 16th of February at 11:59 PM. Invite me to the repository. Please send me an email to confirm that you have completed the project containing the name of all the members of the group and the link to the repository. Additional help: You can find a small example of how to create and orchestrate both the API and the web interface using docker compose here . Try to understand how it works and then modify it to fit the project.","title":"Project:"},{"location":"project/project.html#project","text":"Welcome to the final project presentation page! In the following, you will find the instructions for the project that you will have to complete during the course. This project will be composed of 4 parts. I tried to build it in a way that would make you use all the tools and frameworks that we will see in class and that would let you start working each individual part once you have completed the corresponding practical session. We will build a web application that will help users to perform many operations about movies like: - predicting the genre of a movie according to its plot or its poster - recommending movies according to the posters or their plots - detect wether an image is a poster of a movie During the last hour of every second practical session, you are allowed to start working on the project. I really recommend you to do so, but I also encourage you to finish the corresponding practical session before starting the project. I will provide you with a solution for the practical session that you can use as a reference.","title":"Project:"},{"location":"project/project.html#part-1-development-tools-for-data-scientist","text":"You should now have completed practical sessions associated to the project. If not I would recommend to do them before continuing. In the practical sessions, you should have learned how to use docker and how to create a rest API using flask. In parallel, you should have learned in HDDL how to use a pre-trained model and how to fine-tune it to perform a new task. For this first part of the project, I would like you to train a pretrained model to predict the genre of a movie according to its poster. Then you will have to create a rest API to predict the genre of a movie according to its poster. Then use gradio to create a web interface that calls this API and allows users to upload a movie poster and get the predicted genre. Once all this is done, you will have to create a docker file to package your API and web interface and push it to a github repository that you will have to share with me. Please do not store the models weights in your repository, but rather on a cloud like Google drive and make your docker file download them at runtime.","title":"Part 1: Development tools for Data Scientist"},{"location":"project/project.html#dataset","text":"I created a dataset of movie posters and their corresponding genres. Posters are stored in their corresponding genre folder in such a way that it is easy for you to load with the torchvision.datasets.ImageFolder class. You can download it here . Do not store it in your Github repository!","title":"Dataset:"},{"location":"project/project.html#additional-help","text":"An easy way to run both the web app and the REST API in a single container would consist to create a bash script that runs both the web app and the API service. This is not the best practice, but it might be easier for you to do so. Thus you can start with this solution and then try to run the web app and the annoy index in two different containers. Here is an example of a bash script that runs both the web app and the rest API: #!/bin/bash python python api.py & gradio_app.py The & operator is used to put jobs in the background. Here the API service is run in the background and the web app is run in the foreground. Call this script in your docker file to run the application. The good practice consists in runnnig the web app in a docker container and the API in another container. To do so you can use docker-compose. Look at the docker-compose documentation to learn how to use it. Here are the theroritical steps to follow to run the web app and the API in two different containers using docker-compose. First, you need to create Dockerfiles for both the Gradio web app and the API. Then create a docker-compose.yml file to define and run the multi-container Docker applications. For exemple something like: version: '3.8' services: gradio-app: build: context: . dockerfile: Dockerfile-gradio ports: - \"7860:7860\" depends_on: - model_api model_api: build: context: . dockerfile: Dockerfile-api ports: - \"5000:5000\" Make sure in your gradio app to call the API through the url http://model_api:5000/ as the base URL for API requests. To run the application, run the following command in the same directory as the docker-compose.yml file: docker-compose up The Gradio web app should be accessible at http://localhost:7860. The REST API, if it has endpoints exposed, will be accessible at http://localhost:5000. To stop and remove the containers, networks, and volumes created by docker-compose up, run: docker-compose down","title":"Additional help:"},{"location":"project/project.html#part-2-recommendation-systems-based-on-posters","text":"During the practical session, you saw how to build a recommender system based on content using the movie posters. Use Gradio to build a web app that takes as input a movie poster and returns the images of the 5 most similar movies according to their poster. I would like you to mimic a real recommender system using a vector database. To do so I want the database to be requested by the web app through a REST API. The web app should be light and fast. Use a pre-trained network only to extract the vector representation of the input image and call through the REST API the annoy index you built during the practical session to find the 5 most similar movies. For the sake of simplicity, I would recommend you to use the same network you used in the previous part and just adding a new route to the API to return the 5 most similar movies.","title":"Part 2: Recommendation systems based on posters"},{"location":"project/project.html#dataset_1","text":"Use the same dataset as in the practical session.","title":"Dataset:"},{"location":"project/project.html#part-3-recommendation-systems-based-on-plots","text":"During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use.","title":"Part 3: Recommendation systems based on plots"},{"location":"project/project.html#part-4-xai","text":"Modify the part 1 to show the user the explanation of the prediction. Provide 3 different explanations using 3 different XAI methods of your choice.","title":"Part 4 XAI:"},{"location":"project/project.html#what-you-are-going-to-submit","text":"Make a private repository on github and share it with me. In this repository, you will have to store the following: - The code for the 4 parts of the project - The corresponding docker files and docker-compose file if any - A readme file explaining how to run the project. I will make nothing more than running the commands written in the readme file. I suggest you to try to run the entire project from a clean environment to make sure that it works. - Do not store the models weights in your repository, but rather on a cloud like Google drive and make your docker file download them at runtime. - Same for the dataset, not stored on a drive if it is not necessary but also downloaded at runtime.","title":"What you are going to submit:"},{"location":"project/project.html#evaluation-criteria","text":"The project is working and runs without errors The readme file is well written and explains how to run the project Clarity of the code The project is well organized No unecessary libraries are installed in each container","title":"Evaluation criteria:"},{"location":"project/project.html#deadline","text":"The project is due on the 16th of February at 11:59 PM. Invite me to the repository. Please send me an email to confirm that you have completed the project containing the name of all the members of the group and the link to the repository.","title":"Deadline:"},{"location":"project/project.html#additional-help_1","text":"You can find a small example of how to create and orchestrate both the API and the web interface using docker compose here . Try to understand how it works and then modify it to fit the project.","title":"Additional help:"},{"location":"rec_sys/quizz.html","text":"Quizz: Recommender systems Chargement\u2026","title":"Quizz"},{"location":"rec_sys/quizz.html#quizz-recommender-systems","text":"Chargement\u2026","title":"Quizz: Recommender systems"},{"location":"rec_sys/rec_sys.html","text":"Recommendation Systems Course: Slides recommender systems Practical session: IMDB recommender system: Solution:","title":"Course and practical session"},{"location":"rec_sys/rec_sys.html#recommendation-systems","text":"","title":"Recommendation Systems"},{"location":"rec_sys/rec_sys.html#course","text":"Slides recommender systems","title":"Course:"},{"location":"rec_sys/rec_sys.html#practical-session","text":"IMDB recommender system: Solution:","title":"Practical session:"},{"location":"rl/quizz.html","text":"Chargement\u2026","title":"Quizz"},{"location":"rl/rl.html","text":"Introduction to Reinforcement Learning: Slides Practical sessions: , solution: , solution: , solution:","title":"Course and practical session"},{"location":"rl/rl.html#introduction-to-reinforcement-learning","text":"Slides Practical sessions: , solution: , solution: , solution:","title":"Introduction to Reinforcement Learning:"},{"location":"xai/interpretability.html","text":"Interpretability in Machine Learning Slides Practical session Practical session Solution","title":"Course and practical session"},{"location":"xai/interpretability.html#interpretability-in-machine-learning","text":"Slides","title":"Interpretability in Machine Learning"},{"location":"xai/interpretability.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"xai/project.html","text":"XAI Project. This is the last part of the course project. In this part, you will have to build a XAI tool for a reinforcement learning agent. You will have to use the tools seen in the course, and you can also use other tools if you want. You are supposed to return a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. You will try to explain the decisions of a reinforcement learning agent in a simple video game called fruitbot from the procgen suite . You are not supposed to train the agent yourself, you can use a pretrained agent whose weights are provided here . Before starting, copy the following files ( agent.py , procgen_wrappers.py ) on your folder and install the procgen suite. pip install procgen You can instanciate the environment with the following code: import torch from procgen import ProcgenEnv from procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame env = ProcgenEnv( num_envs=1, env_name=\"fruitbot\", start_level=0, num_levels=100, distribution_mode='easy', ) env = VecExtractDictObs(env, \"rgb\") env = TransposeFrame(env) env = ScaledFloatFrame(env) The Agent class is defined in the agent.py file. An agent is composed of a feature extractor and a policy. The feature extractor is a neural network that takes the image as input and outputs a vector of features. The policy is a neural network that takes the features as input and outputs a score for each action. The action with the highest score is the one that is chosen. You can use it as follows: from agent import Agent agent = Agent() agent.load_state_dict(torch.load('agent_weights.pth')) agent.eval() The following code shows how to use the agent to play a game: obs = env.reset() while True: obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) if done[0]: break env.close() The goal of the project is to explain the decisions of the agent. You are exected to produce several gifs that show the agent playing the game and the explanations. Here is an example of what could be done: You can use the tools seen in the course, and you can also use other tools if you want. You are expected to produce three types of explanations. Don't be surprised if some methods don't work well on every frame (like grad-CAM), it is normal. To help you, here is a snippet that shows how to record a gif: import imageio from IPython.display import Image from tqdm.notebook import tqdm from IPython.display import clear_output import matplotlib.pyplot as plt import numpy as np def obs_to_image(obs): return (obs[0].transpose(1,2,0) * 255).astype(np.uint8) def display_trajectory(frames, fps=25): imageio.mimwrite('./tmp.gif', [obs_to_image(frame) for i, frame in enumerate(frames)], fps=fps) return(Image(open('tmp.gif','rb').read(), width=500, height=500)) frames = [] obs = env.reset() while True: frames.append(obs) obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) img = env.render() if done[0]: break env.close() display_trajectory(frames) You are free to code on your own machine, but you will have to submit a notebook that can be run on colab . Good luck!","title":"XAI Project."},{"location":"xai/project.html#xai-project","text":"This is the last part of the course project. In this part, you will have to build a XAI tool for a reinforcement learning agent. You will have to use the tools seen in the course, and you can also use other tools if you want. You are supposed to return a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. You will try to explain the decisions of a reinforcement learning agent in a simple video game called fruitbot from the procgen suite . You are not supposed to train the agent yourself, you can use a pretrained agent whose weights are provided here . Before starting, copy the following files ( agent.py , procgen_wrappers.py ) on your folder and install the procgen suite. pip install procgen You can instanciate the environment with the following code: import torch from procgen import ProcgenEnv from procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame env = ProcgenEnv( num_envs=1, env_name=\"fruitbot\", start_level=0, num_levels=100, distribution_mode='easy', ) env = VecExtractDictObs(env, \"rgb\") env = TransposeFrame(env) env = ScaledFloatFrame(env) The Agent class is defined in the agent.py file. An agent is composed of a feature extractor and a policy. The feature extractor is a neural network that takes the image as input and outputs a vector of features. The policy is a neural network that takes the features as input and outputs a score for each action. The action with the highest score is the one that is chosen. You can use it as follows: from agent import Agent agent = Agent() agent.load_state_dict(torch.load('agent_weights.pth')) agent.eval() The following code shows how to use the agent to play a game: obs = env.reset() while True: obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) if done[0]: break env.close() The goal of the project is to explain the decisions of the agent. You are exected to produce several gifs that show the agent playing the game and the explanations. Here is an example of what could be done: You can use the tools seen in the course, and you can also use other tools if you want. You are expected to produce three types of explanations. Don't be surprised if some methods don't work well on every frame (like grad-CAM), it is normal. To help you, here is a snippet that shows how to record a gif: import imageio from IPython.display import Image from tqdm.notebook import tqdm from IPython.display import clear_output import matplotlib.pyplot as plt import numpy as np def obs_to_image(obs): return (obs[0].transpose(1,2,0) * 255).astype(np.uint8) def display_trajectory(frames, fps=25): imageio.mimwrite('./tmp.gif', [obs_to_image(frame) for i, frame in enumerate(frames)], fps=fps) return(Image(open('tmp.gif','rb').read(), width=500, height=500)) frames = [] obs = env.reset() while True: frames.append(obs) obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) img = env.render() if done[0]: break env.close() display_trajectory(frames) You are free to code on your own machine, but you will have to submit a notebook that can be run on colab . Good luck!","title":"XAI Project."},{"location":"xai/quizz.html","text":"Chargement\u2026","title":"Quizz"}]}